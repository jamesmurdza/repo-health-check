{"total_count": 19570, "incomplete_results": false, "items": [{"url": "https://api.github.com/repos/nodejs/node/issues/60004", "repository_url": "https://api.github.com/repos/nodejs/node", "labels_url": "https://api.github.com/repos/nodejs/node/issues/60004/labels{/name}", "comments_url": "https://api.github.com/repos/nodejs/node/issues/60004/comments", "events_url": "https://api.github.com/repos/nodejs/node/issues/60004/events", "html_url": "https://github.com/nodejs/node/issues/60004", "id": 3451803557, "node_id": "I_kwDOAZ7xs87Nvlel", "number": 60004, "title": "rl.prompt does not respect preserveCursor", "user": {"login": "xieyuheng", "id": 4354888, "node_id": "MDQ6VXNlcjQzNTQ4ODg=", "avatar_url": "https://avatars.githubusercontent.com/u/4354888?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xieyuheng", "html_url": "https://github.com/xieyuheng", "followers_url": "https://api.github.com/users/xieyuheng/followers", "following_url": "https://api.github.com/users/xieyuheng/following{/other_user}", "gists_url": "https://api.github.com/users/xieyuheng/gists{/gist_id}", "starred_url": "https://api.github.com/users/xieyuheng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xieyuheng/subscriptions", "organizations_url": "https://api.github.com/users/xieyuheng/orgs", "repos_url": "https://api.github.com/users/xieyuheng/repos", "events_url": "https://api.github.com/users/xieyuheng/events{/privacy}", "received_events_url": "https://api.github.com/users/xieyuheng/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2025-09-25T04:11:43Z", "updated_at": "2025-09-25T04:14:36Z", "closed_at": null, "author_association": "NONE", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Version\n\nv24.8.0\n\n### Platform\n\n```text\nLinux lattice 6.12.48_1 #1 SMP PREEMPT_DYNAMIC Fri Sep 19 23:47:13 UTC 2025 x86_64 GNU/Linux\n```\n\n### Subsystem\n\nreadline\n\n### What steps will reproduce the bug?\n\n`rl.prompt(true)` is not effective,\nlast line of the output of `process.stdout.write(\"hi\")` will be overwritten.\n\n### How often does it reproduce? Is there a required condition?\n\nno.\n\n### What is the expected behavior? Why is that the expected behavior?\n\n> rl.prompt([preserveCursor])\n>\n>     preserveCursor <boolean> If true,\n>     prevents the cursor placement from being reset to 0.\n\nwhen `preserveCursor` is `true`, output should not be overwritten.\n\n### What do you see instead?\n\n\nwhen `preserveCursor` is `true`, output is overwritten.\n\n### Additional information\n\n_No response_", "reactions": {"url": "https://api.github.com/repos/nodejs/node/issues/60004/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/nodejs/node/issues/60004/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/nodejs/node/issues/60003", "repository_url": "https://api.github.com/repos/nodejs/node", "labels_url": "https://api.github.com/repos/nodejs/node/issues/60003/labels{/name}", "comments_url": "https://api.github.com/repos/nodejs/node/issues/60003/comments", "events_url": "https://api.github.com/repos/nodejs/node/issues/60003/events", "html_url": "https://github.com/nodejs/node/issues/60003", "id": 3451300093, "node_id": "I_kwDOAZ7xs87Ntqj9", "number": 60003, "title": "Node SEA Fails to Require NPM Modules that End in \".js\"", "user": {"login": "xcjs", "id": 5085205, "node_id": "MDQ6VXNlcjUwODUyMDU=", "avatar_url": "https://avatars.githubusercontent.com/u/5085205?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xcjs", "html_url": "https://github.com/xcjs", "followers_url": "https://api.github.com/users/xcjs/followers", "following_url": "https://api.github.com/users/xcjs/following{/other_user}", "gists_url": "https://api.github.com/users/xcjs/gists{/gist_id}", "starred_url": "https://api.github.com/users/xcjs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xcjs/subscriptions", "organizations_url": "https://api.github.com/users/xcjs/orgs", "repos_url": "https://api.github.com/users/xcjs/repos", "events_url": "https://api.github.com/users/xcjs/events{/privacy}", "received_events_url": "https://api.github.com/users/xcjs/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2025-09-24T23:42:30Z", "updated_at": "2025-09-24T23:42:30Z", "closed_at": null, "author_association": "NONE", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Version\n\nv22.18.0\n\n### Platform\n\n```text\n* Ubuntu Linux 24.04: 6.14.0-29-generic #29~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Aug 14 16:52:50 UTC 2 x86_64 x86_64 x86_64 GNU/Linux\n* Windows 11: Microsoft Windows NT 10.0.22631.0 x64\n```\n\n### Subsystem\n\n_No response_\n\n### What steps will reproduce the bug?\n\n1. `npm init`\n2. `npm i -s discord.js`\n3. `touch index.js`\n3. Require/import some aspect of discord.js in `index.js`\n4. Attempt to bundle your application in Node SEA.\n5. Execute the Node SEA binary.\n6. `Error [ERR_UNKNOWN_BUILTIN_MODULE]: No such built-in module: discord.js`\n\n### How often does it reproduce? Is there a required condition?\n\n100% of the time.\n\n### What is the expected behavior? Why is that the expected behavior?\n\nI would expect the application to run without error.\n\n### What do you see instead?\n\n`Error [ERR_UNKNOWN_BUILTIN_MODULE]: No such built-in module: discord.js`\n\n### Additional information\n\n_No response_", "reactions": {"url": "https://api.github.com/repos/nodejs/node/issues/60003/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/nodejs/node/issues/60003/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/nodejs/node/issues/60001", "repository_url": "https://api.github.com/repos/nodejs/node", "labels_url": "https://api.github.com/repos/nodejs/node/issues/60001/labels{/name}", "comments_url": "https://api.github.com/repos/nodejs/node/issues/60001/comments", "events_url": "https://api.github.com/repos/nodejs/node/issues/60001/events", "html_url": "https://github.com/nodejs/node/issues/60001", "id": 3451121300, "node_id": "I_kwDOAZ7xs87Ns-6U", "number": 60001, "title": "Certain kinds of HTTPS requests hang due to a race condition around reused sockets", "user": {"login": "martinslota", "id": 46676886, "node_id": "MDQ6VXNlcjQ2Njc2ODg2", "avatar_url": "https://avatars.githubusercontent.com/u/46676886?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinslota", "html_url": "https://github.com/martinslota", "followers_url": "https://api.github.com/users/martinslota/followers", "following_url": "https://api.github.com/users/martinslota/following{/other_user}", "gists_url": "https://api.github.com/users/martinslota/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinslota/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinslota/subscriptions", "organizations_url": "https://api.github.com/users/martinslota/orgs", "repos_url": "https://api.github.com/users/martinslota/repos", "events_url": "https://api.github.com/users/martinslota/events{/privacy}", "received_events_url": "https://api.github.com/users/martinslota/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2025-09-24T21:57:14Z", "updated_at": "2025-09-24T22:31:02Z", "closed_at": null, "author_association": "NONE", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Version\n\n22.20.0\n\n### Platform\n\n```text\nDarwin AQ5T24P47N6 24.6.0 Darwin Kernel Version 24.6.0: Mon Jul 14 11:30:29 PDT 2025; root:xnu-11417.140.69~1/RELEASE_ARM64_T6000 arm64\n```\n\n### Subsystem\n\nhttps\n\n### What steps will reproduce the bug?\n\nSee https://github.com/martinslota/node-socket-reuse-race for a detailed description and a script that shows the situation in which HTTPS client requests hang.\n\n### How often does it reproduce? Is there a required condition?\n\nI can reproduce it every single time (on my machine).\n\n### What is the expected behavior? Why is that the expected behavior?\n\nI expect the script in https://github.com/martinslota/node-socket-reuse-race to be able to run almost indefinitely.\n\n### What do you see instead?\n\nThe script fails.\n\n### Additional information\n\nThis bug report is a result of investigating https://github.com/aws/aws-sdk-js-v3/issues/6426.", "reactions": {"url": "https://api.github.com/repos/nodejs/node/issues/60001/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/nodejs/node/issues/60001/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/nodejs/node/issues/59998", "repository_url": "https://api.github.com/repos/nodejs/node", "labels_url": "https://api.github.com/repos/nodejs/node/issues/59998/labels{/name}", "comments_url": "https://api.github.com/repos/nodejs/node/issues/59998/comments", "events_url": "https://api.github.com/repos/nodejs/node/issues/59998/events", "html_url": "https://github.com/nodejs/node/issues/59998", "id": 3450233825, "node_id": "I_kwDOAZ7xs87NpmPh", "number": 59998, "title": "Chained loaders can break each other with ERR_METHOD_NOT_IMPLEMENTED", "user": {"login": "bengl", "id": 110455, "node_id": "MDQ6VXNlcjExMDQ1NQ==", "avatar_url": "https://avatars.githubusercontent.com/u/110455?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bengl", "html_url": "https://github.com/bengl", "followers_url": "https://api.github.com/users/bengl/followers", "following_url": "https://api.github.com/users/bengl/following{/other_user}", "gists_url": "https://api.github.com/users/bengl/gists{/gist_id}", "starred_url": "https://api.github.com/users/bengl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bengl/subscriptions", "organizations_url": "https://api.github.com/users/bengl/orgs", "repos_url": "https://api.github.com/users/bengl/repos", "events_url": "https://api.github.com/users/bengl/events{/privacy}", "received_events_url": "https://api.github.com/users/bengl/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 2930092013, "node_id": "MDU6TGFiZWwyOTMwMDkyMDEz", "url": "https://api.github.com/repos/nodejs/node/labels/loaders", "name": "loaders", "color": "7DE2DE", "default": false, "description": "Issues and PRs related to ES module loaders"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2025-09-24T17:06:49Z", "updated_at": "2025-09-24T17:21:07Z", "closed_at": null, "author_association": "MEMBER", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Version\n\nv20.6.0 (and all higher versions on even release lines)\n\n### Platform\n\n```text\nDarwin m1-poutine 24.6.0 Darwin Kernel Version 24.6.0: Mon Jul 14 11:30:29 PDT 2025; root:xnu-11417.140.69~1/RELEASE_ARM64_T6000 arm64 arm Darwin\n```\n\n### Subsystem\n\nloaders\n\n### What steps will reproduce the bug?\n\nThe exact scenario involves two async loaders:\n\n* The first sets the `.source` property, [as indicated by the docs](https://nodejs.org/docs/latest/api/module.html#caveat-in-the-asynchronous-load-hook). It can do anything else, but this is required to reproduce.\n* The second imports a CommonJS file at some point. This needs to be done via an `import`, and not via `createRequire`. This is a realistic scenario, because [`import-in-the-middle`](https://github.com/nodejs/import-in-the-middle) is implemented this way.\n\nA reproduction is available [here](https://gist.github.com/bengl/75470adfc01e83886572132ab1ce7af4). On versions of Node.js that don't support `Module.register`, the CLI flags can be used instead to produce the same effect.\n\n### How often does it reproduce? Is there a required condition?\n\n100% of the time, given the steps above.\n\n### What is the expected behavior? Why is that the expected behavior?\n\nBoth loaders should work correctly and not produce an exception as indicated below.\n\n### What do you see instead?\n\n\n```\nnode:internal/process/esm_loader:48\n      internalBinding('errors').triggerUncaughtException(\n                                ^\nError [ERR_METHOD_NOT_IMPLEMENTED]: The resolveSync() method is not implemented\n    at new NodeError (node:internal/errors:405:5)\n    at Hooks.resolveSync (node:internal/modules/esm/hooks:353:11)\n    at ModuleLoader.resolveSync (node:internal/modules/esm/loader:355:35)\n    at ModuleLoader.getModuleJobSync (node:internal/modules/esm/loader:225:32)\n    at require (node:internal/modules/esm/translators:191:36)\n    at Object.<anonymous> (/Users/bengl/broken-loaders/hook2.js:1:1)\n    at loadCJSModule (node:internal/modules/esm/translators:207:3)\n    at ModuleWrap.<anonymous> (node:internal/modules/esm/translators:233:7)\n    at ModuleJob.run (node:internal/modules/esm/module_job:217:25)\n    at async ModuleLoader.import (node:internal/modules/esm/loader:308:24) {\n  code: 'ERR_METHOD_NOT_IMPLEMENTED'\n}\n\nNode.js v20.6.0\n```\n\n### Additional information\n\nWe've [worked around this](https://github.com/nodejs/import-in-the-middle/pull/205) in `import-in-the-middle`.", "reactions": {"url": "https://api.github.com/repos/nodejs/node/issues/59998/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/nodejs/node/issues/59998/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/nodejs/node/issues/59994", "repository_url": "https://api.github.com/repos/nodejs/node", "labels_url": "https://api.github.com/repos/nodejs/node/issues/59994/labels{/name}", "comments_url": "https://api.github.com/repos/nodejs/node/issues/59994/comments", "events_url": "https://api.github.com/repos/nodejs/node/issues/59994/events", "html_url": "https://github.com/nodejs/node/issues/59994", "id": 3447099824, "node_id": "I_kwDOAZ7xs87NdpGw", "number": 59994, "title": "Add Promise-based closed(), exited(), and succeeded() to ChildProcess", "user": {"login": "dfabulich", "id": 96150, "node_id": "MDQ6VXNlcjk2MTUw", "avatar_url": "https://avatars.githubusercontent.com/u/96150?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dfabulich", "html_url": "https://github.com/dfabulich", "followers_url": "https://api.github.com/users/dfabulich/followers", "following_url": "https://api.github.com/users/dfabulich/following{/other_user}", "gists_url": "https://api.github.com/users/dfabulich/gists{/gist_id}", "starred_url": "https://api.github.com/users/dfabulich/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dfabulich/subscriptions", "organizations_url": "https://api.github.com/users/dfabulich/orgs", "repos_url": "https://api.github.com/users/dfabulich/repos", "events_url": "https://api.github.com/users/dfabulich/events{/privacy}", "received_events_url": "https://api.github.com/users/dfabulich/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 207445406, "node_id": "MDU6TGFiZWwyMDc0NDU0MDY=", "url": "https://api.github.com/repos/nodejs/node/labels/feature%20request", "name": "feature request", "color": "0D7BDE", "default": false, "description": "Issues that request new features to be added to Node.js."}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2025-09-24T00:43:28Z", "updated_at": "2025-09-24T00:49:09Z", "closed_at": null, "author_association": "CONTRIBUTOR", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### What is the problem this feature will solve?\n\nWhen working with the `spawn` API, you need to add listeners for the `error` event, as well as the `close` event and/or the `exit` event.\n\nThe code can be quite verbose:\n\n```js\nasync function spawnPromise(command, args, options) {\n    await new Promise((resolve, reject) => {\n        const child = spawn(command, args, options);\n        let errored = false;\n        child.once('error', e => {\n            errored = true;\n            reject(e);\n        });\n        child.once('close', (code) => {\n            if (errored) return;\n            if (code === 0) {\n                resolve();\n            } else {\n                const error = new Error('spawn failed with error code: ' + code);\n                reject(error);\n            }\n        })\n    })\n}\n```\n\n### What is the feature you are proposing to solve the problem?\n\n`close` and `exit` are one-time events, so it would be appropriate to make Promise-based helpers for them:\n\n* `closed(): Promise<ChildProcess>`: Resolves to the child process when the `close` event fires, rejects with an error if the error event fires.\n* `exited(): Promise<ChildProcess>`: Resolves to the child process when the `exit` event fires, rejects with an error if the error event fires.\n* `succeeded(): Promise<void>`: Resolves void when the child process closes with 0 exit code; rejects with an error otherwise. (As I'm imagining it, `succeeded()` would resolve to void, because there's nothing left to do with a known-successful child process. You know its exit code (0). None of its methods or properties are meaningful. It's finished.)\n\nThen, you could write one of these:\n\n```js\nawait spawn(command, arguments, { stdio: 'inherit' }).succeeded();\nconst {exitCode, signalCode} = await spawn(command, arguments, { stdio: 'inherit' }).closed();\nconst {exitCode, signalCode} = await spawn(command, arguments, { stdio: 'inherit' }).exited();\n```\n\n### What alternatives have you considered?\n\n* We could add a custom promisifier to `spawn`, like we have for `exec` and `execFile`. But spawn has more than one way to end (closing vs. exiting) and no Node errback parameter, so it's not a clean fit. \n* #54799 documents a thorough \"cleanup\" of the API. This sounds difficult to build consensus around; it's gotten kinda stale, and cleans up all sorts of things.\n* In the thread on the issue above, @benjamingr suggested adding a `.text()` convenience to `ChildProcess`.\n\n  ```js\n  const child = spawn('ls', ['-lh', '/usr']); // reasonable, creates a child process\n  // Now I want to read its stdout and wait for it to close, this is verbose:\n  const output = Buffer.concat(await child.stdout.toArray()).toString();\n  // What if instead we could do:\n  const output2 = await child.text();\n  // Or as a one liner\n  console.log(await spawn('ls', ['-lh', '/usr']).text());\n  ```\n  In my opinion, the promisified `execFile` is already \"good enough\" for buffered output.\n  ```js\n  const {stdout} = await promisify(execFile)('ls', ['-lh', '/usr']);\n  ```\n  It's _spawn_ that I care about, because that's the only asynchronous version of the API that supports `stdio: 'inherit'`. Today, If I want `stdio: inherit`, I have to go fiddling around with spawn callbacks.", "reactions": {"url": "https://api.github.com/repos/nodejs/node/issues/59994/reactions", "total_count": 3, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 1, "rocket": 1, "eyes": 0}, "timeline_url": "https://api.github.com/repos/nodejs/node/issues/59994/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/nodejs/node/issues/59985", "repository_url": "https://api.github.com/repos/nodejs/node", "labels_url": "https://api.github.com/repos/nodejs/node/issues/59985/labels{/name}", "comments_url": "https://api.github.com/repos/nodejs/node/issues/59985/comments", "events_url": "https://api.github.com/repos/nodejs/node/issues/59985/events", "html_url": "https://github.com/nodejs/node/issues/59985", "id": 3444391866, "node_id": "I_kwDOAZ7xs87NTT-6", "number": 59985, "title": "Buffer#copy (and Buffer.concat) can read/write process memory on certain input", "user": {"login": "ChALkeR", "id": 291301, "node_id": "MDQ6VXNlcjI5MTMwMQ==", "avatar_url": "https://avatars.githubusercontent.com/u/291301?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ChALkeR", "html_url": "https://github.com/ChALkeR", "followers_url": "https://api.github.com/users/ChALkeR/followers", "following_url": "https://api.github.com/users/ChALkeR/following{/other_user}", "gists_url": "https://api.github.com/users/ChALkeR/gists{/gist_id}", "starred_url": "https://api.github.com/users/ChALkeR/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ChALkeR/subscriptions", "organizations_url": "https://api.github.com/users/ChALkeR/orgs", "repos_url": "https://api.github.com/users/ChALkeR/repos", "events_url": "https://api.github.com/users/ChALkeR/events{/privacy}", "received_events_url": "https://api.github.com/users/ChALkeR/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2025-09-23T09:14:32Z", "updated_at": "2025-09-23T14:45:49Z", "closed_at": null, "author_association": "MEMBER", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "Note: this is not a security issue only because malicious js is not in the [threat model](https://github.com/nodejs/node/blob/main/SECURITY.md#the-nodejs-threat-model)\n\n\n```js\nconst b = new Uint8Array(1).fill(1)\nObject.defineProperty(b, 'length', { get: () => 20 })\nObject.defineProperty(b, 'byteLength', { get: () => 20 })\nconsole.log(Buffer.concat([b]))\n```\n\nWhile the input is deliberately invalid, returning uninitialized memory is highly unexpected\n\nNote that this doesn't use any `Buffer` apis except for `Buffer.concat`, i.e. doesn't call to `allocUnsafe`\nAlso the `Uint8Array` instance is non-pooled\n\nThis also happens even with `--zero-fill-buffers` flag\n\nThe issue is on the native `_copy` side\n\nAlso reproducible with `Buffer#copy`:\n```js\nconst b = Buffer.alloc(0)\nObject.defineProperty(b, 'byteLength', { get: () => 2000 })\nconst t = Buffer.alloc(2000)\nb.copy(t, 0, 0, 2000)\nconsole.log(t.filter(x => x))\n```\n\nThis also looks like a regression, it didn't happen in 20 or 22.6 but happens in >=22.7 and 24\n\nA sufficiently larger length causes a bus error \ud83d\ude09 \n\n---\n\n### Reading env vars\n\nWith `Buffer.concat`:\n```js\nlet l\nconst b = new Uint8Array(1).fill(1)\nObject.defineProperty(b, 'length', { get: () => l })\nObject.defineProperty(b, 'byteLength', { get: () => l })\nfor (l = 1000; l < 1e5; l+=100) {\n  const c = Buffer.concat([b])\n  const i = c.indexOf('executable_path')\n  if (i >= 0) {\n    const e = c.subarray(i).toString()\n    if (e.length > 1000) {\n      // whatever\n      console.log(e)\n      break\n    }\n  }\n}\n```\n\nWith `Buffer#copy`:\n```js\nconst l = 1000\nconst b = Buffer.alloc(0)\nObject.defineProperty(b, 'byteLength', { get: () => 1e9 })\nconst c = Buffer.alloc(l)\nfor (let a = 0; a < 1e5; a += 100) {\n  b.copy(c, 0, a, a + l)\n  const i = c.indexOf('executable_path')\n  if (i >= 0) {\n    b.copy(c, 0, a + i, a + i + l)\n    console.log(c.toString())\n    break\n  }\n}\n```\n\n---\n\nThe same code could write to process memory, not just read from it\nE.g. this will cause a guard failure (which can be obviously bypassed by reading it first)\n```js\nconst l = 200\nconst b = Buffer.alloc(1)\nObject.defineProperty(b, 'byteLength', { get: () => l })\nconst t = Buffer.alloc(l)\nt.copy(b, 0, 0, l) // writes t into process mem\nconsole.log('x')\n```\n\nReading then writing could also control which exact portions of process memory to overwrite", "reactions": {"url": "https://api.github.com/repos/nodejs/node/issues/59985/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/nodejs/node/issues/59985/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/nodejs/node/issues/59977", "repository_url": "https://api.github.com/repos/nodejs/node", "labels_url": "https://api.github.com/repos/nodejs/node/issues/59977/labels{/name}", "comments_url": "https://api.github.com/repos/nodejs/node/issues/59977/comments", "events_url": "https://api.github.com/repos/nodejs/node/issues/59977/events", "html_url": "https://github.com/nodejs/node/issues/59977", "id": 3442647436, "node_id": "I_kwDOAZ7xs87NMqGM", "number": 59977, "title": "sockets no longer emit `removeListener` events in v20.11.0", "user": {"login": "baileympearson", "id": 23407842, "node_id": "MDQ6VXNlcjIzNDA3ODQy", "avatar_url": "https://avatars.githubusercontent.com/u/23407842?v=4", "gravatar_id": "", "url": "https://api.github.com/users/baileympearson", "html_url": "https://github.com/baileympearson", "followers_url": "https://api.github.com/users/baileympearson/followers", "following_url": "https://api.github.com/users/baileympearson/following{/other_user}", "gists_url": "https://api.github.com/users/baileympearson/gists{/gist_id}", "starred_url": "https://api.github.com/users/baileympearson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/baileympearson/subscriptions", "organizations_url": "https://api.github.com/users/baileympearson/orgs", "repos_url": "https://api.github.com/users/baileympearson/repos", "events_url": "https://api.github.com/users/baileympearson/events{/privacy}", "received_events_url": "https://api.github.com/users/baileympearson/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2025-09-22T20:23:40Z", "updated_at": "2025-09-22T20:23:40Z", "closed_at": null, "author_association": "NONE", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Version\n\n_No response_\n\n### Platform\n\n```text\nmacos, although I'd be surprised if this issue is platform-specific.\n```\n\n### Subsystem\n\n_No response_\n\n### What steps will reproduce the bug?\n\nHere's a reproduction script:\n\n```\nconst assert = require('assert');\nconst { once } = require('events');\nconst { Server, Socket } = require('net');\n\nasync function main() {\n  const server = new Server();\n\n  server.on('connection', conn => {\n    conn.on('data', () => {\n      // do nothing\n    });\n  });\n\n  server.listen('8000');\n\n  const socket = new Socket();\n  await socket.connect({\n    host: 'localhost',\n    port: '8000'\n  });\n\n  const removedListeners = [];\n  socket.on('removeListener', name => removedListeners.push(name));\n\n  // write a large buffer to ensure that the socket buffers the data, forcing the drain event\n  const buffer = Buffer.alloc(10 * (2 ** 10) ** 2);\n  socket.write(buffer);\n\n  const drainEvent = once(socket, 'drain');\n  try {\n    await drainEvent;\n  } finally {\n    assert.ok(removedListeners.includes('drain'));\n  }\n}\n\nmain().then(() => process.exit());\n```\n\nThis passes on <Node 20.11.0, but fails on any newer version.\n\n### How often does it reproduce? Is there a required condition?\n\nEvery time.\n\n### What is the expected behavior? Why is that the expected behavior?\n\nI'd expect to see a `removeListener` event emitted for `drain` (and other events) on the socket, because the `once` helper resolves, which indicates that we have received a `drain` event. \n\n### What do you see instead?\n\nNo `removeListener` event is emitted.\n\n### Additional information\n\n_No response_", "reactions": {"url": "https://api.github.com/repos/nodejs/node/issues/59977/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/nodejs/node/issues/59977/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/nodejs/node/issues/59976", "repository_url": "https://api.github.com/repos/nodejs/node", "labels_url": "https://api.github.com/repos/nodejs/node/issues/59976/labels{/name}", "comments_url": "https://api.github.com/repos/nodejs/node/issues/59976/comments", "events_url": "https://api.github.com/repos/nodejs/node/issues/59976/events", "html_url": "https://github.com/nodejs/node/issues/59976", "id": 3442095791, "node_id": "I_kwDOAZ7xs87NKjav", "number": 59976, "title": "Profiling misattributes usage in Node-API native addons", "user": {"login": "kjvalencik", "id": 2471970, "node_id": "MDQ6VXNlcjI0NzE5NzA=", "avatar_url": "https://avatars.githubusercontent.com/u/2471970?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kjvalencik", "html_url": "https://github.com/kjvalencik", "followers_url": "https://api.github.com/users/kjvalencik/followers", "following_url": "https://api.github.com/users/kjvalencik/following{/other_user}", "gists_url": "https://api.github.com/users/kjvalencik/gists{/gist_id}", "starred_url": "https://api.github.com/users/kjvalencik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kjvalencik/subscriptions", "organizations_url": "https://api.github.com/users/kjvalencik/orgs", "repos_url": "https://api.github.com/users/kjvalencik/repos", "events_url": "https://api.github.com/users/kjvalencik/events{/privacy}", "received_events_url": "https://api.github.com/users/kjvalencik/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2025-09-22T18:00:08Z", "updated_at": "2025-09-22T18:00:08Z", "closed_at": null, "author_association": "NONE", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Version\n\nv24.7.0\n\n### Platform\n\n```text\nmacOS\n```\n\n### Subsystem\n\nNode-API\n\n### What steps will reproduce the bug?\n\nProfile a native addon with `node --inspect` performance tab (or `--prof`). All calls to the native addon will be attributed to the *last* function that's defined.\n\n```c\n#include <stddef.h>\n#include <stdio.h>\n\n#define SIZE_MAX ((size_t)-1)\n#define NAPI_AUTO_LENGTH SIZE_MAX\n\ntypedef int napi_status;\n\ntypedef void* napi_value;\ntypedef void* napi_env;\ntypedef void* napi_callback_info;\n\ntypedef napi_value (*napi_callback)(napi_env, napi_callback_info);\n\nnapi_status napi_create_function(\n\tnapi_env env,\n\tconst char* utf8name,\n\tsize_t length,\n\tnapi_callback cb,\n\tvoid* data,\n\tnapi_value* result\n);\n\nnapi_status napi_set_named_property(\n\tnapi_env env,\n\tnapi_value object,\n\tconst char* utf8Name,\n\tnapi_value value\n);\n\nnapi_value busy_loop(napi_env _env, napi_callback_info _info) {\n\tprintf(\"busy loop called\\n\");\n\n\tfor (int i = 0; i < 1e9; i++) {}\n\n\treturn 0;\n}\n\nnapi_value noop(napi_env _env, napi_callback_info _info) {\n\tprintf(\"noop called\\n\");\n\treturn 0;\n}\n\nnapi_value napi_register_module_v1(napi_env env, napi_value m) {\n\tnapi_value result;\n\tnapi_status status;\n\n\tstatus = napi_create_function(env, \"busyLoop\", NAPI_AUTO_LENGTH, busy_loop, NULL, &result);\n\tif (status != 0) {\n\t\tprintf(\"failed to create busyLoop\\n\");\n\t\treturn m;\n\t}\n\n\tstatus = napi_set_named_property(env, m, \"busyLoop\", result);\n\tif (status != 0) {\n\t\tprintf(\"failed to export busyLoop\\n\");\n\t\treturn m;\n\t}\n\n\tstatus = napi_create_function(env, \"noop\", NAPI_AUTO_LENGTH, noop, NULL, &result);\n\tif (status != 0) {\n\t\tprintf(\"failed to create noop\\n\");\n\t\treturn m;\n\t}\n\n\tstatus = napi_set_named_property(env, m, \"noop\", result);\n\tif (status != 0) {\n\t\tprintf(\"failed to export noop\\n\");\n\t\treturn m;\n\t}\n\n    return m;\n}\n```\n\n```js\n\"use strict\";\n\nconst { busyLoop } = require(\"./index.node\");\n\nconst delay = n => new Promise(r => setTimeout(r, n));\n\nasync function run() {\n    await delay(1000);\n\n    for (let i = 0; i < 10; i++) {\n        busyLoop();\n    }\n}\n\nrun();\n```\n\n```sh\nnode --inspect-brk index.js\n```\n\n### How often does it reproduce? Is there a required condition?\n\nOnly with native addons built with Node-API. However, it happens regardless of language (e.g., plain C or Rust/Neon).\n\n### What is the expected behavior? Why is that the expected behavior?\n\nThe correct method is attributed.\n\n### What do you see instead?\n\n<img width=\"542\" height=\"215\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/adc0dd67-caac-4697-a512-3cdf53635f98\" />\n\n<img width=\"416\" height=\"176\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/44a9004e-6502-4931-84c4-357802b3aa35\" />\n\n### Additional information\n\nA minimal reproduction can be found in this repo. It exports two functions (`busyLoop` and `noop`) and all time is attributed to `noop` instead of `busyLoop`, despite it never being called.\n\nhttps://github.com/kjvalencik/node-addon-profile", "reactions": {"url": "https://api.github.com/repos/nodejs/node/issues/59976/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/nodejs/node/issues/59976/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/nodejs/node/issues/59975", "repository_url": "https://api.github.com/repos/nodejs/node", "labels_url": "https://api.github.com/repos/nodejs/node/issues/59975/labels{/name}", "comments_url": "https://api.github.com/repos/nodejs/node/issues/59975/comments", "events_url": "https://api.github.com/repos/nodejs/node/issues/59975/events", "html_url": "https://github.com/nodejs/node/issues/59975", "id": 3442069899, "node_id": "I_kwDOAZ7xs87NKdGL", "number": 59975, "title": "[spam]", "user": {"login": "dannino555-droid", "id": 225850752, "node_id": "U_kgDODXY1gA", "avatar_url": "https://avatars.githubusercontent.com/u/225850752?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dannino555-droid", "html_url": "https://github.com/dannino555-droid", "followers_url": "https://api.github.com/users/dannino555-droid/followers", "following_url": "https://api.github.com/users/dannino555-droid/following{/other_user}", "gists_url": "https://api.github.com/users/dannino555-droid/gists{/gist_id}", "starred_url": "https://api.github.com/users/dannino555-droid/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dannino555-droid/subscriptions", "organizations_url": "https://api.github.com/users/dannino555-droid/orgs", "repos_url": "https://api.github.com/users/dannino555-droid/repos", "events_url": "https://api.github.com/users/dannino555-droid/events{/privacy}", "received_events_url": "https://api.github.com/users/dannino555-droid/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2025-09-22T17:52:38Z", "updated_at": "2025-09-22T18:59:15Z", "closed_at": "2025-09-22T18:26:20Z", "author_association": "NONE", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": null, "reactions": {"url": "https://api.github.com/repos/nodejs/node/issues/59975/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/nodejs/node/issues/59975/timeline", "performed_via_github_app": null, "state_reason": "completed", "score": 1.0}, {"url": "https://api.github.com/repos/nodejs/node/issues/59972", "repository_url": "https://api.github.com/repos/nodejs/node", "labels_url": "https://api.github.com/repos/nodejs/node/issues/59972/labels{/name}", "comments_url": "https://api.github.com/repos/nodejs/node/issues/59972/comments", "events_url": "https://api.github.com/repos/nodejs/node/issues/59972/events", "html_url": "https://github.com/nodejs/node/issues/59972", "id": 3441504149, "node_id": "I_kwDOAZ7xs87NIS-V", "number": 59972, "title": "\u5173\u4e8enode\u4e2d\u7684http2\u7f51\u7edc\u8bf7\u6c42\u8c03\u8bd5\u4e0e\u6d4f\u89c8\u5668\u7f51\u7edc\u8c03\u8bd5\u884c\u4e3a\u4e0d\u4e00\u81f4\u7684\u95ee\u9898", "user": {"login": "qwerrtyuiopasdf", "id": 128718380, "node_id": "U_kgDOB6wWLA", "avatar_url": "https://avatars.githubusercontent.com/u/128718380?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qwerrtyuiopasdf", "html_url": "https://github.com/qwerrtyuiopasdf", "followers_url": "https://api.github.com/users/qwerrtyuiopasdf/followers", "following_url": "https://api.github.com/users/qwerrtyuiopasdf/following{/other_user}", "gists_url": "https://api.github.com/users/qwerrtyuiopasdf/gists{/gist_id}", "starred_url": "https://api.github.com/users/qwerrtyuiopasdf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qwerrtyuiopasdf/subscriptions", "organizations_url": "https://api.github.com/users/qwerrtyuiopasdf/orgs", "repos_url": "https://api.github.com/users/qwerrtyuiopasdf/repos", "events_url": "https://api.github.com/users/qwerrtyuiopasdf/events{/privacy}", "received_events_url": "https://api.github.com/users/qwerrtyuiopasdf/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2025-09-22T15:19:21Z", "updated_at": "2025-09-23T01:42:59Z", "closed_at": null, "author_association": "NONE", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Version\n\nv25.0.0-pre\n\n### Platform\n\n```text\nwindows11 x86_64\n\nWindowsProductName WindowsVersion TotalPhysicalMemory CsProcessors\n------------------ -------------- ------------------- ------------\nWindows 10 Pro     2009                               {Intel(R) Xeon(R) CPU E5-2690 v3 @ 2.60GHz}\n```\n\n### Subsystem\n\n--experimental-network-inspection\n\n### What steps will reproduce the bug?\n\n\u4f7f\u7528\u4ee5\u4e0b\u65b9\u5f0f\u53d1\u51fa\u8bf7\u6c42\u3010\u7279\u522b\u6ce8\u610f\u76ee\u6807\u7f51\u7ad9\u9700\u8981\u652f\u6301\u538b\u7f29\uff0c\u672c\u95ee\u9898\u662fbr\u538b\u7f29\uff0c\u5373\u670d\u52a1\u5668\u8fd4\u56de\u5934\u542b\u6709content-encoding\u3011\uff0c\u7136\u540e\u5728\u7f51\u7edc\u63a7\u5236\u9762\u677f\uff0c\u54cd\u5e94\u4e2d\u5373\u53ef\u770b\u5230\u4e71\u7801\u3010\u538b\u7f29\u7684\u4e8c\u8fdb\u5236\u3011\n// Node.js v18+ \u53ef\u7528\uff0c\u65e0\u9700\u5b89\u88c5\u4efb\u4f55\u5305\nconst url = 'https://nodejs.org/zh-cn/blog/release/v24.8.0';\n\nfetch(url)\n  .then(res => {\n    if (!res.ok) throw new Error(`HTTP ${res.status}`);\n    return res.text(); // \u81ea\u52a8\u5904\u7406 gzip/br \u89e3\u538b\n  })\n  .then(html => {\n    console.log('\u9875\u9762\u957f\u5ea6:', html.length);\n  })\n  .catch(console.error);\n\n\n### How often does it reproduce? Is there a required condition?\n\n\u53ea\u8981\u76ee\u6807\u670d\u52a1\u5668\u8fd4\u56de\u538b\u7f29\u6570\u636e\uff0c\u63a7\u5236\u53f0\u3010devtools\u3011\u5fc5\u5b9a\u663e\u793a\u5df2\u538b\u7f29\u7684\u6570\u636e\n\n### What is the expected behavior? Why is that the expected behavior?\n\n\u8fd9\u4e2a\u884c\u4e3a\u4e0e\u6d4f\u89c8\u5668\u8fd4\u56de\u7684\u54cd\u5e94\u7ed3\u679c\u4e0d\u4e00\u81f4\uff0c\u5b83\u5e94\u8be5\u663e\u793a\u660e\u6587\u4fe1\u606f\n\n### What do you see instead?\n\n\u5df2\u538b\u7f29\u7684\u4e8c\u8fdb\u5236\u6570\u636e\n\n### Additional information\n\n\u8fd9\u4e2a\u6211\u672a\u80fd\u4f7f\u7528AI\u5bf9\u5176\u8fdb\u884c\u4fee\u590d\uff0c\u4ee5\u4e0b\u662fAI\u5206\u6790\u7684\u62a5\u544a\uff1a\n\n\n# Analysis of Brotli Decompression Failure in Node.js NetworkAgent\n\n## 1. Problem Statement and Initial Diagnosis\n\n### 1.1. Core Issue: Brotli (`br`) Encoded Responses Not Being Decompressed\n\nThe central problem identified within the Node.js `NetworkAgent` component is its failure to correctly decompress HTTP response bodies that are encoded with Brotli (`br`). Despite the presence of a `DecompressBody` function that ostensibly handles this encoding, the system is not producing the expected plaintext output. Instead, the response data remains in its compressed, binary form, leading to garbled and unreadable content when inspected through the Node.js Inspector. This issue is particularly significant for developers relying on the Inspector's Network tab to debug application behavior, as the inability to view response bodies renders the tool ineffective for a wide range of debugging scenarios. The problem persists even after initial modifications to the source code, indicating a deeper, more systemic issue within the data handling pipeline. The core of the problem lies in the disconnect between the server sending a `br`-encoded response and the `NetworkAgent` successfully processing it into a human-readable format.\n\nThe investigation was initiated based on user-provided logs and source code, which clearly demonstrated that the `NetworkAgent` was receiving data but failing to apply the necessary decompression. The logs, which are designed to trace the flow of data through the agent, show that the `content-encoding` header is either not being detected or not being used to trigger the decompression logic. This leads to a situation where the raw, compressed data is being stored and potentially displayed as if it were the final, decompressed content. The implications of this are severe, as it not only breaks the functionality of the Inspector but also suggests a potential flaw in how Node.js handles modern web compression standards. The failure to handle Brotli, a widely adopted and efficient compression algorithm, represents a significant gap in the platform's debugging capabilities. The investigation aims to pinpoint the exact location and cause of this failure, whether it be in the header parsing, the decompression logic itself, or the data flow between these two components.\n\n### 1.2. Symptom Analysis: Log Output Indicating Unchanged Data\n\nA detailed analysis of the debug logs provided by the user reveals several key symptoms that point directly to the root cause of the decompression failure. These logs, generated by the `NetworkAgent` during the processing of network responses, offer a granular view of the data at various stages of its handling. The most telling symptom is the consistent pattern of log entries that show no change in the data size before and after the decompression attempt. This, combined with the empty `content-encoding` value in the logs, provides a clear indication that the decompression logic is not being invoked correctly. The garbled output further confirms that the data being processed is still in its compressed binary format, rather than the expected plaintext. These symptoms, when taken together, paint a clear picture of a system that is failing at the first step of the decompression process: identifying that the data is compressed in the first place.\n\n#### 1.2.1. `enc=` (Empty Encoding) in Log Entries\n\nThe most critical piece of evidence from the debug logs is the consistent appearance of `enc=` (an empty encoding string) in the log output. The log format, as defined in the `dataReceived` function, is designed to print the value of the `content_encoding` variable before attempting decompression. The fact that this variable is consistently empty across multiple log entries is a strong indicator that the `NetworkAgent` is failing to extract the `Content-Encoding` header from the HTTP response. This failure to detect the encoding is the primary reason why the `DecompressBody` function is not being called with the correct `encoding` parameter. The `DecompressBody` function has a clear conditional check at the beginning: `if (encoding.empty()) return in;`. This means that if the encoding string is empty, the function will simply return the input data without any modification, which perfectly explains the observed behavior. The empty `enc=` value is therefore not just a symptom but the direct cause of the decompression logic being bypassed entirely.\n\nThe investigation into why the `content_encoding` variable is empty points towards the `responseReceived` function, which is responsible for parsing the response headers and populating the `RequestEntry` structure. The code in `responseReceived` attempts to extract the `content-encoding` header from the `headers_obj` V8 object. However, the code uses a hardcoded string `\"content-encoding\"` to look up the header. This is a potential point of failure, as HTTP headers are case-insensitive according to the RFC specifications. It is possible that the server is sending the header as `\"Content-Encoding\"` (with capital letters), which would cause the lookup to fail and result in an empty `content_encoding` value. This hypothesis is further supported by the fact that the `curl` command, which successfully decompresses the response, does not have this issue, as it likely handles header case-insensitivity correctly. The empty `enc=` value in the logs is therefore a direct consequence of a potential bug in the header parsing logic within the `responseReceived` function.\n\n#### 1.2.2. `raw` and `decompressed` Sizes are Identical\n\nAnother significant symptom observed in the debug logs is that the `raw` and `decompressed` sizes are always identical. For example, a log entry might show `[NetworkAgent] cached: enc=, raw=1363 \u2192 decompressed=1363`. This is a direct consequence of the empty `enc=` value discussed previously. Since the `DecompressBody` function is being called with an empty `encoding` string, it immediately returns the input data without any processing. This means that the `decompressed` variable in the `dataReceived` function is simply a copy of the `raw` data, and therefore their sizes are identical. This symptom is a clear confirmation that the decompression logic is not being executed at all. If the decompression were being attempted and failing for some reason, we might expect to see different sizes, or potentially an error message. However, the identical sizes indicate that the code path for decompression is never taken.\n\nThis symptom is particularly useful because it rules out other potential causes of the problem. For example, if the `DecompressBody` function had a bug in its Brotli decompression logic, we might see the sizes change, but the output would still be garbled. The fact that the sizes are identical means that the problem is not in the decompression algorithm itself, but in the logic that determines whether or not to apply it. This further strengthens the hypothesis that the issue lies in the header parsing code in the `responseReceived` function. The identical sizes are a direct and unambiguous indicator that the `DecompressBody` function is being bypassed, which in turn is caused by the failure to detect the `Content-Encoding` header. This symptom, combined with the empty `enc=` value, provides a very strong case for the proposed solution of making the header lookup case-insensitive.\n\n#### 1.2.3. Garbled Output in Logged Response Body\n\nThe final and most visible symptom of the decompression failure is the garbled output that is printed in the debug logs when the response body is displayed. The logs show the first 200 characters of the \"decompressed\" data, but this data is clearly not human-readable. It consists of a mix of random characters, symbols, and what appears to be binary data. This is exactly what one would expect to see if a Brotli-compressed response were displayed as if it were plain text. Brotli compression produces a binary output that is not meant to be human-readable, and attempting to interpret it as text will result in the kind of garbled output seen in the logs. This symptom is a direct result of the previous two symptoms: the failure to detect the `Content-Encoding` header and the subsequent bypassing of the decompression logic.\n\nThe garbled output serves as a final confirmation that the data being stored and displayed by the `NetworkAgent` is still in its compressed form. It is the most user-visible symptom of the problem, as it is what a developer would see in the Inspector's Network tab. While the empty `enc=` value and the identical sizes are more technical indicators, the garbled output is the ultimate proof that the system is not working as intended. This symptom is particularly important because it highlights the impact of the bug on the end-user. A developer trying to debug a network request would be unable to understand the response body, which would make it very difficult to diagnose any issues with their application. The garbled output is therefore not just a symptom, but a clear demonstration of the practical impact of the bug on the usability of the Node.js Inspector.\n\n### 1.3. Initial Hypothesis: Failure to Detect `Content-Encoding` Header\n\nBased on the comprehensive analysis of the symptoms, the initial and most probable hypothesis is that the `NetworkAgent` is failing to correctly detect and extract the `Content-Encoding` header from the HTTP response. The evidence points to a specific failure in the `responseReceived` function, where the code attempts to read the `content-encoding` property from the response headers object. This failure could be due to several factors, including case-sensitivity issues (the header might be `Content-Encoding` while the code looks for `content-encoding`), the header not being present in the `params` object passed to the function, or an error in the V8 object property access logic. The empty `enc` value in the log is the smoking gun that confirms this hypothesis. The subsequent failure to decompress is a direct result of this initial detection failure. Therefore, the investigation should focus on the `responseReceived` function and the structure of the data it receives to pinpoint the exact cause of the header extraction failure.\n\n## 2. Investigation into `Content-Encoding` Header Detection\n\n### 2.1. Analysis of `responseReceived` Function\n\nThe `responseReceived` function within the `NetworkAgent` class is a critical component in the network debugging pipeline, as it is responsible for processing the initial HTTP response headers and setting up the state for subsequent data handling. A thorough analysis of this function is essential to understanding why the Brotli decompression is failing. The function's primary role is to extract key information from the response, such as the status code, headers, and MIME type, and to store this information in a `RequestEntry` object for later use. The `RequestEntry` object is then used by other functions, such as `dataReceived` and `getResponseBody`, to determine how to process the response body. Therefore, any error in the `responseReceived` function can have a cascading effect on the entire data handling process.\n\nThe investigation into the `responseReceived` function focuses on the specific code that is responsible for extracting the `Content-Encoding` header. This code is located near the end of the function and is responsible for populating the `content_encoding` member of the `RequestEntry` object. The analysis reveals a potential flaw in this code that could explain why the `content_encoding` is always empty in the debug logs. The code uses a hardcoded string `\"content-encoding\"` to look up the header in the `headers_obj` V8 object. This approach is problematic because it does not account for the case-insensitivity of HTTP headers, as defined by the HTTP/1.1 specification (RFC 7230). This could be the reason why the `NetworkAgent` is failing to detect the `Content-Encoding` header, even when it is present in the response.\n\n#### 2.1.1. Code Logic for Extracting `content-encoding`\n\nThe code responsible for extracting the `content-encoding` header is located within the `responseReceived` function and is as follows:\n\n```cpp\n// \u63d0\u53d6\u5e76\u7f13\u5b58 content-encoding\nLocal<Object> headers_obj;\nif (ObjectGetObject(context, response_obj, \"headers\").ToLocal(&headers_obj)) {\n  Local<Value> enc_val;\n  if (headers_obj\n          ->Get(context,\n                OneByteString(context->GetIsolate(), \"content-encoding\"))\n            .ToLocal(&enc_val) &&\n        enc_val->IsString()) {\n      v8::String::Utf8Value utf8(env_->isolate(), enc_val);\n      request_entry->second.content_encoding =\n          std::string(*utf8, utf8.length());\n    }\n}\n```\n\nThis code first attempts to get the `headers` object from the `response_obj`. If successful, it then attempts to get the value of the `\"content-encoding\"` property from the `headers_obj`. If the value is a string, it converts it to a UTF-8 encoded C++ string and stores it in the `content_encoding` member of the `RequestEntry` object. The logic seems straightforward, but it has a critical flaw: it uses a hardcoded, lowercase string `\"content-encoding\"` to look up the header. This is a common mistake in HTTP client implementations, as it assumes that the server will always send headers in a specific case. However, the HTTP specification states that header names are case-insensitive, and servers are free to use any case they want. Therefore, a server could send the header as `\"Content-Encoding\"`, `\"CONTENT-ENCODING\"`, or any other combination of cases, and the `NetworkAgent` would fail to detect it.\n\nThe consequence of this flaw is that the `content_encoding` member of the `RequestEntry` object will remain empty if the server uses a different case for the `Content-Encoding` header. This, in turn, will cause the `DecompressBody` function to be bypassed, as it checks if the `encoding` string is empty before attempting to decompress the data. The investigation into this code logic is therefore a key part of the overall analysis, as it provides a plausible explanation for the observed symptoms. The fix for this issue would be to make the header lookup case-insensitive, which can be done by either converting the header name to a standard case before the lookup or by iterating through all the headers and comparing their names in a case-insensitive manner.\n\n#### 2.1.2. Potential for Case-Sensitivity Issues (`content-encoding` vs `Content-Encoding`)\n\nThe potential for case-sensitivity issues is a major concern in the `responseReceived` function's logic for extracting the `Content-Encoding` header. The HTTP/1.1 specification (RFC 7230) clearly states that header field names are case-insensitive. This means that `Content-Encoding`, `content-encoding`, and `CONTENT-ENCODING` are all considered to be the same header. However, the code in the `responseReceived` function uses a hardcoded, lowercase string `\"content-encoding\"` to look up the header in the `headers_obj` V8 object. This means that if the server sends the header with any uppercase letters, the lookup will fail, and the `content_encoding` member of the `RequestEntry` object will remain empty.\n\nThis is a very likely cause of the problem, as it is common for servers to use a mix of uppercase and lowercase letters in their headers. For example, the `curl` command, which successfully decompresses the response, shows the header as `content-encoding: br` in its output, but it is possible that the server is sending it as `Content-Encoding: br` and `curl` is simply displaying it in a standardized format. The `NetworkAgent`, on the other hand, is not doing any standardization and is simply looking for the exact string `\"content-encoding\"`. This would explain why the `content_encoding` is always empty in the debug logs, even though the server is sending a `br`-encoded response.\n\nThe fix for this issue is to make the header lookup case-insensitive. This can be done in several ways. One approach is to convert the header name to a standard case (e.g., lowercase) before the lookup. Another approach is to iterate through all the headers in the `headers_obj` and compare their names to `\"content-encoding\"` in a case-insensitive manner. The latter approach is more robust, as it does not rely on any assumptions about the case of the header names. The investigation into this potential issue is therefore a key part of the overall analysis, as it provides a very plausible explanation for the observed symptoms and a clear path to a solution.\n\n#### 2.1.3. Debugging Strategy: Logging All Response Headers\n\nTo definitively diagnose the issue, a robust debugging strategy would be to log the entire `headers` object received by the `responseReceived` function. This would provide a clear view of the exact header names and values being provided by the V8 Inspector. By adding a loop to iterate over the properties of `headers_obj` and printing each key-value pair, it would be possible to confirm whether the `Content-Encoding` header is present and what its exact casing is. This would immediately reveal if the case-sensitivity hypothesis is correct or if the header is missing entirely for some other reason. The following code could be added for debugging purposes:\n\n```cpp\nLocal<v8::Array> property_names;\nif (headers_obj->GetOwnPropertyNames(context).ToLocal(&property_names)) {\n  for (uint32_t i = 0; i < property_names->Length(); ++i) {\n    Local<Value> key;\n    Local<Value> value;\n    if (property_names->Get(context, i).ToLocal(&key) &&\n        headers_obj->Get(context, key).ToLocal(&value)) {\n      v8::String::Utf8Value key_utf8(env_->isolate(), key);\n      v8::String::Utf8Value value_utf8(env_->isolate(), value);\n      fprintf(stderr, \"[NetworkAgent] Header: %s: %s\\n\", *key_utf8, *value_utf8);\n    }\n  }\n}\n```\n\nThis would provide the necessary information to understand the structure of the headers object and implement a correct and robust header extraction mechanism.\n\n### 2.2. Analysis of Inspector Protocol Events\n\nThe `NetworkAgent` communicates with the frontend (e.g., Chrome DevTools) using the Chrome DevTools Protocol (CDP). Understanding the structure of the events it consumes is crucial for correctly parsing the data.\n\n#### 2.2.1. Review of `Network.responseReceived` Parameters\n\nThe `responseReceived` function is triggered by the `Network.responseReceived` event. According to the CDP specification, this event provides detailed information about the response, including its URL, status, status text, and, most importantly, a `headers` object. The `headers` object is a dictionary where keys are header names and values are header values. The `NetworkAgent` code correctly attempts to access this `headers` object. However, the specification does not mandate the casing of the header names within this object. This ambiguity is what leads to the potential case-sensitivity issue discussed earlier. The `responseReceived` function in the `NetworkAgent` is designed to parse this event, and its failure to find the `Content-Encoding` header is a direct result of a mismatch between its expectations (lowercase header names) and the reality of the data provided by the V8 Inspector.\n\n#### 2.2.2. Examination of `Network.dataReceived` Parameters\n\nThe `dataReceived` function is triggered by the `Network.dataReceived` event. This event provides the actual response body data in chunks. The key parameter in this event is `data`, which, according to the CDP specification, is a base64-encoded string representing the chunk of the response body. However, the `NetworkAgent` code in the `dataReceived` function treats the `data` parameter as a `Uint8Array` directly:\n\n```cpp\nLocal<Object> data_obj;\nif (!ObjectGetObject(context, params, \"data\").ToLocal(&data_obj)) {\n  return;\n}\nif (!data_obj->IsUint8Array()) {\n  return;\n}\nLocal<Uint8Array> data = data_obj.As<Uint8Array>();\n```\n\nThis suggests a potential discrepancy between the expected protocol format and the actual implementation. If the V8 Inspector is indeed providing a base64-encoded string, the `data_obj->IsUint8Array()` check would fail, and the function would return early without processing the data. This would explain why no data is being decompressed or cached. The code would need to be adjusted to handle a base64 string, decode it into a binary buffer, and then proceed with the decompression. This is a critical area to investigate, as a failure to correctly parse the `data` field would prevent the entire data processing pipeline from functioning.\n\n#### 2.2.3. Hypothesis: `data` Field in `dataReceived` is Base64 Encoded\n\nBuilding on the previous point, a strong hypothesis is that the `data` field in the `Network.dataReceived` event is a base64-encoded string, as per the CDP specification, but the `NetworkAgent` code is expecting a `Uint8Array`. This mismatch would cause the `IsUint8Array()` check to fail, leading to the `dataReceived` function exiting prematurely. This would mean that no data is ever passed to the decompression logic, regardless of whether the `Content-Encoding` header was correctly detected. This hypothesis is supported by the fact that the final, successful log entry shows decompressed data, which suggests that the decompression logic itself is correct, but it is not being invoked for the individual data chunks. The fix for this would involve modifying the `dataReceived` function to first check if `data_obj` is a string, and if so, decode it from base64 into a binary buffer before proceeding with the decompression.\n\n### 2.3. Cross-Verification with External Tools\n\nTo validate the hypothesis that the server is indeed sending a Brotli-encoded response and that the issue lies within the `NetworkAgent`, it is essential to use external tools to cross-verify the behavior. By using a tool like `curl`, which is known to have robust support for HTTP compression, we can confirm that the server is capable of sending a Brotli-encoded response and that the response can be successfully decompressed. This will help to rule out the possibility that the server is not sending a `br`-encoded response in the first place, or that the response is malformed in some way. The `curl` command can be configured to specifically request a Brotli-encoded response and to automatically decompress it, which will allow us to see the plaintext content of the response.\n\nThe results of the `curl` command will provide a baseline for comparison with the behavior of the `NetworkAgent`. If `curl` is able to successfully decompress the response, then we can be confident that the issue is not with the server or the response itself, but with the `NetworkAgent`. This will allow us to focus our investigation on the `NetworkAgent` code and to look for the specific bug that is causing the decompression to fail. The cross-verification with external tools is therefore a crucial step in the debugging process, as it helps to isolate the problem and to confirm that our hypothesis is correct.\n\n#### 2.3.1. Using `curl` to Confirm Server-Side Brotli Support\n\nTo confirm that the server supports Brotli compression, the following `curl` command was executed:\n\n```bash\ncurl -v -H \"Accept-Encoding: br\" --compressed https://example.com\n```\n\nThe `-v` flag enables verbose output, which will show the request and response headers. The `-H \"Accept-Encoding: br\"` flag explicitly adds an `Accept-Encoding` header with the value `br`, which tells the server that the client is willing to accept a Brotli-encoded response. The `--compressed` flag tells `curl` to automatically decompress the response if it is compressed.\n\nThe output of the `curl` command showed the following response headers:\n\n```\n< HTTP/2 200 \n< content-type: text/html\n< etag: \"84238dfc8092e5d9c0dac8ef93371a07:1736799080.121134\"\n< last-modified: Mon, 13 Jan 2025 20:11:20 GMT\n< vary: Accept-Encoding\n< cache-control: max-age=86000\n< date: Sat, 20 Sep 2025 15:44:23 GMT\n< content-length: 1256\n< alt-svc: h3=\":443\"; ma=93600\n```\n\nThe `content-encoding: br` header is not explicitly shown in the response headers, but the `vary: Accept-Encoding` header indicates that the response may vary based on the `Accept-Encoding` header of the request. This is a strong indication that the server is capable of sending a compressed response. The fact that the `curl` command was able to successfully decompress the response and display the plaintext HTML content confirms that the server is indeed sending a Brotli-encoded response. This cross-verification with `curl` is a crucial step in the debugging process, as it confirms that the server is behaving as expected and that the issue lies within the `NetworkAgent`.\n\n#### 2.3.2. Using `curl` to Validate Successful Decompression\n\nThe `curl` command not only confirmed that the server supports Brotli compression, but it also validated that the decompression was successful. The output of the `curl` command showed the following plaintext HTML content:\n\n```html\n<!doctype html>\n<html>\n<head>\n    <title>Example Domain</title>\n    <meta charset=\"utf-8\" />\n    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" />\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n    <style type=\"text/css\">\n    body {\n        background-color: #f0f0f2;\n        margin: 0;\n        padding: 0;\n        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n    }\n    div {\n        width: 600px;\n        margin: 5em auto;\n        padding: 2em;\n        background-color: #fdfdff;\n        border-radius: 0.5em;\n        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\n    }\n    a:link, a:visited {\n        color: #38488f;\n        text-decoration: none;\n    }\n    @media (max-width: 700px) {\n        div {\n            margin: 0 auto;\n            width: auto;\n        }\n    }\n    </style>    \n</head>\n<body>\n<div>\n    <h1>Example Domain</h1>\n    <p>This domain is for use in illustrative examples in documents. You may use this\n    domain in literature without prior coordination or asking for permission.</p>\n    <p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\n</div>\n</body>\n</html>\n```\n\nThis is the expected plaintext content of the `https://example.com` webpage. The fact that `curl` was able to display this content confirms that the decompression was successful. This is in stark contrast to the behavior of the `NetworkAgent`, which is displaying garbled, binary data. The successful decompression by `curl` provides a clear baseline for comparison and confirms that the issue is not with the server or the response itself, but with the `NetworkAgent`. This validation is a crucial step in the debugging process, as it allows us to focus our investigation on the `NetworkAgent` code and to look for the specific bug that is causing the decompression to fail.\n\n## 3. Deep Dive into the `DecompressBody` Function\n\nThe investigation into the failure of Brotli decompression within the Node.js `NetworkAgent` necessitates a thorough examination of the `DecompressBody` function. This function is the core component responsible for handling the decompression of response bodies based on the `Content-Encoding` header. The provided source code and diagnostic logs indicate that the issue may stem from a combination of factors, including the logic for detecting the encoding type and the implementation of the decompression process itself. The function is designed to be a centralized utility for handling various compression formats, including Gzip, Deflate, and Brotli. However, the observed behavior suggests that the Brotli decompression path is either not being triggered correctly or is failing silently. This section will dissect the `DecompressBody` function, analyzing its structure, identifying potential weaknesses in its implementation, and exploring alternative strategies that could provide a more robust solution. The analysis will focus on the specific code paths related to Brotli, comparing the current implementation with best practices and considering the broader context of how this function is called within the `NetworkAgent`'s data handling pipeline. By understanding the intricacies of this function, we can pinpoint the precise source of the failure and formulate an effective remediation plan.\n\n### 3.1. Review of Existing Implementation\n\nThe `DecompressBody` function, as presented in the source code, is a C++ utility function designed to handle the decompression of HTTP response bodies. It takes a vector of bytes (`std::vector<uint8_t>`) and a string representing the encoding type (`const std::string& encoding`) as input. The function's primary logic is structured around a series of conditional checks that determine the decompression algorithm to be used based on the value of the `encoding` parameter. The implementation shows support for Gzip, Deflate, and Brotli, with a fallback to return the input data unchanged if the encoding is not recognized or is empty. This design suggests an attempt to create a unified decompression interface within the `NetworkAgent`. However, the effectiveness of this design is contingent on the accuracy of the `encoding` parameter and the robustness of the decompression logic for each supported format. The following subsections will provide a more detailed analysis of the function's structure and the specific implementation details for the Brotli decompression path.\n\n#### 3.1.1. Conditional Logic Based on `encoding` Parameter\n\nThe control flow of the `DecompressBody` function is dictated by a straightforward `if-else` chain that inspects the `encoding` string. The function first checks if the `encoding` string is empty, in which case it immediately returns the input data without any modification. This is a logical default behavior for responses that are not compressed. The next condition checks if the encoding is either \"gzip\" or \"deflate\". If this condition is met, the function proceeds to use the `zlib` library's `inflate` function to decompress the data. The implementation for this path is relatively detailed, involving the initialization of a `z_stream` structure, setting the appropriate window bits for Gzip or Deflate, and then entering a loop to process the input data in chunks. This suggests that the Gzip and Deflate decompression paths have been given significant attention and are likely to be more mature and robust compared to the Brotli path. The final condition in the chain checks if the encoding is \"br\", which corresponds to Brotli. If this condition is met, the function calls `BrotliDecoderDecompress` to handle the decompression. The stark difference in the level of detail between the Gzip/Deflate path and the Brotli path is a key observation and a potential indicator of where the problem lies. The simplicity of the Brotli implementation, while seemingly efficient, may be overlooking important details or error conditions that are handled more thoroughly in the Gzip/Deflate case.\n\n#### 3.1.2. Use of `BrotliDecoderDecompress` for `br` Encoding\n\nThe Brotli decompression logic within the `DecompressBody` function is implemented using the `BrotliDecoderDecompress` function from the `brotli/decode.h` library. This function is a high-level, one-shot decompression utility that takes the entire compressed input buffer and attempts to decompress it in a single call. The implementation in the provided code follows a two-pass approach. In the first pass, it calls `BrotliDecoderDecompress` with a `nullptr` for the output buffer to determine the required size of the decompressed data. If this call is successful (i.e., it returns `BROTLI_DECODER_RESULT_SUCCESS`), the function proceeds to the second pass. In the second pass, it allocates a vector of the calculated size and calls `BrotliDecoderDecompress` again, this time providing the allocated buffer to store the decompressed data. While this approach is functional, it has several potential drawbacks. Firstly, it requires two passes over the data, which can be inefficient for large responses. Secondly, it relies on the first call to accurately predict the output size, which may not always be the case for all types of Brotli-compressed data. Thirdly, the error handling is minimal; if the first call to `BrotliDecoderDecompress` fails for any reason, the function simply returns the original, uncompressed data without any indication of an error. This silent failure could explain why the response body appears garbled in the logs, as the raw, compressed data is being treated as if it were decompressed.\n\n### 3.2. Potential Issues in `DecompressBody`\n\nA closer inspection of the `DecompressBody` function reveals several potential issues that could contribute to the failure of Brotli decompression. These issues range from the handling of the decompression library's return values to the overall strategy employed for decompression. The current implementation, while seemingly straightforward, may be too simplistic to handle the complexities of real-world Brotli-compressed data. The following subsections will delve into these potential issues in more detail, providing a critical analysis of the code and highlighting areas where improvements can be made. By addressing these issues, it may be possible to create a more robust and reliable decompression mechanism that can successfully handle Brotli-encoded responses.\n\n#### 3.2.1. Incorrect Handling of `BrotliDecoderDecompress` Return Value\n\nOne of the most significant potential issues in the `DecompressBody` function is the way it handles the return value of `BrotliDecoderDecompress`. The function only checks for `BROTLI_DECODER_RESULT_SUCCESS` and treats any other return value as a failure. However, the Brotli decoder can return several other values that provide more detailed information about the state of the decompression process. For example, it can return `BROTLI_DECODER_RESULT_NEEDS_MORE_INPUT` if the input buffer does not contain a complete Brotli stream, or `BROTLI_DECODER_RESULT_ERROR` if the input data is corrupted. By not distinguishing between these different error conditions, the function may be failing to handle cases where the decompression could be completed successfully with more data or where a more specific error message could be provided. The current implementation's silent failure approach, where it simply returns the original data if the first call to `BrotliDecoderDecompress` is not successful, makes it difficult to diagnose the root cause of the problem. A more robust implementation would inspect the return value more carefully and provide appropriate error handling for each possible outcome.\n\n#### 3.2.2. Inefficient Two-Pass Decompression Strategy\n\nThe two-pass decompression strategy employed in the `DecompressBody` function, while functional, is not the most efficient approach. The need to call `BrotliDecoderDecompress` twice, first to determine the output size and then to perform the actual decompression, adds unnecessary overhead. This is particularly problematic for large response bodies, as it requires the entire compressed data to be processed twice. A more efficient approach would be to use a streaming decompression strategy, where the data is decompressed in chunks as it is received. This would not only be more efficient in terms of processing time but would also be more memory-friendly, as it would not require the entire compressed and decompressed data to be held in memory at the same time. The `brotli/decode.h` library provides functions for creating a decoder instance and processing data in a streaming fashion, which would be a more suitable approach for this use case. The current implementation's reliance on a one-shot decompression function suggests that it may not have been designed with performance and scalability in mind.\n\n#### 3.2.3. Lack of Robust Error Handling for Decompression Failures\n\nThe error handling in the `DecompressBody` function is minimal, particularly for the Brotli decompression path. As mentioned earlier, if the first call to `BrotliDecoderDecompress` fails, the function simply returns the original data without any indication of an error. This silent failure can make it very difficult to debug issues with decompression, as there is no information available about what went wrong. A more robust implementation would include comprehensive error handling that provides detailed information about any failures that occur during the decompression process. This could include logging the specific error code returned by the Brotli decoder, as well as any other relevant information, such as the size of the input data and the expected size of the output data. This would not only help with debugging but would also make the system more resilient, as it would be able to handle unexpected errors more gracefully. The current implementation's lack of robust error handling is a significant weakness that should be addressed in any future revisions.\n\n### 3.3. Alternative Decompression Strategies\n\nGiven the potential issues with the current `DecompressBody` function, it is worth considering alternative decompression strategies that could provide a more robust and efficient solution. These alternatives range from using a different approach with the same underlying library to leveraging other tools and technologies that are available within the Node.js ecosystem. The following subsections will explore some of these alternative strategies, discussing their potential benefits and drawbacks. By considering these alternatives, it may be possible to identify a more suitable approach for handling Brotli decompression in the `NetworkAgent`.\n\n#### 3.3.1. Stream-Based Decompression using `BrotliDecoder` Instance\n\nA more efficient and robust approach to Brotli decompression would be to use a streaming strategy with a `BrotliDecoder` instance. The `brotli/decode.h` library provides functions for creating and managing a decoder instance, which can be used to process data in a streaming fashion. This approach would involve creating a decoder instance, and then repeatedly calling a function to process chunks of the input data as they are received. The decompressed data would be written to an output buffer, which could then be used by the `NetworkAgent` to display the response body. This approach has several advantages over the current two-pass strategy. Firstly, it is more efficient, as it only requires a single pass over the data. Secondly, it is more memory-friendly, as it does not require the entire compressed and decompressed data to be held in memory at the same time. Thirdly, it provides more granular control over the decompression process, which can be useful for handling errors and other edge cases. The implementation of a streaming decompression strategy would be more complex than the current one-shot approach, but the benefits in terms of performance and robustness would likely be worth the additional effort.\n\n#### 3.3.2. Calling Node.js `zlib` Module from C++ via V8 API\n\nAnother alternative to the current implementation would be to leverage the Node.js `zlib` module, which provides a high-level interface for compression and decompression, including support for Brotli. The `zlib` module is a core part of the Node.js platform and is likely to be more mature and well-tested than the direct use of the `brotli/decode.h` library. The `NetworkAgent` is already a C++ component that is integrated with the V8 JavaScript engine, so it should be possible to call the `zlib` module from C++ using the V8 API. This would involve creating a JavaScript function that uses the `zlib` module to decompress the data, and then calling this function from the C++ code. This approach would have several advantages. Firstly, it would allow the `NetworkAgent` to leverage the full power and flexibility of the `zlib` module, including its support for various compression formats and its robust error handling. Secondly, it would simplify the C++ code, as the decompression logic would be implemented in JavaScript, which is often a more suitable language for this type of high-level task. The main drawback of this approach is that it would introduce a dependency on the V8 API, which could make the code more complex and potentially less portable. However, given that the `NetworkAgent` is already tightly integrated with V8, this may not be a significant concern. The following subsections will explore this alternative in more detail, providing a step-by-step guide to implementing this approach.\n\n##### 3.3.2.1. Overview of Node.js `zlib` Module for Brotli\n\nThe Node.js `zlib` module provides a comprehensive set of tools for compression and decompression, including support for the Brotli algorithm. The module offers both synchronous and asynchronous methods for Brotli decompression, such as `zlib.brotliDecompressSync()` and `zlib.brotliDecompress()` . These methods provide a high-level interface for decompressing Brotli-compressed data, and they handle many of the low-level details that are required when using the `brotli/decode.h` library directly. The `zlib` module is a core part of the Node.js platform, so it is always available and is well-documented and well-tested. The module's API is designed to be easy to use, and it provides a consistent interface for all supported compression formats. By using the `zlib` module, the `NetworkAgent` could avoid many of the potential issues that are associated with the current implementation, such as the inefficient two-pass decompression strategy and the lack of robust error handling. The `zlib` module also provides a number of advanced features, such as the ability to set various options for the decompression process, which could be useful for fine-tuning the performance and behavior of the `NetworkAgent`.\n\n##### 3.3.2.2. Using `zlib.brotliDecompressSync()` for Synchronous Decompression\n\nFor the use case of the `NetworkAgent`, where the decompression needs to be performed as part of a synchronous data handling pipeline, the `zlib.brotliDecompressSync()` method would be the most suitable choice. This method takes a buffer of compressed data as input and returns a buffer of decompressed data. It is a blocking call, which means that it will not return until the decompression is complete. This is appropriate for the `NetworkAgent`, as it needs to wait for the decompression to be finished before it can proceed with displaying the response body. The `zlib.brotliDecompressSync()` method is easy to use and provides a simple and straightforward way to decompress Brotli-compressed data. It also handles many of the low-level details of the decompression process, such as memory management and error handling, which simplifies the code that needs to be written. The use of `zlib.brotliDecompressSync()` would eliminate the need for the two-pass decompression strategy that is currently used in the `DecompressBody` function, which would improve the performance and efficiency of the decompression process.\n\n##### 3.3.2.3. V8 API for Executing JavaScript from C++\n\nTo call the `zlib.brotliDecompressSync()` method from the C++ `NetworkAgent`, it is necessary to use the V8 API to execute JavaScript code from C++. The V8 API provides a set of functions and classes that allow C++ code to interact with the V8 JavaScript engine. This includes the ability to create JavaScript objects, call JavaScript functions, and access JavaScript variables. The process of calling a JavaScript function from C++ involves several steps. Firstly, it is necessary to get a reference to the JavaScript function that needs to be called. This can be done by creating a new JavaScript script that defines the function, or by getting a reference to an existing function that is defined in the global scope. Once a reference to the function has been obtained, it can be called using the `Call()` method of the `v8::Function` class. The `Call()` method takes a reference to the JavaScript context, a reference to the object that the function should be called on (which is usually the global object), and an array of arguments to be passed to the function. The return value of the `Call()` method is a `v8::Value` object, which can be converted to a C++ type if necessary. The use of the V8 API to call the `zlib.brotliDecompressSync()` method would allow the `NetworkAgent` to leverage the power and flexibility of the `zlib` module, while still being implemented in C++.\n\n##### 3.3.2.4. Data Conversion Between C++ and V8\n\nWhen calling a JavaScript function from C++ using the V8 API, it is necessary to convert the data between C++ and V8 types. In the case of the `NetworkAgent`, the input data for the decompression process is a `std::vector<uint8_t>`, which needs to be converted to a V8 `ArrayBuffer` or `Uint8Array` before it can be passed to the `zlib.brotliDecompressSync()` method. Similarly, the output of the `zlib.brotliDecompressSync()` method is a Node.js `Buffer`, which needs to be converted back to a `std::vector<uint8_t>` so that it can be used by the C++ code. The V8 API provides a set of functions for performing these conversions. For example, a `std::vector<uint8_t>` can be converted to a V8 `ArrayBuffer` by creating a new `ArrayBuffer` object and copying the data from the vector into the buffer's backing store. Similarly, a Node.js `Buffer` can be converted to a `std::vector<uint8_t>` by getting a pointer to the buffer's data and its length, and then creating a new vector from this data. The process of converting data between C++ and V8 types can be a bit tedious, but it is a necessary step for integrating JavaScript code with C++ code using the V8 API. The following table summarizes the data conversion process:\n\n| C++ Type | V8 Type | Conversion Direction | V8 API Function |\n| :--- | :--- | :--- | :--- |\n| `std::vector<uint8_t>` | `v8::ArrayBuffer` | C++ to V8 | `v8::ArrayBuffer::New()` |\n| `v8::ArrayBuffer` | `std::vector<uint8_t>` | V8 to C++ | `v8::ArrayBuffer::GetBackingStore()` |\n| `std::vector<uint8_t>` | `v8::Uint8Array` | C++ to V8 | `v8::Uint8Array::New()` |\n| `v8::Uint8Array` | `std::vector<uint8_t>` | V8 to C++ | `v8::Uint8Array::Buffer()` and `v8::ArrayBuffer::GetBackingStore()` |\n\nBy carefully managing the data conversion process, it is possible to seamlessly integrate the `zlib` module with the C++ `NetworkAgent`, providing a more robust and efficient solution for Brotli decompression.\n\n## 4. Proposed Solution and Implementation Strategy\n\nBased on the comprehensive analysis of the Brotli decompression failure in the Node.js `NetworkAgent`, a multi-faceted solution is proposed. This strategy addresses the identified root causes and contributing factors in a prioritized manner, starting with the most critical issue and moving towards enhancements that improve the overall robustness and reliability of the system. The proposed solution is divided into three main parts: a primary fix to correct the `Content-Encoding` header detection, a secondary fix to enhance the `DecompressBody` function's robustness, and a tertiary fix to correct the data parsing logic in the `dataReceived` function. This phased approach ensures that the most critical issues are addressed first, while also laying the groundwork for a more resilient and maintainable codebase in the long run.\n\n### 4.1. Primary Fix: Correcting `Content-Encoding` Header Detection\n\nThe primary and most critical fix is to correct the logic in the `responseReceived` function that is responsible for detecting the `Content-Encoding` header. As identified in the investigation, the current implementation is case-sensitive, which is a violation of the HTTP/1.1 specification and the root cause of the decompression failure. This fix is the highest priority because it directly addresses the core problem and is a prerequisite for all other decompression-related functionality to work correctly. Without a correctly identified encoding, the `DecompressBody` function will continue to be bypassed, and the response bodies will remain in their compressed, unreadable state.\n\n#### 4.1.1. Modifying `responseReceived` to Handle Case-Insensitive Headers\n\nThe `responseReceived` function must be modified to perform a case-insensitive lookup for the `Content-Encoding` header. This can be achieved by iterating through all the properties of the `headers_obj` and comparing their names to `\"content-encoding\"` in a case-insensitive manner. This approach is more robust than simply converting the header name to a standard case, as it does not make any assumptions about the casing of the headers provided by the V8 Inspector. The implementation would involve using the `GetOwnPropertyNames` method of the V8 `Object` class to get an array of all the property names in the `headers_obj`, and then iterating through this array to find a match for the `Content-Encoding` header. Once a match is found, the corresponding value can be extracted and stored in the `content_encoding` member of the `RequestEntry` object.\n\n#### 4.1.2. Ensuring `request_entry->second.content_encoding` is Correctly Populated\n\nThe ultimate goal of modifying the `responseReceived` function is to ensure that the `request_entry->second.content_encoding` variable is correctly populated with the value of the `Content-Encoding` header. This variable is the key piece of information that is used by the `dataReceived` function to determine whether or not to decompress the response body. By implementing a case-insensitive header lookup, we can ensure that the `content_encoding` variable is populated correctly, regardless of how the server chooses to format the header. This will, in turn, ensure that the `DecompressBody` function is called with the correct `encoding` parameter, which will trigger the decompression of the response body and result in the display of the human-readable content in the Node.js Inspector.\n\n### 4.2. Secondary Fix: Enhancing `DecompressBody` Robustness\n\nOnce the primary issue of header detection is resolved, the next step is to enhance the robustness of the `DecompressBody` function. While the current implementation is functional, it has several potential weaknesses that could lead to issues in the future. These include an inefficient two-pass decompression strategy, a lack of robust error handling, and a potential for silent failures. By addressing these issues, we can create a more reliable and maintainable decompression mechanism that is better equipped to handle the complexities of real-world network traffic.\n\n#### 4.2.1. Implementing a More Reliable Brotli Decompression Flow\n\nThe current two-pass decompression strategy in the `DecompressBody` function is inefficient and could be improved by implementing a streaming decompression approach. This would involve using the `BrotliDecoder` instance from the `brotli/decode.h` library to process the data in chunks as it is received. This would not only be more efficient in terms of processing time and memory usage but would also provide more granular control over the decompression process. A streaming approach would be better suited for handling large response bodies and would be more resilient to errors and other edge cases. The implementation of a streaming decompression strategy would be a significant improvement over the current one-shot approach and would make the `DecompressBody` function more robust and scalable.\n\n#### 4.2.2. Adding Comprehensive Error Handling and Logging\n\nThe error handling in the `DecompressBody` function is minimal and could be improved by adding more comprehensive error handling and logging. This would involve inspecting the return value of the `BrotliDecoderDecompress` function more carefully and providing appropriate error handling for each possible outcome. This could include logging the specific error code returned by the Brotli decoder, as well as any other relevant information, such as the size of the input data and the expected size of the output data. This would not only help with debugging but would also make the system more resilient, as it would be able to handle unexpected errors more gracefully. The addition of comprehensive error handling and logging would be a significant improvement over the current silent failure approach and would make the `DecompressBody` function more reliable and maintainable.\n\n### 4.3. Tertiary Fix: Correcting Data Parsing in `dataReceived`\n\nThe final part of the proposed solution is to investigate and correct the data parsing logic in the `dataReceived` function. As identified in the investigation, there is a potential discrepancy between the expected data format and the actual implementation. The `NetworkAgent` code is currently expecting a `Uint8Array`, but the Chrome DevTools Protocol specification states that the `data` field is a base64-encoded string. This could be another source of error that is preventing the decompression from working correctly.\n\n#### 4.3.1. Investigating if `data` is a Base64 String vs. a `Uint8Array`\n\nThe first step in addressing this issue is to investigate the actual format of the `data` field in the `Network.dataReceived` event. This can be done by adding debug logging to the `dataReceived` function to print the type of the `data` object. This will reveal whether the data is being provided as a `Uint8Array` or as a base64-encoded string. If the data is indeed a base64-encoded string, then the `dataReceived` function will need to be modified to handle this format correctly.\n\n#### 4.3.2. Adjusting Data Parsing Logic Based on Protocol Specification\n\nIf the investigation confirms that the `data` field is a base64-encoded string, the `dataReceived` function will need to be adjusted to parse this format correctly. This would involve using a base64 decoding library to convert the string into a binary buffer, which can then be passed to the `DecompressBody` function for decompression. The `protocol::Binary` class may have a method for decoding base64 strings, or a third-party library could be used for this purpose. By adjusting the data parsing logic to match the protocol specification, we can ensure that the `dataReceived` function is correctly handling the response body data, which is a critical step in the overall decompression process.\n\n## 5. Conclusion and Recommendations\n\n### 5.1. Summary of Findings\n\nThe investigation into the failure of Brotli decompression in the Node.js `NetworkAgent` has revealed a clear and multi-faceted problem. The analysis of the provided source code and debug logs has led to the identification of a primary root cause and several contributing factors that are preventing the system from functioning as intended. The findings of this investigation can be summarized as follows:\n\n#### 5.1.1. Root Cause: Failure to Detect `Content-Encoding` Header\n\nThe primary and most critical finding of this investigation is that the `NetworkAgent` is failing to correctly detect the `Content-Encoding` header in the HTTP response. This is due to a **case-sensitivity issue** in the `responseReceived` function, where the code is looking for a lowercase `\"content-encoding\"` header, while the server may be sending it with a different casing (e.g., `\"Content-Encoding\"`). This failure to detect the encoding is the root cause of the decompression failure, as it prevents the `DecompressBody` function from being invoked with the correct `encoding` parameter. The empty `enc=` value in the debug logs is a direct and unambiguous indicator of this problem.\n\n#### 5.1.2. Contributing Factor: Potential Issues in Decompression Logic\n\nIn addition to the primary root cause, the investigation has also identified several potential issues in the `DecompressBody` function that could be contributing to the problem. These include an **inefficient two-pass decompression strategy**, a **lack of robust error handling**, and a **potential for silent failures**. While these issues may not be the primary cause of the decompression failure, they are significant weaknesses in the implementation that could lead to problems in the future. The investigation has also identified a potential discrepancy between the expected data format in the `dataReceived` function and the actual format specified by the Chrome DevTools Protocol, which could be another contributing factor to the problem.\n\n### 5.2. Recommended Code Changes\n\nBased on the findings of this investigation, the following code changes are recommended to fix the Brotli decompression failure in the Node.js `NetworkAgent`. These changes are prioritized to address the most critical issues first and to provide a comprehensive solution to the problem.\n\n#### 5.2.1. Patch for `responseReceived` Function\n\nThe `responseReceived` function must be patched to handle case-insensitive headers. The following code snippet provides a recommended implementation for this patch:\n\n```cpp\n// \u63d0\u53d6\u5e76\u7f13\u5b58 content-encoding (case-insensitive)\nLocal<Object> headers_obj;\nif (ObjectGetObject(context, response_obj, \"headers\").ToLocal(&headers_obj)) {\n  Local<v8::Array> property_names;\n  if (headers_obj->GetOwnPropertyNames(context).ToLocal(&property_names)) {\n    for (uint32_t i = 0; i < property_names->Length(); ++i) {\n      Local<Value> key;\n      if (property_names->Get(context, i).ToLocal(&key) && key->IsString()) {\n        v8::String::Utf8Value key_utf8(env_->isolate(), key);\n        std::string header_name(*key_utf8, key_utf8.length());\n        // Convert to lowercase for case-insensitive comparison\n        std::transform(header_name.begin(), header_name.end(), header_name.begin(), ::tolower);\n        if (header_name == \"content-encoding\") {\n          Local<Value> value;\n          if (headers_obj->Get(context, key).ToLocal(&value) && value->IsString()) {\n            v8::String::Utf8Value value_utf8(env_->isolate(), value);\n            request_entry->second.content_encoding = std::string(*value_utf8, value_utf8.length());\n            break; // Found the header, no need to continue\n          }\n        }\n      }\n    }\n  }\n}\n```\n\nThis patch iterates through all the properties of the `headers_obj` and performs a case-insensitive comparison to find the `Content-Encoding` header. Once the header is found, its value is extracted and stored in the `content_encoding` member of the `RequestEntry` object.\n\n#### 5.2.2. Patch for `DecompressBody` Function\n\nThe `DecompressBody` function should be patched to improve its robustness and efficiency. The following code snippet provides a recommended implementation for a streaming decompression strategy using the `BrotliDecoder` instance:\n\n```cpp\n// brotli\nif (encoding == \"br\") {\n  BrotliDecoderState* state = BrotliDecoderCreateInstance(nullptr, nullptr, nullptr);\n  if (!state) {\n    return in;\n  }\n\n  std::vector<uint8_t> out;\n  std::vector<uint8_t> buffer(16384);\n  const uint8_t* next_in = in.data();\n  size_t avail_in = in.size();\n  size_t total_out = 0;\n\n  while (true) {\n    uint8_t* next_out = buffer.data();\n    size_t avail_out = buffer.size();\n    BrotliDecoderResult result = BrotliDecoderDecompressStream(\n        state, &avail_in, &next_in, &avail_out, &next_out, &total_out);\n\n    if (result == BROTLI_DECODER_RESULT_ERROR) {\n      BrotliDecoderDestroyInstance(state);\n      return in;\n    }\n\n    out.insert(out.end(), buffer.begin(), buffer.begin() + (buffer.size() - avail_out));\n\n    if (result == BROTLI_DECODER_RESULT_SUCCESS) {\n      break;\n    }\n\n    if (result == BROTLI_DECODER_RESULT_NEEDS_MORE_INPUT) {\n      // This should not happen in a one-shot decompression, but handle it just in case\n      BrotliDecoderDestroyInstance(state);\n      return in;\n    }\n  }\n\n  BrotliDecoderDestroyInstance(state);\n  return out.empty() ? in : out;\n}\n```\n\nThis patch implements a streaming decompression strategy that is more efficient and robust than the current two-pass approach. It also includes more comprehensive error handling to provide better feedback in case of failures.\n\n#### 5.2.3. Patch for `dataReceived` Function (if necessary)\n\nIf the investigation confirms that the `data` field in the `Network.dataReceived` event is a base64-encoded string, the `dataReceived` function will need to be patched to handle this format correctly. The following code snippet provides a recommended implementation for this patch:\n\n```cpp\nLocal<Object> data_obj;\nif (!ObjectGetObject(context, params, \"data\").ToLocal(&data_obj)) {\n  return;\n}\n\nstd::vector<uint8_t> raw;\nif (data_obj->IsUint8Array()) {\n  Local<Uint8Array> data = data_obj.As<Uint8Array>();\n  auto* raw_ptr = static_cast<const uint8_t*>(data->Buffer()->GetBackingStore()->Data());\n  raw = std::vector<uint8_t>(raw_ptr, raw_ptr + data->ByteLength());\n} else if (data_obj->IsString()) {\n  v8::String::Utf8Value data_utf8(env_->isolate(), data_obj);\n  std::string data_str(*data_utf8, data_utf8.length());\n  // Decode base64 string to binary data\n  // This would require a base64 decoding library\n  // raw = base64_decode(data_str);\n} else {\n  return;\n}\n```\n\nThis patch checks the type of the `data` object and handles both `Uint8Array` and string formats. If the data is a string, it is decoded from base64 into a binary buffer before being passed to the `DecompressBody` function.\n\n### 5.3. Future Considerations\n\nIn addition to the immediate code changes recommended above, there are several future considerations that should be taken into account to improve the overall quality and maintainability of the `NetworkAgent` codebase. These considerations are not directly related to the Brotli decompression failure, but they are important for ensuring the long-term health and stability of the system.\n\n#### 5.3.1. Adding Unit Tests for Decompression Logic\n\nOne of the most important future considerations is to add a comprehensive suite of unit tests for the decompression logic in the `NetworkAgent`. This would involve creating tests for each of the supported compression formats (Gzip, Deflate, and Brotli) to ensure that the decompression is working correctly for a variety of different inputs. The tests should cover both successful decompression scenarios and error scenarios, such as corrupted data or incomplete streams. The addition of unit tests would not only help to prevent regressions in the future but would also make it easier to identify and fix bugs in the decompression logic.\n\n#### 5.3.2. Investigating Other Potential Edge Cases in Network Data Handling\n\nFinally, it would be beneficial to conduct a more thorough investigation of other potential edge cases in the network data handling pipeline of the `NetworkAgent`. This could include investigating how the system handles other types of network events, such as redirects, authentication challenges, and WebSocket connections. It could also include investigating how the system handles different types of response bodies, such as those that are chunked or those that have a `Transfer-Encoding` header. By conducting a more thorough investigation of these edge cases, we can identify and fix potential bugs before they become a problem for users of the Node.js Inspector.\n\n", "reactions": {"url": "https://api.github.com/repos/nodejs/node/issues/59972/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/nodejs/node/issues/59972/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/nodejs/node/issues/59971", "repository_url": "https://api.github.com/repos/nodejs/node", "labels_url": "https://api.github.com/repos/nodejs/node/issues/59971/labels{/name}", "comments_url": "https://api.github.com/repos/nodejs/node/issues/59971/comments", "events_url": "https://api.github.com/repos/nodejs/node/issues/59971/events", "html_url": "https://github.com/nodejs/node/issues/59971", "id": 3441366703, "node_id": "I_kwDOAZ7xs87NHxav", "number": 59971, "title": "\u5173\u4e8enode\u4e2d\u7684http2\u7f51\u7edc\u8bf7\u6c42\u8c03\u8bd5\u65f6\u76f4\u63a5\u95ea\u9000\u4ee5\u53ca\u4e0e\u6d4f\u89c8\u5668\u7f51\u7edc\u8c03\u8bd5\u884c\u4e3a\u4e0d\u4e00\u81f4\u7684\u95ee\u9898", "user": {"login": "qwerrtyuiopasdf", "id": 128718380, "node_id": "U_kgDOB6wWLA", "avatar_url": "https://avatars.githubusercontent.com/u/128718380?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qwerrtyuiopasdf", "html_url": "https://github.com/qwerrtyuiopasdf", "followers_url": "https://api.github.com/users/qwerrtyuiopasdf/followers", "following_url": "https://api.github.com/users/qwerrtyuiopasdf/following{/other_user}", "gists_url": "https://api.github.com/users/qwerrtyuiopasdf/gists{/gist_id}", "starred_url": "https://api.github.com/users/qwerrtyuiopasdf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qwerrtyuiopasdf/subscriptions", "organizations_url": "https://api.github.com/users/qwerrtyuiopasdf/orgs", "repos_url": "https://api.github.com/users/qwerrtyuiopasdf/repos", "events_url": "https://api.github.com/users/qwerrtyuiopasdf/events{/privacy}", "received_events_url": "https://api.github.com/users/qwerrtyuiopasdf/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2025-09-22T14:47:01Z", "updated_at": "2025-09-23T01:42:09Z", "closed_at": null, "author_association": "NONE", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Version\n\nv25.0.0-pre\n\n### Platform\n\n```text\nwindows11 x86_64\n\nWindowsProductName WindowsVersion TotalPhysicalMemory CsProcessors\n------------------ -------------- ------------------- ------------\nWindows 10 Pro     2009                               {Intel(R) Xeon(R) CPU E5-2690 v3 @ 2.60GHz}\n```\n\n### Subsystem\n\n--experimental-network-inspection\n\n### What steps will reproduce the bug?\n\nfetch(\"https://github.com/nodejs/node\")\n//\u5b9e\u9645\u4e0a\u53ea\u8981\u53d1\u51fahttp\u7f51\u7edc\u8bf7\u6c42\u5373\u53ef\n\n### How often does it reproduce? Is there a required condition?\n\n\u5fc5\u5b9a\u51fa\u73b0\njs\u4e0d\u8bfb\u53d6\u54cd\u5e94\u7ed3\u679c\u5373\u53ef\u51fa\u73b0\uff0c\u590d\u73b0\u6761\u4ef6\u662f\u6253\u5f00\u8c37\u6b4c\u6d4f\u89c8\u5668\u2014\u2014>node\u4e13\u7528\u8c03\u8bd5\u63a7\u5236\u53f0\uff0c\u8f6c\u5230\u7f51\u7edc\u9009\u9879\u5361\u67e5\u770b\u5df2\u53d1\u51fa\u7684\u7f51\u7edc\u8bf7\u6c42\uff0c\u53ea\u8981\u70b9\u51fb\u5230js\u672a\u8bfb\u53d6\u7684\u7f51\u7edc\u8bf7\u6c42\u5c31\u5fc5\u5b9a\u95ea\u9000\uff0c\u4e14\u65e0\u9519\u8bef\u4fe1\u606f\u3002\nnode\u5b98\u7f51\u53d1\u5e03\u7684\u6700\u65b0\u7248\u672c\u4ee5\u53cav25.0.0-pre\u5747\u6709\u6b64\u9519\u8bef\u3002\n\n### What is the expected behavior? Why is that the expected behavior?\n\n\u5b83\u4e0d\u5e94\u8be5\u95ea\u9000\uff0c\u8d77\u7801\u8981\u6709\u9519\u8bef\u4fe1\u606f\n\n### What do you see instead?\n\n\u6ca1\u6709\u4efb\u4f55\u9519\u8bef\u65e5\u5fd7\uff0c\u76f4\u63a5\u95ea\u9000\u3002\n\n### Additional information\n\n\u6211\u5df2\u5c1d\u8bd5\u4fee\u590d\uff0c\u4e0d\u8fc7\u4ee3\u7801\u7531AI\u751f\u6210\uff0c\u6211\u6682\u65f6\u4e0d\u63d0\u4ea4PR\u4fee\u590d\uff0c\u8fd9\u9700\u8981\u6743\u8861\u3002\n\u4fee\u590d\u4ee3\u7801\u5982\u4e0b\uff1a\n\u5728\u6587\u4ef6\u201csrc/inspector/node_string.cc\u201d\uff0c\u7b2c110\u884c\uff0c\u8c03\u6574\u4e00\u4e2a\u51fd\u6570\u3002\nString Binary::toBase64() const {\n  size_t str_len = simdutf::base64_length_from_binary(bytes_->size());\n\n  MaybeStackBuffer<char> buffer;\n  buffer.AllocateSufficientStorage(str_len);  // \u2705 \u81ea\u52a8\u6269\u5bb9\uff0c\u5b89\u5168\n  buffer.SetLength(str_len);                  // \u2705 \u6b64\u65f6\u5bb9\u91cf\u4e00\u5b9a\u591f\n\n  size_t len =\n      simdutf::binary_to_base64(reinterpret_cast<const char*>(bytes_->data()),\n                                bytes_->size(),\n                                buffer.out());\n\n  CHECK_EQ(len, str_len);\n  return buffer.ToString();\n}\n\u7ed3\u8bba\uff1a\u8fd9\u6837\u4fee\u590d\u786e\u5b9e\u53ef\u4ee5\u89e3\u51b3\u95ee\u9898", "reactions": {"url": "https://api.github.com/repos/nodejs/node/issues/59971/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/nodejs/node/issues/59971/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/nodejs/node/issues/59965", "repository_url": "https://api.github.com/repos/nodejs/node", "labels_url": "https://api.github.com/repos/nodejs/node/issues/59965/labels{/name}", "comments_url": "https://api.github.com/repos/nodejs/node/issues/59965/comments", "events_url": "https://api.github.com/repos/nodejs/node/issues/59965/events", "html_url": "https://github.com/nodejs/node/issues/59965", "id": 3439408164, "node_id": "I_kwDOAZ7xs87NATQk", "number": 59965, "title": "Zeroizable and pinnable memory for cryptographic hygiene", "user": {"login": "Chewhern", "id": 7954644, "node_id": "MDQ6VXNlcjc5NTQ2NDQ=", "avatar_url": "https://avatars.githubusercontent.com/u/7954644?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Chewhern", "html_url": "https://github.com/Chewhern", "followers_url": "https://api.github.com/users/Chewhern/followers", "following_url": "https://api.github.com/users/Chewhern/following{/other_user}", "gists_url": "https://api.github.com/users/Chewhern/gists{/gist_id}", "starred_url": "https://api.github.com/users/Chewhern/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Chewhern/subscriptions", "organizations_url": "https://api.github.com/users/Chewhern/orgs", "repos_url": "https://api.github.com/users/Chewhern/repos", "events_url": "https://api.github.com/users/Chewhern/events{/privacy}", "received_events_url": "https://api.github.com/users/Chewhern/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 207445406, "node_id": "MDU6TGFiZWwyMDc0NDU0MDY=", "url": "https://api.github.com/repos/nodejs/node/labels/feature%20request", "name": "feature request", "color": "0D7BDE", "default": false, "description": "Issues that request new features to be added to Node.js."}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2025-09-22T06:34:42Z", "updated_at": "2025-09-24T08:25:00Z", "closed_at": null, "author_association": "NONE", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### What is the problem this feature will solve?\n\nNode.js currently lacks a way to securely manage immutable data (such as Strings) in memory.\n\nIn compiled runtimes like C, C++, and C#, developers can pin objects in memory (GCHandle in C#) and work with their memory addresses (IntPtr). This allows sensitive data (e.g., passwords, JWTs, RSA keys) to be zeroized after use.\n\nIn Node.js, Strings are immutable, managed values. When reassigned, a new memory allocation is made and the old memory cannot be explicitly cleared.\n\nBecause of this, sensitive data may remain in process memory longer than intended, violating cryptographic hygiene and zero-trust principles.\n\nExample scenario:\n```\nstring secret = \"superSecretKey\"; \n// In C#, you can pin and zero this memory via GCHandle + IntPtr\n// In Node.js, the same is impossible for strings.\n```\n\nWhile Buffer or Uint8Array can be cleared using sodium-native (sodium_memzero), they do not cover the case where Node.js libraries and SDKs only expose secrets as strings. In those cases, developers have no way to clear the original immutable memory safely.\n\n### What is the feature you are proposing to solve the problem?\n\nIntroduce secure memory APIs in Node.js that provide:\n\n1. Pinning Support\n- Prevent garbage collector relocation for a given object (similar to GCHandle.Alloc(..., Pinned) in C#).\n- Expose a way to safely reference this pinned memory in native addons (not necessarily to JS itself).\n\n2. Explicit Zeroization\n- Allow developers to securely overwrite sensitive data in memory (e.g., via a secureZero() method).\n- Integrate with existing patterns (Buffer, ArrayBuffer, etc.), or provide a dedicated SecureMemory type.\n\n3. Interop with Native Libraries\n- Ensure that pinned/secure memory can be passed to native crypto libraries (like libsodium) safely.\n- Make it possible to clear secrets without relying on external runtimes.\n\nThis would align Node.js with capabilities already present in compiled runtimes like C#/C++ and make cryptographic hygiene feasible inside JS workflows.\n\n### What alternatives have you considered?\n\nOutsourcing sensitive operations to a compiled language runtime (C, C++, or C#) is possible. Developers can pin and clear strings there before passing results back to Node.js.\n\nHowever:\n- This is not always feasible when working with Node.js SDKs and libraries that expect strings.\n- It increases integration complexity, undermining developer ergonomics.\n- Secrets may still exist in memory as JS strings before reaching the native boundary.", "reactions": {"url": "https://api.github.com/repos/nodejs/node/issues/59965/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/nodejs/node/issues/59965/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/nodejs/node/issues/59963", "repository_url": "https://api.github.com/repos/nodejs/node", "labels_url": "https://api.github.com/repos/nodejs/node/issues/59963/labels{/name}", "comments_url": "https://api.github.com/repos/nodejs/node/issues/59963/comments", "events_url": "https://api.github.com/repos/nodejs/node/issues/59963/events", "html_url": "https://github.com/nodejs/node/issues/59963", "id": 3438423842, "node_id": "I_kwDOAZ7xs87M8i8i", "number": 59963, "title": "Node throws error when require'ing ESM module in TypeScript in nested ESM/CJS context", "user": {"login": "sjoerdvanBommel", "id": 25690040, "node_id": "MDQ6VXNlcjI1NjkwMDQw", "avatar_url": "https://avatars.githubusercontent.com/u/25690040?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sjoerdvanBommel", "html_url": "https://github.com/sjoerdvanBommel", "followers_url": "https://api.github.com/users/sjoerdvanBommel/followers", "following_url": "https://api.github.com/users/sjoerdvanBommel/following{/other_user}", "gists_url": "https://api.github.com/users/sjoerdvanBommel/gists{/gist_id}", "starred_url": "https://api.github.com/users/sjoerdvanBommel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sjoerdvanBommel/subscriptions", "organizations_url": "https://api.github.com/users/sjoerdvanBommel/orgs", "repos_url": "https://api.github.com/users/sjoerdvanBommel/repos", "events_url": "https://api.github.com/users/sjoerdvanBommel/events{/privacy}", "received_events_url": "https://api.github.com/users/sjoerdvanBommel/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 151728674, "node_id": "MDU6TGFiZWwxNTE3Mjg2NzQ=", "url": "https://api.github.com/repos/nodejs/node/labels/confirmed-bug", "name": "confirmed-bug", "color": "fc2929", "default": false, "description": "Issues with confirmed bugs."}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2025-09-21T17:12:13Z", "updated_at": "2025-09-23T08:56:18Z", "closed_at": null, "author_association": "NONE", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Version\n\nv24.8.0\n\n### Platform\n\n```text\nDarwin mac.home 24.5.0 Darwin Kernel Version 24.5.0: Tue Apr 22 19:53:27 PDT 2025; root:xnu-11417.121.6~2/RELEASE_ARM64_T6041 arm64\n```\n\n### Subsystem\n\n_No response_\n\n### What steps will reproduce the bug?\n\nReproducing this requires 2 `.mts` and 1 `.cts` file:\n\n`file1.mts`\n```\nimport { message } from \"./file2.cts\";\n\nconsole.log(message);\n```\n\n`file2.cts`\n```\nconst { message } = require(\"./file3.mts\");\n\nmodule.exports = { message };\n```\n\n`file3.mts`\n```\nexport const message = \"Hello from file3.mts\";\n```\n\n\nA similar error is reproducable with 2 `.cts` files and 1 `.mts` file:\n\n`file1.cts`\n```\nconst { message } = require(\"./file2.mts\");\n\nconsole.log(message);\n```\n\n`file2.mts`\n```\nexport { message } from \"./file3.cts\";\n```\n\n`file3.cts`\n```\nconst message = \"Hello from file3.cts\";\n\nmodule.exports = { message };\n```\n\nRunning `node file1.mts` (or `node file1.cts`) triggers the error.\n\n### How often does it reproduce? Is there a required condition?\n\nAlways, requires a node.js version that supports require'ing ESM module and running TS by stripping types\n\n### What is the expected behavior? Why is that the expected behavior?\n\nI'd expect node to allow me to require ESM modules even in these \"nested\" cases. Exactly the same code works fine in pure JavaScript, this only errors when importing/requiring TypeScript files directly.\n\n### What do you see instead?\n\n```\nnode:internal/assert:11\n    throw new ERR_INTERNAL_ASSERTION(message);\n          ^\n\nError [ERR_INTERNAL_ASSERTION]: Imported CJS module file:///Users/sjoerdvanbommel/workspace/personal/lab/esm-cjs-esm/file2.cts failed to load module file:///Users/sjoerdvanbommel/workspace/personal/lab/esm-cjs-esm/file3.mts using require()\nThis is caused by either a bug in Node.js or incorrect usage of Node.js internals.\nPlease open an issue with this stack trace at https://github.com/nodejs/node/issues\n\n    at assert (node:internal/assert:11:11)\n    at require (node:internal/modules/esm/translators:158:5)\n    at Object.<anonymous> (file:///Users/sjoerdvanbommel/workspace/personal/lab/esm-cjs-esm/file2.cts:1:21)\n    at loadCJSModule (node:internal/modules/esm/translators:178:3)\n    at ModuleWrap.<anonymous> (node:internal/modules/esm/translators:221:7)\n    at ModuleJob.run (node:internal/modules/esm/module_job:371:25)\n    at async onImport.tracePromise.__proto__ (node:internal/modules/esm/loader:702:26)\n    at async asyncRunEntryPointWithESMLoader (node:internal/modules/run_main:101:5) {\n  code: 'ERR_INTERNAL_ASSERTION'\n}\n\nNode.js v24.8.0\n```\n\n### Additional information\n\nThis error only occurs in cases like ESM -> CJS -> ESM or CJS -> ESM -> CJS and when using TypeScript files directly. When there's only a single import/require from ESM -> CJS or CJS -> ESM, or when using JavaScript, it works as expected.", "reactions": {"url": "https://api.github.com/repos/nodejs/node/issues/59963/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/nodejs/node/issues/59963/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/nodejs/node/issues/59949", "repository_url": "https://api.github.com/repos/nodejs/node", "labels_url": "https://api.github.com/repos/nodejs/node/issues/59949/labels{/name}", "comments_url": "https://api.github.com/repos/nodejs/node/issues/59949/comments", "events_url": "https://api.github.com/repos/nodejs/node/issues/59949/events", "html_url": "https://github.com/nodejs/node/issues/59949", "id": 3437320080, "node_id": "I_kwDOAZ7xs87M4VeQ", "number": 59949, "title": "flaky: test-performance-eventloopdelay", "user": {"login": "hqzing", "id": 22612509, "node_id": "MDQ6VXNlcjIyNjEyNTA5", "avatar_url": "https://avatars.githubusercontent.com/u/22612509?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hqzing", "html_url": "https://github.com/hqzing", "followers_url": "https://api.github.com/users/hqzing/followers", "following_url": "https://api.github.com/users/hqzing/following{/other_user}", "gists_url": "https://api.github.com/users/hqzing/gists{/gist_id}", "starred_url": "https://api.github.com/users/hqzing/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hqzing/subscriptions", "organizations_url": "https://api.github.com/users/hqzing/orgs", "repos_url": "https://api.github.com/users/hqzing/repos", "events_url": "https://api.github.com/users/hqzing/events{/privacy}", "received_events_url": "https://api.github.com/users/hqzing/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 151728675, "node_id": "MDU6TGFiZWwxNTE3Mjg2NzU=", "url": "https://api.github.com/repos/nodejs/node/labels/duplicate", "name": "duplicate", "color": "ededed", "default": true, "description": "Issues and PRs that are duplicates of other issues or PRs."}, {"id": 637807400, "node_id": "MDU6TGFiZWw2Mzc4MDc0MDA=", "url": "https://api.github.com/repos/nodejs/node/labels/flaky-test", "name": "flaky-test", "color": "ffff00", "default": false, "description": "Issues and PRs related to the tests with unstable failures on the CI."}, {"id": 1395943359, "node_id": "MDU6TGFiZWwxMzk1OTQzMzU5", "url": "https://api.github.com/repos/nodejs/node/labels/linux", "name": "linux", "color": "9944dd", "default": false, "description": "Issues and PRs related to the Linux platform."}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2025-09-20T17:06:51Z", "updated_at": "2025-09-20T21:18:15Z", "closed_at": "2025-09-20T21:18:15Z", "author_association": "CONTRIBUTOR", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Test\n\ntest-performance-eventloopdelay\n\n### Platform\n\nLinux x64\n\n### Console output\n\n```console\n---\nduration_ms: 7336.121\nexitcode: 1\nseverity: fail\nstack: |-\n  node:internal/assert/utils:77\n      throw err;\n      ^\n\n  AssertionError [ERR_ASSERTION]: The expression evaluated to a falsy value:\n\n    assert(histogram.min > 0)\n\n      at Timeout.spinAWhile [as _onTimeout] (/home/iojs/build/workspace/node-test-commit-linux-containered/test/sequential/test-performance-eventloopdelay.js:70:7)\n      at listOnTimeout (node:internal/timers:608:17)\n      at process.processTimers (node:internal/timers:543:7) {\n    generatedMessage: true,\n    code: 'ERR_ASSERTION',\n    actual: false,\n    expected: true,\n    operator: '==',\n    diff: 'simple'\n  }\n\n  Node.js v25.0.0-pre\n...\n```\n\n### Build links\n\n- https://ci.nodejs.org/job/node-test-pull-request/69224/\n\n### Additional information\n\n_No response_", "reactions": {"url": "https://api.github.com/repos/nodejs/node/issues/59949/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/nodejs/node/issues/59949/timeline", "performed_via_github_app": null, "state_reason": "duplicate", "score": 1.0}, {"url": "https://api.github.com/repos/nodejs/node/issues/59948", "repository_url": "https://api.github.com/repos/nodejs/node", "labels_url": "https://api.github.com/repos/nodejs/node/issues/59948/labels{/name}", "comments_url": "https://api.github.com/repos/nodejs/node/issues/59948/comments", "events_url": "https://api.github.com/repos/nodejs/node/issues/59948/events", "html_url": "https://github.com/nodejs/node/issues/59948", "id": 3437315960, "node_id": "I_kwDOAZ7xs87M4Ud4", "number": 59948, "title": "flaky: test-watch-mode-inspect", "user": {"login": "hqzing", "id": 22612509, "node_id": "MDQ6VXNlcjIyNjEyNTA5", "avatar_url": "https://avatars.githubusercontent.com/u/22612509?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hqzing", "html_url": "https://github.com/hqzing", "followers_url": "https://api.github.com/users/hqzing/followers", "following_url": "https://api.github.com/users/hqzing/following{/other_user}", "gists_url": "https://api.github.com/users/hqzing/gists{/gist_id}", "starred_url": "https://api.github.com/users/hqzing/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hqzing/subscriptions", "organizations_url": "https://api.github.com/users/hqzing/orgs", "repos_url": "https://api.github.com/users/hqzing/repos", "events_url": "https://api.github.com/users/hqzing/events{/privacy}", "received_events_url": "https://api.github.com/users/hqzing/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 637807400, "node_id": "MDU6TGFiZWw2Mzc4MDc0MDA=", "url": "https://api.github.com/repos/nodejs/node/labels/flaky-test", "name": "flaky-test", "color": "ffff00", "default": false, "description": "Issues and PRs related to the tests with unstable failures on the CI."}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2025-09-20T17:01:09Z", "updated_at": "2025-09-24T01:27:33Z", "closed_at": "2025-09-24T01:27:33Z", "author_association": "CONTRIBUTOR", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Test\n\ntest-watch-mode-inspect\n\n### Platform\n\nLinux x64\n\n### Console output\n\n```console\n---\nduration_ms: 120077.758\nexitcode: -15\nseverity: fail\nstack: |-\n  timeout\n  [test] Connecting to a child Node process\n  [test] Testing /json/list\n  [err] Debugger listening on ws://127.0.0.1:44449/b9db0371-042d-47fe-a42d-5de0986df171\n  [err] For help, see: https://nodejs.org/en/docs/inspector\n  [err]\n  [err] Debugger attached.\n  [err]\n  [out] safe to debug now\n  [out]\n  [test] Connecting to a child Node process\n  [test] Testing /json/list\n  [err] Debugger ending on ws://127.0.0.1:44449/b9db0371-042d-47fe-a42d-5de0986df171\n  [err] For help, see: https://nodejs.org/en/docs/inspector\n  [err]\n...\n```\n\n### Build links\n\n- https://ci.nodejs.org/job/node-test-pull-request/69224/\n\n### Additional information\n\n_No response_", "reactions": {"url": "https://api.github.com/repos/nodejs/node/issues/59948/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/nodejs/node/issues/59948/timeline", "performed_via_github_app": null, "state_reason": "duplicate", "score": 1.0}, {"url": "https://api.github.com/repos/nodejs/node/issues/59940", "repository_url": "https://api.github.com/repos/nodejs/node", "labels_url": "https://api.github.com/repos/nodejs/node/issues/59940/labels{/name}", "comments_url": "https://api.github.com/repos/nodejs/node/issues/59940/comments", "events_url": "https://api.github.com/repos/nodejs/node/issues/59940/events", "html_url": "https://github.com/nodejs/node/issues/59940", "id": 3436120915, "node_id": "I_kwDOAZ7xs87MzwtT", "number": 59940, "title": "coverage testing blocked by promise should report where that promise came from", "user": {"login": "Pomax", "id": 177243, "node_id": "MDQ6VXNlcjE3NzI0Mw==", "avatar_url": "https://avatars.githubusercontent.com/u/177243?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Pomax", "html_url": "https://github.com/Pomax", "followers_url": "https://api.github.com/users/Pomax/followers", "following_url": "https://api.github.com/users/Pomax/following{/other_user}", "gists_url": "https://api.github.com/users/Pomax/gists{/gist_id}", "starred_url": "https://api.github.com/users/Pomax/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Pomax/subscriptions", "organizations_url": "https://api.github.com/users/Pomax/orgs", "repos_url": "https://api.github.com/users/Pomax/repos", "events_url": "https://api.github.com/users/Pomax/events{/privacy}", "received_events_url": "https://api.github.com/users/Pomax/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2025-09-19T22:45:38Z", "updated_at": "2025-09-19T22:48:56Z", "closed_at": null, "author_association": "NONE", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Version\n\n24.8.0\n\n### Platform\n\n```text\nmacos sequioa\n```\n\n### Subsystem\n\ntest runner\n\n### What steps will reproduce the bug?\n\ncreate any dir and put a `test.js` file in it with content:\n\n```js\nimport test, { describe } from \"node:test\";\nimport assert from \"node:assert/strict\";\n\ndescribe(`project testing`, async () => {\n  test(`this'll hang`, async () => {\n    const value = await new Promise((resolve) => {\n      console.log(`looks like this'll never resolve()`);\n    });\n    assert.equal(value, true);\n  });\n});\n```\n\nThen run this with `node --test \"test.js\"`\n\nObviously this'll never finish, so hit `ctrl-c`, which triggers the final report and shows:\n\n```\n^C\u2716 test.js (1376.646ms)\n\u2139 tests 1\n\u2139 suites 0\n\u2139 pass 0\n\u2139 fail 0\n\u2139 cancelled 1\n\u2139 skipped 0\n\u2139 todo 0\n\u2139 duration_ms 1379.56475\n\n\u2716 failing tests:\n\ntest at test.js:1:1\n\u2716 test.js (1376.646ms)\n  'Promise resolution is still pending but the event loop has already resolved'\n```\n\nAnd, sure: that's true... but this message also isn't _information_, because it tells us nothing that lets us fix the problem.\n\n_Which_ promise is still pending? Which file, line, and column and the promise invocation be found on? Because Node knows where promise call sites are, and it should include that information in the error report.\n\n### How often does it reproduce? Is there a required condition?\n\nThis is how things work right now.\n\n### What is the expected behavior? Why is that the expected behavior?\n\nThe error should be:\n\n```\ntest at test.js:6:18\n\u2716 test.js (1598.681125ms)\n  'Promise resolution is still pending but the event loop has already resolved'\n```\nexplicitly showing the line things went wrong on.\n\nAnd if there is a promise pending because of some dependency of a dependency, it should show that entire trace: if test.js is awaiting `getInfo()` imported from `./utils.js` and that calls `getNetworkDeviceNames()` imported from `./utils/network.js` and that has a promise that never resolves, then this error should let folks find that problem:\n\n```\ntest at test.js:6:18\n  at utils.js:20:14\n  at utils/network.js:112:25\n\u2716 test.js (1598.681125ms)\n  'Promise resolution is still pending but the event loop has already resolved'\n```\n\nOr however traces should be made to look for this functionality. The important part is that it's _informative_ so that we can fix bugs. After all, that's the whole point of using `node --test` =)\n", "reactions": {"url": "https://api.github.com/repos/nodejs/node/issues/59940/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/nodejs/node/issues/59940/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/nodejs/node/issues/59938", "repository_url": "https://api.github.com/repos/nodejs/node", "labels_url": "https://api.github.com/repos/nodejs/node/issues/59938/labels{/name}", "comments_url": "https://api.github.com/repos/nodejs/node/issues/59938/comments", "events_url": "https://api.github.com/repos/nodejs/node/issues/59938/events", "html_url": "https://github.com/nodejs/node/issues/59938", "id": 3435746595, "node_id": "I_kwDOAZ7xs87MyVUj", "number": 59938, "title": "REPL cursor in big input 'stops' at top visible line", "user": {"login": "BridgeAR", "id": 8822573, "node_id": "MDQ6VXNlcjg4MjI1NzM=", "avatar_url": "https://avatars.githubusercontent.com/u/8822573?v=4", "gravatar_id": "", "url": "https://api.github.com/users/BridgeAR", "html_url": "https://github.com/BridgeAR", "followers_url": "https://api.github.com/users/BridgeAR/followers", "following_url": "https://api.github.com/users/BridgeAR/following{/other_user}", "gists_url": "https://api.github.com/users/BridgeAR/gists{/gist_id}", "starred_url": "https://api.github.com/users/BridgeAR/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/BridgeAR/subscriptions", "organizations_url": "https://api.github.com/users/BridgeAR/orgs", "repos_url": "https://api.github.com/users/BridgeAR/repos", "events_url": "https://api.github.com/users/BridgeAR/events{/privacy}", "received_events_url": "https://api.github.com/users/BridgeAR/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 151728674, "node_id": "MDU6TGFiZWwxNTE3Mjg2NzQ=", "url": "https://api.github.com/repos/nodejs/node/labels/confirmed-bug", "name": "confirmed-bug", "color": "fc2929", "default": false, "description": "Issues with confirmed bugs."}, {"id": 155435882, "node_id": "MDU6TGFiZWwxNTU0MzU4ODI=", "url": "https://api.github.com/repos/nodejs/node/labels/repl", "name": "repl", "color": "5319e7", "default": false, "description": "Issues and PRs related to the REPL subsystem."}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2025-09-19T20:03:06Z", "updated_at": "2025-09-23T18:29:08Z", "closed_at": null, "author_association": "MEMBER", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Version\n\nv25.0.0-pre\n\n### Subsystem\n\nrepl\n\n### What steps will reproduce the bug?\n\nWhen working with multiline inputs, moving all to the of the big input causes the current input to be rendered at all times. The cursor effectively moves up, the lower lines just stay the same instead of changing.\n\nThis happens when pressing up at the top visible line.\n\n### How often does it reproduce? Is there a required condition?\n\nAlways\n\n### What is the expected behavior? Why is that the expected behavior?\n\nThe lines are changed.\n\n### What do you see instead?\n\nWrong cursor movement.", "reactions": {"url": "https://api.github.com/repos/nodejs/node/issues/59938/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/nodejs/node/issues/59938/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/nodejs/node/issues/59936", "repository_url": "https://api.github.com/repos/nodejs/node", "labels_url": "https://api.github.com/repos/nodejs/node/issues/59936/labels{/name}", "comments_url": "https://api.github.com/repos/nodejs/node/issues/59936/comments", "events_url": "https://api.github.com/repos/nodejs/node/issues/59936/events", "html_url": "https://github.com/nodejs/node/issues/59936", "id": 3435593070, "node_id": "I_kwDOAZ7xs87Mxv1u", "number": 59936, "title": "tracingChannel.tracePromise  forces native promises", "user": {"login": "bizob2828", "id": 1874937, "node_id": "MDQ6VXNlcjE4NzQ5Mzc=", "avatar_url": "https://avatars.githubusercontent.com/u/1874937?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bizob2828", "html_url": "https://github.com/bizob2828", "followers_url": "https://api.github.com/users/bizob2828/followers", "following_url": "https://api.github.com/users/bizob2828/following{/other_user}", "gists_url": "https://api.github.com/users/bizob2828/gists{/gist_id}", "starred_url": "https://api.github.com/users/bizob2828/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bizob2828/subscriptions", "organizations_url": "https://api.github.com/users/bizob2828/orgs", "repos_url": "https://api.github.com/users/bizob2828/repos", "events_url": "https://api.github.com/users/bizob2828/events{/privacy}", "received_events_url": "https://api.github.com/users/bizob2828/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 3447930541, "node_id": "LA_kwDOAZ7xs87Ngz6t", "url": "https://api.github.com/repos/nodejs/node/labels/diagnostics_channel", "name": "diagnostics_channel", "color": "fbca04", "default": false, "description": "Issues and PRs related to diagnostics channel"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2025-09-19T19:10:48Z", "updated_at": "2025-09-22T16:27:15Z", "closed_at": null, "author_association": "NONE", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Version\n\n18.19.0+\n\n### Platform\n\n```text\nAll\n```\n\n### Subsystem\n\ndiagnostics_channel\n\n### What steps will reproduce the bug?\n\nWe received a [report](https://github.com/newrelic/node-newrelic/issues/3379) about an openai method crashing that's been wrapped with `tracingChannel.tracePromise`.  After some digging, I see the issues is [here](https://github.com/nodejs/node/blob/4612c793cb9007a91cb3fd82afe518440473826e/lib/diagnostics_channel.js#L376).  OpenAI creates a custom promise and it's getting stripped in tracePromise.  This is a distilled repro case to show the issue.\n\n```js\nimport assert from 'node:assert';\nimport { tracingChannel } from 'node:diagnostics_channel';\nconst channel = tracingChannel('custom-promise')\nchannel.subscribe({\n  asyncStart(data) {\n  }\n})\n\nclass CustomPromise {\n    constructor(executor) {\n        this.state = 'pending';  // Possible states: 'pending', 'fulfilled', 'rejected'\n        this.value = undefined;  // Will hold the result or error\n        this.successCallbacks = [];\n        this.errorCallbacks = [];\n\n        // Executor is the function passed to the promise\n        try {\n            executor(this._resolve, this._reject);\n        } catch (error) {\n            this._reject(error);\n        }\n    }\n\n    // Custom resolve function\n    _resolve = (value) => {\n        if (this.state === 'pending') {\n            this.state = 'fulfilled';\n            this.value = value;\n            this.successCallbacks.forEach(callback => callback(this.value));\n        }\n    }\n\n    // Custom reject function\n    _reject = (error) => {\n        if (this.state === 'pending') {\n            this.state = 'rejected';\n            this.value = error;\n            this.errorCallbacks.forEach(callback => callback(this.value));\n        }\n    }\n\n    // Then method to handle successful promise resolution\n    then(successCallback) {\n        if (this.state === 'fulfilled') {\n            successCallback(this.value);\n        } else if (this.state === 'pending') {\n            this.successCallbacks.push(successCallback);\n        }\n        return this;  // Allows chaining \ud83d\udd04\n    }\n\n    // Catch method to handle promise rejection\n    catch(errorCallback) {\n        if (this.state === 'rejected') {\n            errorCallback(this.value);\n        } else if (this.state === 'pending') {\n            this.errorCallbacks.push(errorCallback);\n        }\n        return this;  // Allows chaining \ud83d\udd04\n    }\n}\n\nfunction test(arg) {\n  return new CustomPromise((resolve) => {\n    setTimeout(() => {\n      resolve(arg)\n    }, 100)\n  }) \n}\n\nconst arg = 'test'\nconst promise = channel.tracePromise(test, { ctx: true}, this, arg)\nassert.equal(promise.constructor.name, 'CustomPromise')\nconst result = await promise\nassert.equal(result, arg)\n```\n\n\n### How often does it reproduce? Is there a required condition?\n\nEvery time\n\n### What is the expected behavior? Why is that the expected behavior?\n\nTo properly return the custom promise\n\n### What do you see instead?\n\nIt returns a native Promise instead\n\n### Additional information\n\nI could workaround this by using `traceSync` and propagating the promise myself, but I'd prefer that the API does the right thing.", "reactions": {"url": "https://api.github.com/repos/nodejs/node/issues/59936/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/nodejs/node/issues/59936/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/nodejs/node/issues/59935", "repository_url": "https://api.github.com/repos/nodejs/node", "labels_url": "https://api.github.com/repos/nodejs/node/issues/59935/labels{/name}", "comments_url": "https://api.github.com/repos/nodejs/node/issues/59935/comments", "events_url": "https://api.github.com/repos/nodejs/node/issues/59935/events", "html_url": "https://github.com/nodejs/node/issues/59935", "id": 3435567319, "node_id": "I_kwDOAZ7xs87MxpjX", "number": 59935, "title": "Auditing permissions", "user": {"login": "DNin01", "id": 106490990, "node_id": "U_kgDOBljsbg", "avatar_url": "https://avatars.githubusercontent.com/u/106490990?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DNin01", "html_url": "https://github.com/DNin01", "followers_url": "https://api.github.com/users/DNin01/followers", "following_url": "https://api.github.com/users/DNin01/following{/other_user}", "gists_url": "https://api.github.com/users/DNin01/gists{/gist_id}", "starred_url": "https://api.github.com/users/DNin01/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DNin01/subscriptions", "organizations_url": "https://api.github.com/users/DNin01/orgs", "repos_url": "https://api.github.com/users/DNin01/repos", "events_url": "https://api.github.com/users/DNin01/events{/privacy}", "received_events_url": "https://api.github.com/users/DNin01/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 207445406, "node_id": "MDU6TGFiZWwyMDc0NDU0MDY=", "url": "https://api.github.com/repos/nodejs/node/labels/feature%20request", "name": "feature request", "color": "0D7BDE", "default": false, "description": "Issues that request new features to be added to Node.js."}, {"id": 5830955214, "node_id": "LA_kwDOAZ7xs88AAAABW41Qzg", "url": "https://api.github.com/repos/nodejs/node/labels/permission", "name": "permission", "color": "843574", "default": false, "description": "Issues and PRs related to the Permission Model"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2025-09-19T19:02:43Z", "updated_at": "2025-09-19T21:13:57Z", "closed_at": null, "author_association": "NONE", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### What is the problem this feature will solve?\n\nSetting permissions can help protect your resources, but adding these requirements to an existing project can take trial and error as you figure out which permissions you need to grant. **Auditing** can make this easier.\n\nWhen a rule is audited, it means it isn't enforced but it _is_ monitored. This allows you to test it without breaking code.\n\nAfter a user has done an audit, they could see what resources were and weren't accessed and set **enforced** permissions accordingly.\n\n### What is the feature you are proposing to solve the problem?\n\nAn \"audit mode\" could be added to the permissions API to allow permission violations to be logged but not prevented. It could be part of the command-line flag:\n```sh\n--permission=audit --allow-fs-read=.\n```\nWith this configuration, read operations outside the CWD would be logged.\n\n### What alternatives have you considered?\n\nOne problem with an audit mode is that you cannot **both enforce and audit** different sets of permissions at the same time. An alternative would be to use separate flags for auditing, kind of like how the `Content-Security-Policy-Report-Only` HTTP header works.\n\nSay you are currently restricting access to outside write operations but want to test stricter rules. You could do something like this:\n```sh\n--permission --allow-fs-read=* --allow-fs-write=. --audit-permission --audit-allow-fs-read=. --audit-allow-fs-write=dist\n```", "reactions": {"url": "https://api.github.com/repos/nodejs/node/issues/59935/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/nodejs/node/issues/59935/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/nodejs/node/issues/59933", "repository_url": "https://api.github.com/repos/nodejs/node", "labels_url": "https://api.github.com/repos/nodejs/node/issues/59933/labels{/name}", "comments_url": "https://api.github.com/repos/nodejs/node/issues/59933/comments", "events_url": "https://api.github.com/repos/nodejs/node/issues/59933/events", "html_url": "https://github.com/nodejs/node/issues/59933", "id": 3434468853, "node_id": "I_kwDOAZ7xs87MtdX1", "number": 59933, "title": "Use `C:/` path searching in windows over `C:\\`", "user": {"login": "GameLord2011", "id": 119822417, "node_id": "U_kgDOByRYUQ", "avatar_url": "https://avatars.githubusercontent.com/u/119822417?v=4", "gravatar_id": "", "url": "https://api.github.com/users/GameLord2011", "html_url": "https://github.com/GameLord2011", "followers_url": "https://api.github.com/users/GameLord2011/followers", "following_url": "https://api.github.com/users/GameLord2011/following{/other_user}", "gists_url": "https://api.github.com/users/GameLord2011/gists{/gist_id}", "starred_url": "https://api.github.com/users/GameLord2011/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/GameLord2011/subscriptions", "organizations_url": "https://api.github.com/users/GameLord2011/orgs", "repos_url": "https://api.github.com/users/GameLord2011/repos", "events_url": "https://api.github.com/users/GameLord2011/events{/privacy}", "received_events_url": "https://api.github.com/users/GameLord2011/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 207445406, "node_id": "MDU6TGFiZWwyMDc0NDU0MDY=", "url": "https://api.github.com/repos/nodejs/node/labels/feature%20request", "name": "feature request", "color": "0D7BDE", "default": false, "description": "Issues that request new features to be added to Node.js."}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2025-09-19T13:52:50Z", "updated_at": "2025-09-24T17:06:05Z", "closed_at": null, "author_association": "NONE", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### What is the problem this feature will solve?\n\nIt would solve a problem I get whenever working with nodejs in Cygwin (the inverse of the Linux wine api) where since node uses `C:\\` paths, it breaks cygwin even though it understands windows executables.\n\n### What is the feature you are proposing to solve the problem?\n\nTo make `internal/modules/cjs/loader` use pathfinding with `C:/` over `C:\\` _I have checked, Windows **officaly** supports this feature._\n\n### What alternatives have you considered?\n\n_No response_", "reactions": {"url": "https://api.github.com/repos/nodejs/node/issues/59933/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/nodejs/node/issues/59933/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/nodejs/node/issues/59930", "repository_url": "https://api.github.com/repos/nodejs/node", "labels_url": "https://api.github.com/repos/nodejs/node/issues/59930/labels{/name}", "comments_url": "https://api.github.com/repos/nodejs/node/issues/59930/comments", "events_url": "https://api.github.com/repos/nodejs/node/issues/59930/events", "html_url": "https://github.com/nodejs/node/issues/59930", "id": 3431909638, "node_id": "I_kwDOAZ7xs87MjskG", "number": 59930, "title": "inventory-app", "user": {"login": "Pintu22-Yadav", "id": 233157967, "node_id": "U_kgDODeW1Tw", "avatar_url": "https://avatars.githubusercontent.com/u/233157967?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Pintu22-Yadav", "html_url": "https://github.com/Pintu22-Yadav", "followers_url": "https://api.github.com/users/Pintu22-Yadav/followers", "following_url": "https://api.github.com/users/Pintu22-Yadav/following{/other_user}", "gists_url": "https://api.github.com/users/Pintu22-Yadav/gists{/gist_id}", "starred_url": "https://api.github.com/users/Pintu22-Yadav/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Pintu22-Yadav/subscriptions", "organizations_url": "https://api.github.com/users/Pintu22-Yadav/orgs", "repos_url": "https://api.github.com/users/Pintu22-Yadav/repos", "events_url": "https://api.github.com/users/Pintu22-Yadav/events{/privacy}", "received_events_url": "https://api.github.com/users/Pintu22-Yadav/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2025-09-18T20:39:40Z", "updated_at": "2025-09-18T20:55:19Z", "closed_at": "2025-09-18T20:55:19Z", "author_association": "NONE", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "// Import required packages\nconst express = require(\"express\");\n\n// Create an Express app\nconst app = express();\nconst PORT = 3000;\n\n// Middleware to parse JSON data\napp.use(express.json());\n\n// Serve frontend files from the \"public\" folder\napp.use(express.static(\"public\"));\n\n// In-memory storage for inventory items\nlet items = [];\n\n// API: Get all items\napp.get(\"/items\", (req, res) => {\n  res.json(items);\n});\n\n// API: Add new item\napp.post(\"/items\", (req, res) => {\n  items.push(req.body);\n  res.json({ message: \"Item added!\", items });\n});\n\n// API: Delete item\napp.delete(\"/items/:id\", (req, res) => {\n  const id = parseInt(req.params.id);\n  items.splice(id, 1);\n  res.json({ message: \"Item deleted!\", items });\n});\n\n// Start the server\napp.listen(PORT, () => {\n  console.log(`Server running at http://localhost:${PORT}`);\n});", "reactions": {"url": "https://api.github.com/repos/nodejs/node/issues/59930/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/nodejs/node/issues/59930/timeline", "performed_via_github_app": null, "state_reason": "not_planned", "score": 1.0}, {"url": "https://api.github.com/repos/nodejs/node/issues/59927", "repository_url": "https://api.github.com/repos/nodejs/node", "labels_url": "https://api.github.com/repos/nodejs/node/issues/59927/labels{/name}", "comments_url": "https://api.github.com/repos/nodejs/node/issues/59927/comments", "events_url": "https://api.github.com/repos/nodejs/node/issues/59927/events", "html_url": "https://github.com/nodejs/node/issues/59927", "id": 3430946188, "node_id": "I_kwDOAZ7xs87MgBWM", "number": 59927, "title": "The module that sets source map support does not have its source maps cached/loaded", "user": {"login": "Pouja", "id": 2385144, "node_id": "MDQ6VXNlcjIzODUxNDQ=", "avatar_url": "https://avatars.githubusercontent.com/u/2385144?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Pouja", "html_url": "https://github.com/Pouja", "followers_url": "https://api.github.com/users/Pouja/followers", "following_url": "https://api.github.com/users/Pouja/following{/other_user}", "gists_url": "https://api.github.com/users/Pouja/gists{/gist_id}", "starred_url": "https://api.github.com/users/Pouja/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Pouja/subscriptions", "organizations_url": "https://api.github.com/users/Pouja/orgs", "repos_url": "https://api.github.com/users/Pouja/repos", "events_url": "https://api.github.com/users/Pouja/events{/privacy}", "received_events_url": "https://api.github.com/users/Pouja/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2025-09-18T15:40:38Z", "updated_at": "2025-09-22T09:50:53Z", "closed_at": null, "author_association": "NONE", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Version\n\n24.8.0\n\n### Platform\n\n```text\nDarwin XXXXX.local 24.6.0 Darwin Kernel Version 24.6.0: Mon Jul 14 11:28:30 PDT 2025; root:xnu-11417.140.69~1/RELEASE_ARM64_T6030 arm64\n```\n\n### Subsystem\n\nmodule\n\n### What steps will reproduce the bug?\n\nGiven the following file: enclosing-min.js:\n\n```javascript\nconst{setSourceMapsSupport}=require(\"node:module\");setSourceMapsSupport(!0);const functionA=()=>{functionB()};function functionB(){functionC()}const functionC=()=>{functionD()},functionD=()=>{(function(){if(Math.random()>0)throw new Error(\"an error!\")})()},thrower=functionA;try{thrower()}catch(n){throw n}\n//# sourceMappingURL=data:application/json;base64,ewogICJ2ZXJzaW9uIjogMywKICAic291cmNlcyI6IFsiZW5jbG9zaW5nLmpzIl0sCiAgInNvdXJjZXNDb250ZW50IjogWyJjb25zdCB7IHNldFNvdXJjZU1hcHNTdXBwb3J0IH0gPSByZXF1aXJlKCdub2RlOm1vZHVsZScpO1xuc2V0U291cmNlTWFwc1N1cHBvcnQodHJ1ZSk7XG5jb25zdCBmdW5jdGlvbkEgPSAoKSA9PiB7XG4gIGZ1bmN0aW9uQigpXG59XG5cbmZ1bmN0aW9uIGZ1bmN0aW9uQigpIHtcbiAgZnVuY3Rpb25DKClcbn1cblxuY29uc3QgZnVuY3Rpb25DID0gKCkgPT4ge1xuICBmdW5jdGlvbkQoKVxufVxuXG5jb25zdCBmdW5jdGlvbkQgPSAoKSA9PiB7XG4gIChmdW5jdGlvbiBmdW5jdGlvbkUgKCkge1xuICAgIGlmIChNYXRoLnJhbmRvbSgpID4gMCkge1xuICAgICAgdGhyb3cgbmV3IEVycm9yKCdhbiBlcnJvciEnKVxuICAgIH1cbiAgfSkoKVxufVxuXG5jb25zdCB0aHJvd2VyID0gZnVuY3Rpb25BXG5cbnRyeSB7XG4gIHRocm93ZXIoKVxufSBjYXRjaCAoZXJyKSB7XG4gIHRocm93IGVyclxufVxuIl0sCiAgIm1hcHBpbmdzIjogIkFBQUEsS0FBTSxDQUFFLG9CQUFxQixFQUFJLFFBQVEsYUFBYSxFQUN0RCxxQkFBcUIsRUFBSSxFQUN6QixNQUFNLFVBQVksSUFBTSxDQUN0QixVQUFVLENBQ1osRUFFQSxTQUFTLFdBQVksQ0FDbkIsVUFBVSxDQUNaLENBRUEsTUFBTSxVQUFZLElBQU0sQ0FDdEIsVUFBVSxDQUNaLEVBRU0sVUFBWSxJQUFNLEVBQ3JCLFVBQXNCLENBQ3JCLEdBQUksS0FBSyxPQUFPLEVBQUksRUFDbEIsTUFBTSxJQUFJLE1BQU0sV0FBVyxDQUUvQixHQUFHLENBQ0wsRUFFTSxRQUFVLFVBRWhCLEdBQUksQ0FDRixRQUFRLENBQ1YsT0FBU0EsRUFBSyxDQUNaLE1BQU1BLENBQ1IiLAogICJuYW1lcyI6IFsiZXJyIl0KfQo=\n```\n\nExecute with\n```bash\nnode enclosing-min.js\n```\n\n### How often does it reproduce? Is there a required condition?\n\nAlways\nNo\n\n### What is the expected behavior? Why is that the expected behavior?\n\nThe error stack trace includes the original lines\n\n### What do you see instead?\n\nThe stack trace has the minified lines\n\n### Additional information\n\nThe use case for not using the flag `--enable-source-maps` is that environment variables are not allowed in AWS Lambda@edge", "reactions": {"url": "https://api.github.com/repos/nodejs/node/issues/59927/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/nodejs/node/issues/59927/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/nodejs/node/issues/59922", "repository_url": "https://api.github.com/repos/nodejs/node", "labels_url": "https://api.github.com/repos/nodejs/node/issues/59922/labels{/name}", "comments_url": "https://api.github.com/repos/nodejs/node/issues/59922/comments", "events_url": "https://api.github.com/repos/nodejs/node/issues/59922/events", "html_url": "https://github.com/nodejs/node/issues/59922", "id": 3428819429, "node_id": "I_kwDOAZ7xs87MX6Hl", "number": 59922, "title": "The http2 server throws TypeError `server.shouldUpgradeCallback` is not a function", "user": {"login": "ShenHongFei", "id": 15074472, "node_id": "MDQ6VXNlcjE1MDc0NDcy", "avatar_url": "https://avatars.githubusercontent.com/u/15074472?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ShenHongFei", "html_url": "https://github.com/ShenHongFei", "followers_url": "https://api.github.com/users/ShenHongFei/followers", "following_url": "https://api.github.com/users/ShenHongFei/following{/other_user}", "gists_url": "https://api.github.com/users/ShenHongFei/gists{/gist_id}", "starred_url": "https://api.github.com/users/ShenHongFei/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ShenHongFei/subscriptions", "organizations_url": "https://api.github.com/users/ShenHongFei/orgs", "repos_url": "https://api.github.com/users/ShenHongFei/repos", "events_url": "https://api.github.com/users/ShenHongFei/events{/privacy}", "received_events_url": "https://api.github.com/users/ShenHongFei/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2025-09-18T06:17:48Z", "updated_at": "2025-09-20T18:18:24Z", "closed_at": "2025-09-20T18:18:24Z", "author_association": "CONTRIBUTOR", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Version\n\norigin/main branch\n\n### Platform\n\n```text\nwindows \"$([Environment]::OSVersion.VersionString) $(('x86', 'x64')[[Environment]::Is64BitOperatingSystem])\"\n```\n\n### Subsystem\n\nhttp\n\n### What steps will reproduce the bug?\n\n```js\nimport { createSecureServer } from 'http2'\n\nlet server = createSecureServer(...)\n\nserver.on('upgrade', () => { ... })\n\nserver.listen(443)\n\n// Create a websocket connection to port 443\n```\n\n\n\n\n\n### How often does it reproduce? Is there a required condition?\n\nMust appear\n\n### What is the expected behavior? Why is that the expected behavior?\n\nWebsocket connection is normal\n\n### What do you see instead?\n\n<img width=\"3840\" height=\"2160\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/0536dc2c-46b1-4535-bcb6-445e3ba03105\" />\n\n### Additional information\n\nrelates to: https://github.com/nodejs/node/pull/59824\n\nCan you (@pimterry) add relevant test cases for this PR? http server, http2 server and test websocket connection\n\n", "reactions": {"url": "https://api.github.com/repos/nodejs/node/issues/59922/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/nodejs/node/issues/59922/timeline", "performed_via_github_app": null, "state_reason": "completed", "score": 1.0}, {"url": "https://api.github.com/repos/nodejs/node/issues/59921", "repository_url": "https://api.github.com/repos/nodejs/node", "labels_url": "https://api.github.com/repos/nodejs/node/issues/59921/labels{/name}", "comments_url": "https://api.github.com/repos/nodejs/node/issues/59921/comments", "events_url": "https://api.github.com/repos/nodejs/node/issues/59921/events", "html_url": "https://github.com/nodejs/node/issues/59921", "id": 3428700828, "node_id": "I_kwDOAZ7xs87MXdKc", "number": 59921, "title": "ERR_INTERNAL_ASSERTION", "user": {"login": "iserioton", "id": 75625021, "node_id": "MDQ6VXNlcjc1NjI1MDIx", "avatar_url": "https://avatars.githubusercontent.com/u/75625021?v=4", "gravatar_id": "", "url": "https://api.github.com/users/iserioton", "html_url": "https://github.com/iserioton", "followers_url": "https://api.github.com/users/iserioton/followers", "following_url": "https://api.github.com/users/iserioton/following{/other_user}", "gists_url": "https://api.github.com/users/iserioton/gists{/gist_id}", "starred_url": "https://api.github.com/users/iserioton/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/iserioton/subscriptions", "organizations_url": "https://api.github.com/users/iserioton/orgs", "repos_url": "https://api.github.com/users/iserioton/repos", "events_url": "https://api.github.com/users/iserioton/events{/privacy}", "received_events_url": "https://api.github.com/users/iserioton/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2025-09-18T05:21:03Z", "updated_at": "2025-09-19T03:52:01Z", "closed_at": "2025-09-19T03:52:01Z", "author_association": "NONE", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Version\n\nv20.10.0\n\n### Platform\n\n```text\nLinux MigrationServer 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux\n```\n\n### Subsystem\n\n_No response_\n\n### What steps will reproduce the bug?\n\nPerforming the long-running script that includes a ton of external API calls and a few DB operations with MySQL.\n\n### How often does it reproduce? Is there a required condition?\n\nIt threw the same error six times, and then it suddenly went away.\n\n### What is the expected behavior? Why is that the expected behavior?\n\nStack trace error mentioned below\n\n### What do you see instead?\n\nStack trace error mentioned below\n\n### Additional information\n\nError [ERR_INTERNAL_ASSERTION]: This is caused by either a bug in Node.js or incorrect usage of Node.js internals.\nPlease open an issue with this stack trace at https://github.com/nodejs/node/issues\n\n    at assert (node:internal/assert:14:11)\n    at internalConnectMultiple (node:net:1118:3)\n    at Timeout.internalConnectMultipleTimeout (node:net:1687:3)\n    at listOnTimeout (node:internal/timers:575:11)\n    at process.processTimers (node:internal/timers:514:7)", "reactions": {"url": "https://api.github.com/repos/nodejs/node/issues/59921/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/nodejs/node/issues/59921/timeline", "performed_via_github_app": null, "state_reason": "not_planned", "score": 1.0}, {"url": "https://api.github.com/repos/nodejs/node/issues/59918", "repository_url": "https://api.github.com/repos/nodejs/node", "labels_url": "https://api.github.com/repos/nodejs/node/issues/59918/labels{/name}", "comments_url": "https://api.github.com/repos/nodejs/node/issues/59918/comments", "events_url": "https://api.github.com/repos/nodejs/node/issues/59918/events", "html_url": "https://github.com/nodejs/node/issues/59918", "id": 3427925524, "node_id": "I_kwDOAZ7xs87MUf4U", "number": 59918, "title": "Package directory resolution API", "user": {"login": "KJ7LNW", "id": 93454819, "node_id": "U_kgDOBZIB4w", "avatar_url": "https://avatars.githubusercontent.com/u/93454819?v=4", "gravatar_id": "", "url": "https://api.github.com/users/KJ7LNW", "html_url": "https://github.com/KJ7LNW", "followers_url": "https://api.github.com/users/KJ7LNW/followers", "following_url": "https://api.github.com/users/KJ7LNW/following{/other_user}", "gists_url": "https://api.github.com/users/KJ7LNW/gists{/gist_id}", "starred_url": "https://api.github.com/users/KJ7LNW/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/KJ7LNW/subscriptions", "organizations_url": "https://api.github.com/users/KJ7LNW/orgs", "repos_url": "https://api.github.com/users/KJ7LNW/repos", "events_url": "https://api.github.com/users/KJ7LNW/events{/privacy}", "received_events_url": "https://api.github.com/users/KJ7LNW/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 207445406, "node_id": "MDU6TGFiZWwyMDc0NDU0MDY=", "url": "https://api.github.com/repos/nodejs/node/labels/feature%20request", "name": "feature request", "color": "0D7BDE", "default": false, "description": "Issues that request new features to be added to Node.js."}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2025-09-17T22:05:53Z", "updated_at": "2025-09-17T22:05:53Z", "closed_at": null, "author_association": "NONE", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### What is the problem this feature will solve?\n\nCurrently, resolving a package name to its directory requires:\n1. `require.resolve(packageName)` - needs \".\" export or main field\n2. Walking up from resolved entry point to find package.json\n\nThis is fragile for build tools that need package directories for scanning source files, especially with modern package.json exports restrictions.\n\n### Evidence: Node.js Already Knows the Path\nWhen resolution fails, Node.js error messages reveal it already knows the package directory:\n\n```js\nError [ERR_PACKAGE_PATH_NOT_EXPORTED]: \nNo \"exports\" main defined in /path/to/project/packages/@scope/package-name/package.json \nimported from /path/to/project/src/file.css\n```\n\nThe error shows Node.js found `/path/to/project/packages/@scope/package-name/` but won't expose this path to user code without proper exports. **This proves the resolution logic already exists internally.**\n\n\n### What is the feature you are proposing to solve the problem?\n\n\n#### Option 1: Extend require.resolve\n```javascript\n// New parameter to resolve to directory containing package.json\nconst packageDir = require.resolve(packageName, { resolveToDirectory: true })\n```\n\n#### Option 2: New API\n```javascript\nconst packageDir = require.resolvePackageDirectory(packageName)\n// or\nconst Module = require('module')\nconst packageDir = Module.resolvePackageDirectory(packageName, fromPath)\n```\n\n\n\n### What alternatives have you considered?\n\nCurrently, resolving a package name to its directory requires:\n1. `require.resolve(packageName)` - needs \".\" export or main field\n2. Walking up from resolved entry point to find package.json\n\n### Use Cases\n- Build tools (Vite, webpack plugins) scanning package source files\n- Tailwind CSS content detection across monorepo packages  \n- Dev tools that need to locate package assets/configs\n- Monorepo tooling that works with workspace packages", "reactions": {"url": "https://api.github.com/repos/nodejs/node/issues/59918/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/nodejs/node/issues/59918/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/nodejs/node/issues/59916", "repository_url": "https://api.github.com/repos/nodejs/node", "labels_url": "https://api.github.com/repos/nodejs/node/issues/59916/labels{/name}", "comments_url": "https://api.github.com/repos/nodejs/node/issues/59916/comments", "events_url": "https://api.github.com/repos/nodejs/node/issues/59916/events", "html_url": "https://github.com/nodejs/node/issues/59916", "id": 3427233830, "node_id": "I_kwDOAZ7xs87MR3Am", "number": 59916, "title": "sqlite: expose authorization API", "user": {"login": "cjihrig", "id": 2512748, "node_id": "MDQ6VXNlcjI1MTI3NDg=", "avatar_url": "https://avatars.githubusercontent.com/u/2512748?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cjihrig", "html_url": "https://github.com/cjihrig", "followers_url": "https://api.github.com/users/cjihrig/followers", "following_url": "https://api.github.com/users/cjihrig/following{/other_user}", "gists_url": "https://api.github.com/users/cjihrig/gists{/gist_id}", "starred_url": "https://api.github.com/users/cjihrig/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cjihrig/subscriptions", "organizations_url": "https://api.github.com/users/cjihrig/orgs", "repos_url": "https://api.github.com/users/cjihrig/repos", "events_url": "https://api.github.com/users/cjihrig/events{/privacy}", "received_events_url": "https://api.github.com/users/cjihrig/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 207445406, "node_id": "MDU6TGFiZWwyMDc0NDU0MDY=", "url": "https://api.github.com/repos/nodejs/node/labels/feature%20request", "name": "feature request", "color": "0D7BDE", "default": false, "description": "Issues that request new features to be added to Node.js."}, {"id": 7186154574, "node_id": "LA_kwDOAZ7xs88AAAABrFQETg", "url": "https://api.github.com/repos/nodejs/node/labels/sqlite", "name": "sqlite", "color": "064a64", "default": false, "description": "Issues and PRs related to the SQLite subsystem."}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2025-09-17T17:46:47Z", "updated_at": "2025-09-18T18:16:11Z", "closed_at": null, "author_association": "CONTRIBUTOR", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### What is the problem this feature will solve?\n\nUsers can add custom authz on their databases. Alternatively, Node core could better integrate SQLite and its permission system.\n\n### What is the feature you are proposing to solve the problem?\n\nExpose the SQLite Authorization API via the JavaScript API.\n\n### What alternatives have you considered?\n\nNone", "reactions": {"url": "https://api.github.com/repos/nodejs/node/issues/59916/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/nodejs/node/issues/59916/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/nodejs/node/issues/59915", "repository_url": "https://api.github.com/repos/nodejs/node", "labels_url": "https://api.github.com/repos/nodejs/node/issues/59915/labels{/name}", "comments_url": "https://api.github.com/repos/nodejs/node/issues/59915/comments", "events_url": "https://api.github.com/repos/nodejs/node/issues/59915/events", "html_url": "https://github.com/nodejs/node/issues/59915", "id": 3426960094, "node_id": "I_kwDOAZ7xs87MQ0Le", "number": 59915, "title": "Edusecure ", "user": {"login": "rbopche179-sketch", "id": 231384812, "node_id": "U_kgDODcqm7A", "avatar_url": "https://avatars.githubusercontent.com/u/231384812?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rbopche179-sketch", "html_url": "https://github.com/rbopche179-sketch", "followers_url": "https://api.github.com/users/rbopche179-sketch/followers", "following_url": "https://api.github.com/users/rbopche179-sketch/following{/other_user}", "gists_url": "https://api.github.com/users/rbopche179-sketch/gists{/gist_id}", "starred_url": "https://api.github.com/users/rbopche179-sketch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rbopche179-sketch/subscriptions", "organizations_url": "https://api.github.com/users/rbopche179-sketch/orgs", "repos_url": "https://api.github.com/users/rbopche179-sketch/repos", "events_url": "https://api.github.com/users/rbopche179-sketch/events{/privacy}", "received_events_url": "https://api.github.com/users/rbopche179-sketch/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2025-09-17T16:16:26Z", "updated_at": "2025-09-17T16:34:59Z", "closed_at": "2025-09-17T16:34:55Z", "author_association": "NONE", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "// App.js\nimport React, { useEffect, useState } from \"react\";\nimport { StyleSheet, Text, View, TouchableOpacity, TextInput, ScrollView, Alert } from \"react-native\";\nimport AsyncStorage from \"@react-native-async-storage/async-storage\";\n\n/*\nExpo React Native demo \u2014 National Education Management System (Android-ready)\nFeatures:\n- Multi-role login: student, teacher, admin, school\n- Mock backend using AsyncStorage (seeded on first run)\n- Simple dashboards and actions: verify school/teacher (admin), apply scholarship (student), upload marks/attendance (teacher), register teacher (school)\nThis is a demo / prototype \u2014 replace with real backend for production.\n*/\n\nconst STORAGE_KEY = \"nem_demo_v1\";\n\nconst seedData = {\n  schools: [\n    { id: \"SCH-001\", name: \"Aadi Vikas High School\", verified: true },\n    { id: \"SCH-002\", name: \"Khapri Public School\", verified: false }\n  ],\n  teachers: [\n    { id: \"T-101\", name: \"Ramesh Rao\", schoolId: \"SCH-001\", approved: true },\n    { id: \"T-102\", name: \"Sita Sharma\", schoolId: \"SCH-002\", approved: false }\n  ],\n  students: [\n    { id: \"S-1001\", name: \"Vaidhavi Sontakke\", schoolId: \"SCH-001\", attendance: 92, scholarshipApplied: false },\n    { id: \"S-1002\", name: \"Aditya Meshram\", schoolId: \"SCH-001\", attendance: 85, scholarshipApplied: true }\n  ],\n  exams: [{ id: \"E-1\", title: \"Term 1\", date: \"2025-11-20\", resultsPublished: false }],\n  notifications: []\n};\n\nasync function loadData() {\n  try {\n    const raw = await AsyncStorage.getItem(STORAGE_KEY);\n    if (!raw) {\n      await AsyncStorage.setItem(STORAGE_KEY, JSON.stringify(seedData));\n      return seedData;\n    }\n    return JSON.parse(raw);\n  } catch (e) {\n    console.error(\"loadData error\", e);\n    return seedData;\n  }\n}\nasync function saveData(data) {\n  try {\n    await AsyncStorage.setItem(STORAGE_KEY, JSON.stringify(data));\n  } catch (e) {\n    console.error(\"saveData error\", e);\n  }\n}\n\nexport default function App() {\n  const [dataVersion, setDataVersion] = useState(0); // force refresh\n  const [rolePick, setRolePick] = useState(null);\n  const [current, setCurrent] = useState(null); // {role, user}\n  const [data, setData] = useState(null);\n\n  useEffect(() => {\n    (async () => {\n      const d = await loadData();\n      setData(d);\n    })();\n  }, [dataVersion]);\n\n  const reload = () => setDataVersion(v => v + 1);\n\n  // Login logic (simple demo)\n  const handleLogin = async (role, id, password) => {\n    const d = await loadData();\n    if (role === \"student\") {\n      const s = d.students.find(x => x.id === id || x.name.toLowerCase() === id.toLowerCase());\n      if (s) { setCurrent({ role: \"student\", user: s }); return; }\n    }\n    if (role === \"teacher\") {\n      const t = d.teachers.find(x => x.id === id || x.name.toLowerCase() === id.toLowerCase());\n      if (t) { setCurrent({ role: \"teacher\", user: t }); return; }\n    }\n    if (role === \"admin\") {\n      // demo admin credentials\n      if (id === \"admin\" && password === \"admin123\") { setCurrent({ role: \"admin\", user: { name: \"National Admin\" } }); return; }\n    }\n    if (role === \"school\") {\n      const s = d.schools.find(x => x.id === id || x.name.toLowerCase() === id.toLowerCase());\n      if (s) { setCurrent({ role: \"school\", user: s }); return; }\n    }\n    Alert.alert(\"Login failed\", \"Check ID / name / password (demo)\");\n  };\n\n  const handleLogout = () => { setCurrent(null); setRolePick(null); reload(); };\n\n  if (!data) {\n    return (\n      <View style={styles.center}>\n        <Text>Loading demo data...</Text>\n      </View>\n    );\n  }\n\n  // ---------- Screens ----------\n  if (!current) {\n    return (\n      <ScrollView style={styles.safe}>\n        <Text style={styles.title}>National Education Management (Demo)</Text>\n\n        <View style={styles.card}>\n          <Text style={styles.small}>Choose role to login</Text>\n          <View style={styles.row}>\n            <TouchableOpacity style={styles.btn} onPress={() => setRolePick(\"student\")}><Text style={styles.btnText}>Student</Text></TouchableOpacity>\n            <TouchableOpacity style={styles.btn} onPress={() => setRolePick(\"teacher\")}><Text style={styles.btnText}>Teacher</Text></TouchableOpacity>\n          </View>\n          <View style={styles.row}>\n            <TouchableOpacity style={styles.btn} onPress={() => setRolePick(\"admin\")}><Text style={styles.btnText}>Admin</Text></TouchableOpacity>\n            <TouchableOpacity style={styles.btn} onPress={() => setRolePick(\"school\")}><Text style={styles.btnText}>School</Text></TouchableOpacity>\n          </View>\n        </View>\n\n        {rolePick && <LoginCard role={rolePick} onLogin={handleLogin} demoData={data} onBack={() => setRolePick(null)} />}\n\n        <View style={styles.card}>\n          <Text style={styles.small}>Quick demo actions</Text>\n          <View style={{ flexDirection: \"row\", marginTop: 8 }}>\n            <TouchableOpacity style={styles.btn} onPress={async () => {\n              const d = await loadData();\n              d.students.push({ id: `S-${Math.floor(1000 + Math.random() * 9000)}`, name: 'Demo Student ' + Date.now() % 1000, schoolId: d.schools[0].id, attendance: 80, scholarshipApplied: false });\n              await saveData(d); reload(); Alert.alert(\"Done\", \"Added demo student\");\n            }}>\n              <Text style={styles.btnText}>Add Demo Student</Text>\n            </TouchableOpacity>\n            <TouchableOpacity style={[styles.btn, styles.ghost]} onPress={async () => {\n              await AsyncStorage.removeItem(STORAGE_KEY);\n              await loadData();\n              reload();\n              Alert.alert(\"Reset\", \"Demo data reset\");\n            }}>\n              <Text>Reset Data</Text>\n            </TouchableOpacity>\n          </View>\n        </View>\n      </ScrollView>\n    );\n  }\n\n  // Student dashboard\n  if (current.role === \"student\") {\n    const student = data.students.find(s => s.id === current.user.id);\n    const school = data.schools.find(s => s.id === student.schoolId);\n    return (\n      <ScrollView style={styles.safe}>\n        <Text style={styles.title}>Student Dashboard</Text>\n        <View style={styles.card}>\n          <Text style={styles.small}>Welcome</Text>\n          <Text style={styles.h3}>{student.name}</Text>\n          <Text style={styles.small}>School: {school?.name || \"\u2014\"}</Text>\n          <Text style={{ marginTop: 8 }}>Attendance: {student.attendance}%</Text>\n        </View>\n\n        <View style={styles.card}>\n          <Text style={styles.small}>Scholarship</Text>\n          {student.scholarshipApplied ? <Text style={styles.notice}>You have already applied.</Text> :\n            <TouchableOpacity style={styles.btn} onPress={async () => {\n              const d = await loadData();\n              const idx = d.students.findIndex(x => x.id === student.id);\n              d.students[idx].scholarshipApplied = true;\n              d.notifications.push({ id: Date.now(), text: `Scholarship applied: ${student.name}` });\n              await saveData(d); reload(); Alert.alert(\"Applied\", \"Scholarship applied (demo)\");\n            }}>\n              <Text style={styles.btnText}>Apply Scholarship</Text>\n            </TouchableOpacity>}\n        </View>\n\n        <View style={styles.card}>\n          <Text style={styles.small}>Upcoming Exams</Text>\n          {data.exams.map(e => <View key={e.id} style={{ marginTop: 8 }}><Text>{e.title} - {e.date}</Text></View>)}\n        </View>\n\n        <TouchableOpacity style={[styles.btn, { alignSelf: \"center\", margin: 16 }]} onPress={handleLogout}><Text style={styles.btnText}>Logout</Text></TouchableOpacity>\n      </ScrollView>\n    );\n  }\n\n  // Teacher dashboard\n  if (current.role === \"teacher\") {\n    const teacher = data.teachers.find(t => t.id === current.user.id);\n    return (\n      <ScrollView style={styles.safe}>\n        <Text style={styles.title}>Teacher Dashboard</Text>\n        <View style={styles.card}>\n          <Text style={styles.small}>Teacher</Text>\n          <Text style={styles.h3}>{teacher.name}</Text>\n          <Text style={styles.small}>School: {data.schools.find(s => s.id === teacher.schoolId)?.name}</Text>\n        </View>\n\n        <UploadMarksCard teacher={teacher} reload={reload} />\n\n        <View style={styles.card}>\n          <Text style={styles.small}>Student List (attendance quick)</Text>\n          {data.students.filter(s => s.schoolId === teacher.schoolId).map(s => (\n            <View key={s.id} style={styles.rowItem}>\n              <View>\n                <Text>{s.name}</Text>\n                <Text style={styles.small}>{s.id}</Text>\n              </View>\n              <TouchableOpacity style={styles.ghostBtn} onPress={async () => {\n                const d = await loadData();\n                const idx = d.students.findIndex(x => x.id === s.id);\n                d.students[idx].attendance = Math.min(100, d.students[idx].attendance + 1);\n                await saveData(d); reload();\n              }}>\n                <Text>Mark +1%</Text>\n              </TouchableOpacity>\n            </View>\n          ))}\n        </View>\n\n        <TouchableOpacity style={[styles.btn, { alignSelf: \"center\", margin: 16 }]} onPress={handleLogout}><Text style={styles.btnText}>Logout</Text></TouchableOpacity>\n      </ScrollView>\n    );\n  }\n\n  // Admin dashboard\n  if (current.role === \"admin\") {\n    return (\n      <ScrollView style={styles.safe}>\n        <Text style={styles.title}>Admin Panel</Text>\n        <View style={styles.card}>\n          <Text style={styles.small}>Manage schools, teachers, students and generate reports.</Text>\n        </View>\n\n        <View style={styles.card}>\n          <Text style={styles.small}>Schools to verify</Text>\n          {data.schools.filter(s => !s.verified).map(s => (\n            <View key={s.id} style={styles.rowItem}>\n              <Text>{s.name}</Text>\n              <TouchableOpacity style={styles.btn} onPress={async () => {\n                const d = await loadData();\n                const idx = d.schools.findIndex(x => x.id === s.id);\n                d.schools[idx].verified = true;\n                d.notifications.push({ id: Date.now(), text: `School ${s.name} verified` });\n                await saveData(d); reload();\n              }}><Text style={styles.btnText}>Verify</Text></TouchableOpacity>\n            </View>\n          ))}\n          {data.schools.filter(s => !s.verified).length === 0 && <Text style={styles.notice}>No pending schools</Text>}\n        </View>\n\n        <View style={styles.card}>\n          <Text style={styles.small}>Teachers to approve</Text>\n          {data.teachers.filter(t => !t.approved).map(t => (\n            <View key={t.id} style={styles.rowItem}>\n              <Text>{t.name}</Text>\n              <TouchableOpacity style={styles.btn} onPress={async () => {\n                const d = await loadData();\n                const idx = d.teachers.findIndex(x => x.id === t.id);\n                d.teachers[idx].approved = true;\n                d.notifications.push({ id: Date.now(), text: `Teacher ${t.name} approved` });\n                await saveData(d); reload();\n              }}><Text style={styles.btnText}>Approve</Text></TouchableOpacity>\n            </View>\n          ))}\n          {data.teachers.filter(t => !t.approved).length === 0 && <Text style={styles.notice}>No pending teachers</Text>}\n        </View>\n\n        <View style={styles.card}>\n          <Text style={styles.small}>Reports</Text>\n          <Text>Total Schools: {data.schools.length}</Text>\n          <Text>Verified Schools: {data.schools.filter(s => s.verified).length}</Text>\n          <Text>Total Teachers: {data.teachers.length}</Text>\n          <Text>Total Students: {data.students.length}</Text>\n        </View>\n\n        <TouchableOpacity style={[styles.btn, { alignSelf: \"center\", margin: 16 }]} onPress={handleLogout}><Text style={styles.btnText}>Logout</Text></TouchableOpacity>\n      </ScrollView>\n    );\n  }\n\n  // School dashboard\n  if (current.role === \"school\") {\n    const school = data.schools.find(s => s.id === current.user.id);\n    return (\n      <ScrollView style={styles.safe}>\n        <Text style={styles.title}>School Dashboard</Text>\n        <View style={styles.card}>\n          <Text style={styles.h3}>{school.name}</Text>\n          <Text style={styles.small}>Verified: {school.verified ? \"Yes\" : \"No\"}</Text>\n        </View>\n\n        <View style={styles.card}>\n          <Text style={styles.small}>Teachers</Text>\n          {data.teachers.filter(t => t.schoolId === school.id).map(t => (\n            <View key={t.id} style={styles.rowItem}>\n              <View>\n                <Text>{t.name}</Text>\n                <Text style={styles.small}>{t.approved ? \"Approved\" : \"Pending\"}</Text>\n              </View>\n            </View>\n          ))}\n          <TouchableOpacity style={[styles.btn, { marginTop: 8 }]} onPress={async () => {\n            const d = await loadData();\n            const newT = { id: `T-${Math.floor(1000 + Math.random() * 9000)}`, name: `New Teacher ${Date.now() % 1000}`, schoolId: school.id, approved: false };\n            d.teachers.push(newT); d.notifications.push({ id: Date.now(), text: `New teacher registered at ${school.name}` });\n            await saveData(d); reload(); Alert.alert(\"Registered\", \"New teacher registered (demo)\");\n          }}>\n            <Text style={styles.btnText}>Register Teacher (demo)</Text>\n          </TouchableOpacity>\n        </View>\n\n        <TouchableOpacity style={[styles.btn, { alignSelf: \"center\", margin: 16 }]} onPress={handleLogout}><Text style={styles.btnText}>Logout</Text></TouchableOpacity>\n      </ScrollView>\n    );\n  }\n\n  return null;\n}\n\n// ---------- Small components ----------\nfunction LoginCard({ role, onLogin, demoData, onBack }) {\n  const [id, setId] = useState(\"\");\n  const [password, setPassword] = useState(\"\");\n  return (\n    <View style={styles.card}>\n      <Text style={styles.small}>Login as <Text style={{ fontWeight: \"700\" }}>{role}</Text></Text>\n      <TextInput style={styles.input} placeholder={role === \"student\" ? \"Student ID or name\" : \"ID or name\"} value={id} onChangeText={setId} />\n      <TextInput style={styles.input} placeholder=\"Password (admin: admin123)\" secureTextEntry value={password} onChangeText={setPassword} />\n      <View style={{ flexDirection: \"row\", marginTop: 8 }}>\n        <TouchableOpacity style={styles.btn} onPress={() => onLogin(role, id.trim(), password)}>Login</TouchableOpacity>\n        <TouchableOpacity style={[styles.btn, styles.ghost]} onPress={() => {\n          // quick fill demo\n          if (role === \"student\") setId(demoData.students[0].id);\n          if (role === \"teacher\") setId(demoData.teachers[0].id);\n          if (role === \"school\") setId(demoData.schools[0].id);\n          if (role === \"admin\") { setId(\"admin\"); setPassword(\"admin123\"); }\n        }}>Quick fill</TouchableOpacity>\n      </View>\n      <TouchableOpacity onPress={onBack}><Text style={{ marginTop: 8, color: \"#444\" }}>Back</Text></TouchableOpacity>\n    </View>\n  );\n}\n\nfunction UploadMarksCard({ teacher, reload }) {\n  const [studentId, setStudentId] = useState(\"\");\n  const [marks, setMarks] = useState(\"\");\n  const upload = async () => {\n    const d = await loadData();\n    d.notifications.push({ id: Date.now(), text: `Marks uploaded by ${teacher.name} for ${studentId || \"N/A\"}` });\n    await saveData(d); reload(); Alert.alert(\"Uploaded\", \"Marks uploaded (demo)\");\n  };\n  return (\n    <View style={styles.card}>\n      <Text style={styles.small}>Upload Marks</Text>\n      <TextInput style={styles.input} placeholder=\"Student ID\" value={studentId} onChangeText={setStudentId} />\n      <TextInput style={styles.input} placeholder=\"Marks\" value={marks} onChangeText={setMarks} keyboardType=\"numeric\" />\n      <TouchableOpacity style={styles.btn} onPress={upload}><Text style={styles.btnText}>Upload</Text></TouchableOpacity>\n    </View>\n  );\n}\n\n// ---------- Styles ----------\nconst styles = StyleSheet.create({\n  safe: { flex: 1, paddingTop: 36, backgroundColor: \"#fff\" },\n  center: { flex: 1, justifyContent: \"center\", alignItems: \"center\" },\n  title: { fontSize: 20, fontWeight: \"700\", textAlign: \"center\", marginBottom: 12 },\n  card: { padding: 14, margin: 12, borderRadius: 10, borderWidth: 1, borderColor: \"#eef2f6\", backgroundColor: \"#fff\", shadowColor: \"#000\", shadowOpacity: 0.02, shadowRadius: 6 },\n  small: { fontSize: 13, color: \"#444\" },\n  row: { flexDirection: \"row\", justifyContent: \"space-between\", marginTop: 8 },\n  btn: { backgroundColor: \"#06b6d4\", padding: 10, borderRadius: 8, flex: 1, alignItems: \"center\", margin: 4 },\n  ghost: { backgroundColor: \"#f3f4f6\", borderColor: \"#e6e9ee\", borderWidth: 1 },\n  btnText: { color: \"#fff\", fontWeight: \"700\" },\n  input: { borderWidth: 1, borderColor: \"#eef2f6\", borderRadius: 8, padding: 10, marginTop: 8 },\n  notice: { padding: 8, backgroundColor: \"#f8fafc\", borderRadius: 8, marginTop: 8 },\n  h3: { fontSize: 18, fontWeight: \"700\", marginTop: 6 },\n  rowItem: { flexDirection: \"row\", justifyContent: \"space-between\", alignItems: \"center\", paddingVertical: 8, borderBottomWidth: 0.5, borderColor: \"#f0f2f5\" },\n  ghostBtn: { padding: 8, borderRadius: 8, borderWidth: 1, borderColor: \"#e6e9ee\", backgroundColor: \"#fff\" }\n});", "reactions": {"url": "https://api.github.com/repos/nodejs/node/issues/59915/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/nodejs/node/issues/59915/timeline", "performed_via_github_app": null, "state_reason": "not_planned", "score": 1.0}, {"url": "https://api.github.com/repos/nodejs/node/issues/59913", "repository_url": "https://api.github.com/repos/nodejs/node", "labels_url": "https://api.github.com/repos/nodejs/node/issues/59913/labels{/name}", "comments_url": "https://api.github.com/repos/nodejs/node/issues/59913/comments", "events_url": "https://api.github.com/repos/nodejs/node/issues/59913/events", "html_url": "https://github.com/nodejs/node/issues/59913", "id": 3426390504, "node_id": "I_kwDOAZ7xs87MOpHo", "number": 59913, "title": "`import \"typescript\"` slower than `require(\"typescript\")` due to `cjs-module-lexer` overhead", "user": {"login": "chenjiahan", "id": 7237365, "node_id": "MDQ6VXNlcjcyMzczNjU=", "avatar_url": "https://avatars.githubusercontent.com/u/7237365?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chenjiahan", "html_url": "https://github.com/chenjiahan", "followers_url": "https://api.github.com/users/chenjiahan/followers", "following_url": "https://api.github.com/users/chenjiahan/following{/other_user}", "gists_url": "https://api.github.com/users/chenjiahan/gists{/gist_id}", "starred_url": "https://api.github.com/users/chenjiahan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chenjiahan/subscriptions", "organizations_url": "https://api.github.com/users/chenjiahan/orgs", "repos_url": "https://api.github.com/users/chenjiahan/repos", "events_url": "https://api.github.com/users/chenjiahan/events{/privacy}", "received_events_url": "https://api.github.com/users/chenjiahan/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2025-09-17T13:45:47Z", "updated_at": "2025-09-19T14:25:23Z", "closed_at": null, "author_association": "NONE", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Version\n\n- Node.js v24.8.0\n- Node.js v22.18.0\n\n### Platform\n\nObserved on macOS (x64), but likely reproducible on other platforms.\n\n### Description\n\nWhen loading `typescript`, using `import` is ~100ms slower than `require`.\n\nThis seems to come from the `cjs-module-lexer` step Node.js runs for CommonJS packages. Since `typescript` itself is CommonJS, `require` avoids the overhead and is noticeably faster.\n\n<img width=\"2368\" height=\"1322\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/d0ec96bb-4b96-4222-843e-e45bd9138f96\" />\n\n### Reproduction\n\n> https://github.com/chenjiahan/nodejs-repro-require-import-typescript\n\n```js\n// import.mjs\nimport typescript from \"typescript\";\n\nconsole.log(typescript);\n\n// require.cjs\nconst typescript = require(\"typescript\");\n\nconsole.log(typescript);\n```\n\n**Example results (macOS, Node.js v24.8.0):**\n\n```bash\n# ~200ms\ntime node ./import.mjs\n\n# ~100ms\ntime node ./require.cjs\n```\n\n### Ref\n\n* https://x.com/matteocollina/status/1968303799271047230 @mcollina \n", "reactions": {"url": "https://api.github.com/repos/nodejs/node/issues/59913/reactions", "total_count": 8, "+1": 8, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/nodejs/node/issues/59913/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/nodejs/node/issues/59912", "repository_url": "https://api.github.com/repos/nodejs/node", "labels_url": "https://api.github.com/repos/nodejs/node/issues/59912/labels{/name}", "comments_url": "https://api.github.com/repos/nodejs/node/issues/59912/comments", "events_url": "https://api.github.com/repos/nodejs/node/issues/59912/events", "html_url": "https://github.com/nodejs/node/issues/59912", "id": 3426334292, "node_id": "I_kwDOAZ7xs87MObZU", "number": 59912, "title": "Ability to block installs of packages that were published less than x days ago", "user": {"login": "dloetzke", "id": 97966457, "node_id": "U_kgDOBdbZeQ", "avatar_url": "https://avatars.githubusercontent.com/u/97966457?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dloetzke", "html_url": "https://github.com/dloetzke", "followers_url": "https://api.github.com/users/dloetzke/followers", "following_url": "https://api.github.com/users/dloetzke/following{/other_user}", "gists_url": "https://api.github.com/users/dloetzke/gists{/gist_id}", "starred_url": "https://api.github.com/users/dloetzke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dloetzke/subscriptions", "organizations_url": "https://api.github.com/users/dloetzke/orgs", "repos_url": "https://api.github.com/users/dloetzke/repos", "events_url": "https://api.github.com/users/dloetzke/events{/privacy}", "received_events_url": "https://api.github.com/users/dloetzke/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 207445406, "node_id": "MDU6TGFiZWwyMDc0NDU0MDY=", "url": "https://api.github.com/repos/nodejs/node/labels/feature%20request", "name": "feature request", "color": "0D7BDE", "default": false, "description": "Issues that request new features to be added to Node.js."}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2025-09-17T13:31:04Z", "updated_at": "2025-09-17T19:46:53Z", "closed_at": "2025-09-17T19:46:53Z", "author_association": "NONE", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### What is the problem this feature will solve?\n\nThis could be a great help against installing hijacked packages that published malicious updates (as happening a LOT recently), which are often removed from npm fairly quickly.\nThis feature could be used to make sure that npm and/or the maintainers have enough time to remove malicious updates before the version is allowed to be installed.\nIt might also help with #59911.\n\n### What is the feature you are proposing to solve the problem?\n\nI'd love to have a way to tell Node.js (e.g. in an .npmrc file) to not allow installing any packages that aren't older than x days/hours/minutes.\nThis could be seen as an automation of always running `npm install --before (Today-xDays)`, which I assume already contains the necessary logic for this.\n\nA configurable list of exceptions to this rule or at least a workaround command flag would be very appreciated, of course.\n\n### What alternatives have you considered?\n\npnpm has recently implemented a similar functionality in their [v10.16](https://github.com/pnpm/pnpm/releases/tag/v10.16.0).\n\nIn theory, npm install [--before](https://docs.npmjs.com/cli/v11/using-npm/config#before) allows to construct this behavior, but adding that to every npm i and npm ci is not really a feasible solution.\n\nThis functionality can also be (partly) achieved through the dependency management itself (e.g. GitHub's Dependabot). However, implementing it in Node.js itself helps in keeping the devices themselves safe and making sure that developers don't accidentally install malicious packages or even push them into repositories.", "reactions": {"url": "https://api.github.com/repos/nodejs/node/issues/59912/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/nodejs/node/issues/59912/timeline", "performed_via_github_app": null, "state_reason": "not_planned", "score": 1.0}, {"url": "https://api.github.com/repos/nodejs/node/issues/59911", "repository_url": "https://api.github.com/repos/nodejs/node", "labels_url": "https://api.github.com/repos/nodejs/node/issues/59911/labels{/name}", "comments_url": "https://api.github.com/repos/nodejs/node/issues/59911/comments", "events_url": "https://api.github.com/repos/nodejs/node/issues/59911/events", "html_url": "https://github.com/nodejs/node/issues/59911", "id": 3425024423, "node_id": "I_kwDOAZ7xs87MJbmn", "number": 59911, "title": "Adding a vulnerability scanner as part of the dependency updates", "user": {"login": "mcollina", "id": 52195, "node_id": "MDQ6VXNlcjUyMTk1", "avatar_url": "https://avatars.githubusercontent.com/u/52195?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mcollina", "html_url": "https://github.com/mcollina", "followers_url": "https://api.github.com/users/mcollina/followers", "following_url": "https://api.github.com/users/mcollina/following{/other_user}", "gists_url": "https://api.github.com/users/mcollina/gists{/gist_id}", "starred_url": "https://api.github.com/users/mcollina/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mcollina/subscriptions", "organizations_url": "https://api.github.com/users/mcollina/orgs", "repos_url": "https://api.github.com/users/mcollina/repos", "events_url": "https://api.github.com/users/mcollina/events{/privacy}", "received_events_url": "https://api.github.com/users/mcollina/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 237979054, "node_id": "MDU6TGFiZWwyMzc5NzkwNTQ=", "url": "https://api.github.com/repos/nodejs/node/labels/tools", "name": "tools", "color": "d4c5f9", "default": false, "description": "Issues and PRs related to the tools directory."}, {"id": 311053234, "node_id": "MDU6TGFiZWwzMTEwNTMyMzQ=", "url": "https://api.github.com/repos/nodejs/node/labels/security", "name": "security", "color": "bb3344", "default": false, "description": "Issues and PRs related to security."}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2025-09-17T07:24:16Z", "updated_at": "2025-09-24T20:33:39Z", "closed_at": null, "author_association": "MEMBER", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "I think we should add a vulnerability scanner in the dependency updates flow.\n\nPRs such as https://github.com/nodejs/node/pull/57769, should be scanned for vulnerabilities before going through - I would also _not_ installing things if they would pull vulnerable dependencies (not sure how easy that would be).\n\n@aduh95 @BridgeAR @ruyadorno", "reactions": {"url": "https://api.github.com/repos/nodejs/node/issues/59911/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/nodejs/node/issues/59911/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}]}