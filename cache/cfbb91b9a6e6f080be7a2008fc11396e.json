{"total_count": 8, "incomplete_results": false, "items": [{"url": "https://api.github.com/repos/langchain-ai/langchain/issues/32223", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32223/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32223/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32223/events", "html_url": "https://github.com/langchain-ai/langchain/issues/32223", "id": 3258986710, "node_id": "I_kwDOIPDwls7CQDDW", "number": 32223, "title": "`@chain` decorator argument type is not fully compatible with `RunnableLambda`", "user": {"login": "mshavliuk", "id": 6589665, "node_id": "MDQ6VXNlcjY1ODk2NjU=", "avatar_url": "https://avatars.githubusercontent.com/u/6589665?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mshavliuk", "html_url": "https://github.com/mshavliuk", "followers_url": "https://api.github.com/users/mshavliuk/followers", "following_url": "https://api.github.com/users/mshavliuk/following{/other_user}", "gists_url": "https://api.github.com/users/mshavliuk/gists{/gist_id}", "starred_url": "https://api.github.com/users/mshavliuk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mshavliuk/subscriptions", "organizations_url": "https://api.github.com/users/mshavliuk/orgs", "repos_url": "https://api.github.com/users/mshavliuk/repos", "events_url": "https://api.github.com/users/mshavliuk/events{/privacy}", "received_events_url": "https://api.github.com/users/mshavliuk/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 5680700839, "node_id": "LA_kwDOIPDwls8AAAABUpidpw", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/bug", "name": "bug", "color": "b60205", "default": true, "description": "Related to a bug, vulnerability, unexpected error with an existing feature"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2025-07-24T08:24:07Z", "updated_at": "2025-07-24T08:27:29Z", "closed_at": null, "author_association": "CONTRIBUTOR", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Checked other resources\n\n- [x] This is a bug, not a usage question. For questions, please use the LangChain Forum (https://forum.langchain.com/).\n- [x] I added a clear and descriptive title that summarizes this issue.\n- [x] I used the GitHub search to find a similar question and didn't find it.\n- [x] I am sure that this is a bug in LangChain rather than my code.\n- [x] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n- [x] I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n- [x] I posted a self-contained, minimal, reproducible example. A maintainer can copy it and run it AS IS.\n\n### Example Code\n\n```python\nfrom langchain_core.messages import AIMessage\nfrom langchain_core.prompt_values import ChatPromptValue\nfrom langchain_core.runnables import RunnableLambda, chain, RunnableConfig\n\n\n############### This code works without any issues or mypy errors ###############\n\ndef to_plain_text(input: ChatPromptValue, config: RunnableConfig) -> str:\n    if config[\"configurable\"].get(\"upper_case\", False):\n        return input.to_string().upper()\n    else:\n        return input.to_string()\n\n\nto_plain_text_lambda = RunnableLambda(to_plain_text)\nprompt = ChatPromptValue(messages=[AIMessage('This is a test message.')])\nresult = (\n    to_plain_text_lambda\n    .invoke(prompt, config=RunnableConfig(configurable={\"upper_case\": True}))\n)\nprint(result)\n\n\n############### This code also works, but mypy raises error ###############\n\n@chain\ndef to_plain_text_runnable(input: ChatPromptValue, config: RunnableConfig) -> str:\n    if config[\"configurable\"].get(\"upper_case\", False):\n        return input.to_string().upper()\n    else:\n        return input.to_string()\n\n\nresult = (\n    to_plain_text_runnable\n          .invoke(prompt, config=RunnableConfig(configurable={\"upper_case\": True}))\n)\nprint(result)\n```\n\nThen run `mypy example.py`\n\n### Error Message and Stack Trace (if applicable)\n\nerror: Argument 1 to \"chain\" has incompatible type \"Callable[[ChatPromptValue, RunnableConfig], str]\"; expected \"Callable[[ChatPromptValue], Coroutine[Any, Any, Never]]\"  [arg-type]\n\n\n### Description\n\nThe first usage of `RunnableLambda(to_plain_text)` does not raise any `mypy` errors since the function complies to the expected type. However, the `@chain` decorator `func` argument type is not fully compatible with `RunnableLambda` and does not support functions with `config: RunnableConfig` parameter, see:\nhttps://github.com/langchain-ai/langchain/blob/bd3d6496f3c67c8be10df0847ee346c9379f7169/libs/core/langchain_core/runnables/base.py#L5960-L5967\n\nand \n\nhttps://github.com/langchain-ai/langchain/blob/bd3d6496f3c67c8be10df0847ee346c9379f7169/libs/core/langchain_core/runnables/base.py#L4331-L4364\n\n### System Info\n\n```\nPackage Information\n-------------------\n> langchain_core: 0.3.55\n```", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/32223/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32223/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/31729", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/31729/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/31729/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/31729/events", "html_url": "https://github.com/langchain-ai/langchain/issues/31729", "id": 3176075369, "node_id": "I_kwDOIPDwls69TxBp", "number": 31729, "title": "Perplexity uncomplete usage metadata parsing", "user": {"login": "ch-amal0", "id": 98335250, "node_id": "U_kgDOBdx6Eg", "avatar_url": "https://avatars.githubusercontent.com/u/98335250?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ch-amal0", "html_url": "https://github.com/ch-amal0", "followers_url": "https://api.github.com/users/ch-amal0/followers", "following_url": "https://api.github.com/users/ch-amal0/following{/other_user}", "gists_url": "https://api.github.com/users/ch-amal0/gists{/gist_id}", "starred_url": "https://api.github.com/users/ch-amal0/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ch-amal0/subscriptions", "organizations_url": "https://api.github.com/users/ch-amal0/orgs", "repos_url": "https://api.github.com/users/ch-amal0/repos", "events_url": "https://api.github.com/users/ch-amal0/events{/privacy}", "received_events_url": "https://api.github.com/users/ch-amal0/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 5680700839, "node_id": "LA_kwDOIPDwls8AAAABUpidpw", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/bug", "name": "bug", "color": "b60205", "default": true, "description": "Related to a bug, vulnerability, unexpected error with an existing feature"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2025-06-25T15:26:02Z", "updated_at": "2025-07-17T17:23:15Z", "closed_at": null, "author_association": "NONE", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Checked other resources\n\n- [x] I added a very descriptive title to this issue.\n- [x] I used the GitHub search to find a similar question and didn't find it.\n- [x] I am sure that this is a bug in LangChain rather than my code.\n- [x] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n- [x] I posted a self-contained, minimal, reproducible example. A maintainer can copy it and run it AS IS.\n\n### Example Code\n\n```python\nresponse.usage_metadata.get(\"num_search_queries\", 0)\n```\n\nwill always returns 0\n\n### Error Message and Stack Trace (if applicable)\n\n_No response_\n\n### Description\n\nIn the perplexity chat model with find the code : \n```python\ndef _create_usage_metadata(token_usage: dict) -> UsageMetadata:\n    input_tokens = token_usage.get(\"prompt_tokens\", 0)\n    output_tokens = token_usage.get(\"completion_tokens\", 0)\n    total_tokens = token_usage.get(\"total_tokens\", input_tokens + output_tokens)\n    return UsageMetadata(\n        input_tokens=input_tokens,\n        output_tokens=output_tokens,\n        total_tokens=total_tokens,\n    )\n```\n\nBut according to http://docs.perplexity.ai/api-reference/chat-completions-post the API now returns \n\n![Image](https://github.com/user-attachments/assets/5120bcc4-2652-40c9-9250-983de3d3d0c1)\n\nSo metadata will be missing\n\n### System Info\n\npython3.10 ubuntu, langchain-perplexity 0.1.1", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/31729/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/31729/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/31351", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/31351/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/31351/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/31351/events", "html_url": "https://github.com/langchain-ai/langchain/issues/31351", "id": 3088968181, "node_id": "I_kwDOIPDwls64Hen1", "number": 31351, "title": "False Number of Tokens", "user": {"login": "frankoo21", "id": 89979587, "node_id": "MDQ6VXNlcjg5OTc5NTg3", "avatar_url": "https://avatars.githubusercontent.com/u/89979587?v=4", "gravatar_id": "", "url": "https://api.github.com/users/frankoo21", "html_url": "https://github.com/frankoo21", "followers_url": "https://api.github.com/users/frankoo21/followers", "following_url": "https://api.github.com/users/frankoo21/following{/other_user}", "gists_url": "https://api.github.com/users/frankoo21/gists{/gist_id}", "starred_url": "https://api.github.com/users/frankoo21/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/frankoo21/subscriptions", "organizations_url": "https://api.github.com/users/frankoo21/orgs", "repos_url": "https://api.github.com/users/frankoo21/repos", "events_url": "https://api.github.com/users/frankoo21/events{/privacy}", "received_events_url": "https://api.github.com/users/frankoo21/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 5680700839, "node_id": "LA_kwDOIPDwls8AAAABUpidpw", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/bug", "name": "bug", "color": "b60205", "default": true, "description": "Related to a bug, vulnerability, unexpected error with an existing feature"}, {"id": 6411661606, "node_id": "LA_kwDOIPDwls8AAAABfioxJg", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/investigate", "name": "investigate", "color": "0e8a16", "default": false, "description": "Flagged for investigation"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2025-05-25T01:42:30Z", "updated_at": "2025-05-26T12:01:08Z", "closed_at": null, "author_association": "NONE", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Checked other resources\n\n- [x] I added a very descriptive title to this issue.\n- [x] I used the GitHub search to find a similar question and didn't find it.\n- [x] I am sure that this is a bug in LangChain rather than my code.\n- [x] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n- [x] I posted a self-contained, minimal, reproducible example. A maintainer can copy it and run it AS IS.\n\n### Example Code\n\nllm = ChatOpenAI(\n            openai_api_base=\"_\",\n            openai_api_key=\"_\",\n            model=\"deepseek-r1-distill-llama-70b\",\n            max_tokens=4096,\n            temperature=0.1,\n            model_kwargs={\"top_p\": 0.95, \"presence_penalty\": 0},\n            streaming=False,\n            stream_usage=True\n        )\ndef response_generator():\n    full_response = \"\"\n\n    with get_openai_callback() as cb:\n        # Correct usage of sync stream\n        for chunk in llm.stream(\"hey can u help me\",stream_options={\"include_usage\": True}):\n            chunk_text = getattr(chunk, \"content\", \"\")  # safest way to extract content\n            print(chunk_text, end=\"|\")  # visual stream display\n            full_response += chunk_text\n\n        print(\"\\n\\nFull response:\")\n        print(full_response)\n\n        # Accurate token usage\n        print(\"\\nActual Token Usage:\")\n        print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n        print(f\"Completion Tokens: {cb.completion_tokens}\")\n        print(f\"Total Tokens: {cb.total_tokens}\")\n        print(\"*\" * 10)\n\n\n### Error Message and Stack Trace (if applicable)\n\ncontent='' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 0, 'total_tokens': 10, 'input_token_details': {}, 'output_token_details': {}}\ncontent='Okay' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 1, 'total_tokens': 11, 'input_token_details': {}, 'output_token_details': {}}\ncontent=',' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 2, 'total_tokens': 12, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' the' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 3, 'total_tokens': 13, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' user' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 4, 'total_tokens': 14, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' just' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 5, 'total_tokens': 15, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' said' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 6, 'total_tokens': 16, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' \"' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 7, 'total_tokens': 17, 'input_token_details': {}, 'output_token_details': {}}\ncontent='hey' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 8, 'total_tokens': 18, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' can' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 9, 'total_tokens': 19, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' u' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 10, 'total_tokens': 20, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' help' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 11, 'total_tokens': 21, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' me' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 12, 'total_tokens': 22, 'input_token_details': {}, 'output_token_details': {}}\ncontent='.\"' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 13, 'total_tokens': 23, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' I' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 14, 'total_tokens': 24, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' should' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 15, 'total_tokens': 25, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' respond' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 16, 'total_tokens': 26, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' in' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 17, 'total_tokens': 27, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' a' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 18, 'total_tokens': 28, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' friendly' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 19, 'total_tokens': 29, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' and' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 20, 'total_tokens': 30, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' approach' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 21, 'total_tokens': 31, 'input_token_details': {}, 'output_token_details': {}}\ncontent='able' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 22, 'total_tokens': 32, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' way' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 23, 'total_tokens': 33, 'input_token_details': {}, 'output_token_details': {}}\ncontent='.\\n\\n' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 24, 'total_tokens': 34, 'input_token_details': {}, 'output_token_details': {}}\ncontent='I' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 25, 'total_tokens': 35, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' want' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 26, 'total_tokens': 36, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' to' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 27, 'total_tokens': 37, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' make' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 28, 'total_tokens': 38, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' sure' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 29, 'total_tokens': 39, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' they' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 30, 'total_tokens': 40, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' feel' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 31, 'total_tokens': 41, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' comfortable' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 32, 'total_tokens': 42, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' asking' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 33, 'total_tokens': 43, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' for' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 34, 'total_tokens': 44, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' help' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 35, 'total_tokens': 45, 'input_token_details': {}, 'output_token_details': {}}\ncontent='.\\n\\n' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 36, 'total_tokens': 46, 'input_token_details': {}, 'output_token_details': {}}\ncontent='Maybe' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 37, 'total_tokens': 47, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' I' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 38, 'total_tokens': 48, 'input_token_details': {}, 'output_token_details': {}}\ncontent=\"'ll\" additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 39, 'total_tokens': 49, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' ask' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 40, 'total_tokens': 50, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' them' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 41, 'total_tokens': 51, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' to' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 42, 'total_tokens': 52, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' provide' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 43, 'total_tokens': 53, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' more' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 44, 'total_tokens': 54, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' details' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 45, 'total_tokens': 55, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' about' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 46, 'total_tokens': 56, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' what' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 47, 'total_tokens': 57, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' they' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 48, 'total_tokens': 58, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' need' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 49, 'total_tokens': 59, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' assistance' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 50, 'total_tokens': 60, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' with' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 51, 'total_tokens': 61, 'input_token_details': {}, 'output_token_details': {}}\ncontent='.\\n\\n' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 52, 'total_tokens': 62, 'input_token_details': {}, 'output_token_details': {}}\ncontent='Keeping' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 53, 'total_tokens': 63, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' it' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 54, 'total_tokens': 64, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' open' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 55, 'total_tokens': 65, 'input_token_details': {}, 'output_token_details': {}}\ncontent='-ended' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 56, 'total_tokens': 66, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' should' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 57, 'total_tokens': 67, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' encourage' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 58, 'total_tokens': 68, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' them' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 59, 'total_tokens': 69, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' to' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 60, 'total_tokens': 70, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' share' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 61, 'total_tokens': 71, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' more' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 62, 'total_tokens': 72, 'input_token_details': {}, 'output_token_details': {}}\ncontent='.\\n' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 63, 'total_tokens': 73, 'input_token_details': {}, 'output_token_details': {}}\ncontent='</think>' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 64, 'total_tokens': 74, 'input_token_details': {}, 'output_token_details': {}}\ncontent='\\n\\n' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 65, 'total_tokens': 75, 'input_token_details': {}, 'output_token_details': {}}\ncontent='Of' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 66, 'total_tokens': 76, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' course' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 67, 'total_tokens': 77, 'input_token_details': {}, 'output_token_details': {}}\ncontent='!' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 68, 'total_tokens': 78, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' I' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 69, 'total_tokens': 79, 'input_token_details': {}, 'output_token_details': {}}\ncontent=\"'d\" additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 70, 'total_tokens': 80, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' be' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 71, 'total_tokens': 81, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' happy' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 72, 'total_tokens': 82, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' to' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 73, 'total_tokens': 83, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' help' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 74, 'total_tokens': 84, 'input_token_details': {}, 'output_token_details': {}}\ncontent='.' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 75, 'total_tokens': 85, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' What' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 76, 'total_tokens': 86, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' do' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 77, 'total_tokens': 87, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' you' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 78, 'total_tokens': 88, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' need' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 79, 'total_tokens': 89, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' assistance' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 80, 'total_tokens': 90, 'input_token_details': {}, 'output_token_details': {}}\ncontent=' with' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 81, 'total_tokens': 91, 'input_token_details': {}, 'output_token_details': {}}\ncontent='?' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 82, 'total_tokens': 92, 'input_token_details': {}, 'output_token_details': {}}\ncontent='' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'deepseek-r1-distill-llama-70b'} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 83, 'tusage_metadata={'input_tokens': 10, 'output_tokens': 83, 'total_tokens': 93, 'input_token_details': {}, 'output_token_details': {}}\n\notal_tokens': 93, 'input_token_details': {}, 'output_token_details': {}}\ncontent='' additional_kwargs={} response_metadata={} id='run--c530b47b-0c0d-44c8-8de8-a35afdd6ff14' usage_metadata={'input_tokens': 10, 'output_tokens': 83, 'total_tokens': 93, 'input_token_details': {}, 'output_token_details': {}}\n\n\n\n\nFull response:\nOkay, the user just said \"hey can u help me.\" I should respond in a friendly and approachable way.\n\nI want to make sure they feel comfortable asking for help.\n\nMaybe I'll ask them to provide more details about what they need assistance with.\n\nKeeping it open-ended should encourage them to share more.\n</think>\n\nOf course! I'd be happy to help. What do you need assistance with?\n\nActual Token Usage:\nPrompt Tokens: 850\nCompletion Tokens: 3569\nTotal Tokens: 4419\n**********\n\n\n### Description\n\nError when couting the tokens      with get_openai_callback() as cb:\nbut it looks correct in usage_metadata={'input_tokens': 10, 'output_tokens': 83, 'total_tokens': 93, 'input_token_details': {}, 'output_token_details': {}}\ncan u tell me why ? \n\n### System Info\n\nfrom langchain.callbacks import get_openai_callback\nfrom langchain_openai import ChatOpenAI\n", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/31351/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/31351/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/30797", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/30797/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/30797/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/30797/events", "html_url": "https://github.com/langchain-ai/langchain/issues/30797", "id": 2989545747, "node_id": "I_kwDOIPDwls6yMNkT", "number": 30797, "title": "Support recursive tool schemas", "user": {"login": "ccurme", "id": 26529506, "node_id": "MDQ6VXNlcjI2NTI5NTA2", "avatar_url": "https://avatars.githubusercontent.com/u/26529506?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ccurme", "html_url": "https://github.com/ccurme", "followers_url": "https://api.github.com/users/ccurme/followers", "following_url": "https://api.github.com/users/ccurme/following{/other_user}", "gists_url": "https://api.github.com/users/ccurme/gists{/gist_id}", "starred_url": "https://api.github.com/users/ccurme/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ccurme/subscriptions", "organizations_url": "https://api.github.com/users/ccurme/orgs", "repos_url": "https://api.github.com/users/ccurme/repos", "events_url": "https://api.github.com/users/ccurme/events{/privacy}", "received_events_url": "https://api.github.com/users/ccurme/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 5680700839, "node_id": "LA_kwDOIPDwls8AAAABUpidpw", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/bug", "name": "bug", "color": "b60205", "default": true, "description": "Related to a bug, vulnerability, unexpected error with an existing feature"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2025-04-11T19:12:39Z", "updated_at": "2025-04-26T12:40:17Z", "closed_at": null, "author_association": "COLLABORATOR", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Privileged issue\n\n- [x] I am a LangChain maintainer, or was asked directly by a LangChain maintainer to create an issue here.\n\n### Issue Content\n\n`convert_to_openai_tool` will [dereference refs](https://github.com/langchain-ai/langchain/blob/2803a486614f220134059115f84674961aaf58d3/libs/core/langchain_core/utils/function_calling.py#L118), which interferes with schema generation for recursive schemas ([OpenAI](https://platform.openai.com/docs/guides/function-calling?api-mode=chat) explicitly supports recursive schemas, for example).\n\nExample:\n```python\nclass Recursive(BaseModel):\n    inner: Expr\n\nExpr = int | Recursive\n\nclass WrappedExpr(BaseModel):\n    wrapped: Expr\n\n\nconvert_to_openai_tool(WrappedExpr)\n```", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/30797/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/30797/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/30719", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/30719/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/30719/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/30719/events", "html_url": "https://github.com/langchain-ai/langchain/issues/30719", "id": 2978651788, "node_id": "I_kwDOIPDwls6xip6M", "number": 30719, "title": "`Graph.draw_png()` silently returns `None` with invalid node types, causing unexpected crash on usage", "user": {"login": "Alioth99", "id": 199570263, "node_id": "U_kgDOC-UzVw", "avatar_url": "https://avatars.githubusercontent.com/u/199570263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Alioth99", "html_url": "https://github.com/Alioth99", "followers_url": "https://api.github.com/users/Alioth99/followers", "following_url": "https://api.github.com/users/Alioth99/following{/other_user}", "gists_url": "https://api.github.com/users/Alioth99/gists{/gist_id}", "starred_url": "https://api.github.com/users/Alioth99/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Alioth99/subscriptions", "organizations_url": "https://api.github.com/users/Alioth99/orgs", "repos_url": "https://api.github.com/users/Alioth99/repos", "events_url": "https://api.github.com/users/Alioth99/events{/privacy}", "received_events_url": "https://api.github.com/users/Alioth99/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 5680700839, "node_id": "LA_kwDOIPDwls8AAAABUpidpw", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/bug", "name": "bug", "color": "b60205", "default": true, "description": "Related to a bug, vulnerability, unexpected error with an existing feature"}, {"id": 6411661606, "node_id": "LA_kwDOIPDwls8AAAABfioxJg", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/investigate", "name": "investigate", "color": "0e8a16", "default": false, "description": "Flagged for investigation"}], "state": "open", "locked": false, "assignee": {"login": "eyurtsev", "id": 3205522, "node_id": "MDQ6VXNlcjMyMDU1MjI=", "avatar_url": "https://avatars.githubusercontent.com/u/3205522?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eyurtsev", "html_url": "https://github.com/eyurtsev", "followers_url": "https://api.github.com/users/eyurtsev/followers", "following_url": "https://api.github.com/users/eyurtsev/following{/other_user}", "gists_url": "https://api.github.com/users/eyurtsev/gists{/gist_id}", "starred_url": "https://api.github.com/users/eyurtsev/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eyurtsev/subscriptions", "organizations_url": "https://api.github.com/users/eyurtsev/orgs", "repos_url": "https://api.github.com/users/eyurtsev/repos", "events_url": "https://api.github.com/users/eyurtsev/events{/privacy}", "received_events_url": "https://api.github.com/users/eyurtsev/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "assignees": [{"login": "eyurtsev", "id": 3205522, "node_id": "MDQ6VXNlcjMyMDU1MjI=", "avatar_url": "https://avatars.githubusercontent.com/u/3205522?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eyurtsev", "html_url": "https://github.com/eyurtsev", "followers_url": "https://api.github.com/users/eyurtsev/followers", "following_url": "https://api.github.com/users/eyurtsev/following{/other_user}", "gists_url": "https://api.github.com/users/eyurtsev/gists{/gist_id}", "starred_url": "https://api.github.com/users/eyurtsev/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eyurtsev/subscriptions", "organizations_url": "https://api.github.com/users/eyurtsev/orgs", "repos_url": "https://api.github.com/users/eyurtsev/repos", "events_url": "https://api.github.com/users/eyurtsev/events{/privacy}", "received_events_url": "https://api.github.com/users/eyurtsev/received_events", "type": "User", "user_view_type": "public", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2025-04-08T05:39:37Z", "updated_at": "2025-07-14T23:14:00Z", "closed_at": null, "author_association": "NONE", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Checked other resources\n\n- [x] I added a very descriptive title to this issue.\n- [x] I used the GitHub search to find a similar question and didn't find it.\n- [x] I am sure that this is a bug in LangChain rather than my code.\n- [x] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n- [x] I posted a self-contained, minimal, reproducible example. A maintainer can copy it and run it AS IS.\n\n### Example Code\n\n```python\nfrom langchain_core.runnables.graph import Graph\n\n# Test graph with invalid node types\ngraph = Graph()\nn1 = graph.add_node(object(), id=\"node1\")     # Node with invalid type\nn2 = graph.add_node(lambda x: x, id=\"node2\")   # Node as a lambda function\ngraph.add_edge(n1, n2, data=None, conditional=False)\n\n# Mermaid rendering works fine\nprint(\"Checking graph with invalid node types\")\nmermaid = graph.draw_mermaid(with_styles=False)\nassert isinstance(mermaid, str), \"Mermaid output should be string\"\nprint(\"Mermaid output:\", mermaid)\n\n# PNG rendering silently fails and returns None\npng_bytes = graph.draw_png(output_file_path=\"invalid_path.png\")\nprint(\"Checking PNG output length:\", len(png_bytes))  # <-- This line raises TypeError\nassert isinstance(png_bytes, bytes), \"PNG output should be bytes\"\n```\n\n### Error Message and Stack Trace (if applicable)\n\nChecking graph with invalid node types\nMermaid output: graph TD;\n    node1 --> node2;\nTraceback (most recent call last):\n  File \"repro.py\", line 15, in <module>\n    print(\"Checking PNG output length:\", len(png_bytes))\nTypeError: object of type 'NoneType' has no len()\n\n### Description\n\n- I'm trying to use the Graph class in langchain_core.runnables.graph to visualize a graph using .draw_png().\n\n- I expect it to return PNG bytes or raise a clear error if something is invalid.\n\n- Instead, .draw_png() silently returns None, and calling len() on the result causes a crash with TypeError.\n\nThis makes debugging difficult. There's no validation when adding nodes of invalid types (e.g., object() or lambdas), and .draw_png() fails without any error message, log, or warning. It would be helpful if the method raised a ValueError or returned an informative message on failure.\n\n### System Info\n\nSystem Information\n------------------\n> OS:  Linux\n> OS Version:  #107-Ubuntu SMP Wed Feb 7 13:26:48 UTC 2024\n> Python Version:  3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.50\n> langchain: 0.3.22\n> langchain_community: 0.3.13\n> langsmith: 0.3.23\n> langchain_anthropic: 0.3.1\n> langchain_aws: 0.2.2\n> langchain_chroma: 0.2.0\n> langchain_experimental: 0.3.2\n> langchain_fireworks: 0.2.6\n> langchain_google_vertexai: 2.0.5\n> langchain_groq: 0.2.2\n> langchain_mistralai: 0.2.4\n> langchain_openai: 0.2.14\n> langchain_text_splitters: 0.3.7\n> langchain_together: 0.2.0\n> langchain_unstructured: 0.1.5\n> langgraph_sdk: 0.1.34\n\nOptional packages not installed\n-------------------------------\n> langserve\n\nOther Dependencies\n------------------\n> aiohttp: 3.10.10\n> anthropic: 0.40.0\n> anthropic[vertexai]: Installed. No version info available.\n> async-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\n> boto3: 1.35.42\n> chromadb: 0.5.15\n> dataclasses-json: 0.6.7\n> defusedxml: 0.7.1\n> fastapi: 0.115.3\n> fireworks-ai: 0.15.7\n> google-cloud-aiplatform: 1.70.0\n> google-cloud-storage: 2.18.2\n> groq: 0.11.0\n> httpx: 0.28.1\n> httpx-sse: 0.4.0\n> jsonpatch<2.0,>=1.33: Installed. No version info available.\n> langchain-anthropic;: Installed. No version info available.\n> langchain-aws;: Installed. No version info available.\n> langchain-azure-ai;: Installed. No version info available.\n> langchain-cohere;: Installed. No version info available.\n> langchain-community;: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.45: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.49: Installed. No version info available.\n> langchain-deepseek;: Installed. No version info available.\n> langchain-fireworks;: Installed. No version info available.\n> langchain-google-genai;: Installed. No version info available.\n> langchain-google-vertexai;: Installed. No version info available.\n> langchain-groq;: Installed. No version info available.\n> langchain-huggingface;: Installed. No version info available.\n> langchain-mistralai;: Installed. No version info available.\n> langchain-ollama;: Installed. No version info available.\n> langchain-openai;: Installed. No version info available.\n> langchain-text-splitters<1.0.0,>=0.3.7: Installed. No version info available.\n> langchain-together;: Installed. No version info available.\n> langchain-xai;: Installed. No version info available.\n> langsmith-pyo3: Installed. No version info available.\n> langsmith<0.4,>=0.1.125: Installed. No version info available.\n> langsmith<0.4,>=0.1.17: Installed. No version info available.\n> numpy: 1.26.4\n> openai: 1.57.4\n> openai-agents: Installed. No version info available.\n> opentelemetry-api: 1.27.0\n> opentelemetry-exporter-otlp-proto-http: Installed. No version info available.\n> opentelemetry-sdk: 1.27.0\n> orjson: 3.10.16\n> packaging: 24.1\n> packaging<25,>=23.2: Installed. No version info available.\n> pydantic: 2.11.1\n> pydantic-settings: 2.6.0\n> pydantic<3.0.0,>=2.5.2;: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4;: Installed. No version info available.\n> pytest: 8.3.4\n> PyYAML: 6.0.2\n> PyYAML>=5.3: Installed. No version info available.\n> requests: 2.32.3\n> requests-toolbelt: 1.0.0\n> requests<3,>=2: Installed. No version info available.\n> rich: 13.9.3\n> SQLAlchemy: 2.0.40\n> SQLAlchemy<3,>=1.4: Installed. No version info available.\n> tenacity: 9.1.2\n> tenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\n> tiktoken: 0.8.0\n> tokenizers: 0.21.0\n> typing-extensions>=4.7: Installed. No version info available.\n> unstructured-client: 0.25.9\n> unstructured[all-docs]: Installed. No version info available.\n> zstandard: 0.23.0", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/30719/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/30719/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/30708", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/30708/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/30708/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/30708/events", "html_url": "https://github.com/langchain-ai/langchain/issues/30708", "id": 2977117738, "node_id": "I_kwDOIPDwls6xczYq", "number": 30708, "title": "Bug caused by `create_react_agent` method and runnable `astream_events`", "user": {"login": "deershark", "id": 29668663, "node_id": "MDQ6VXNlcjI5NjY4NjYz", "avatar_url": "https://avatars.githubusercontent.com/u/29668663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/deershark", "html_url": "https://github.com/deershark", "followers_url": "https://api.github.com/users/deershark/followers", "following_url": "https://api.github.com/users/deershark/following{/other_user}", "gists_url": "https://api.github.com/users/deershark/gists{/gist_id}", "starred_url": "https://api.github.com/users/deershark/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/deershark/subscriptions", "organizations_url": "https://api.github.com/users/deershark/orgs", "repos_url": "https://api.github.com/users/deershark/repos", "events_url": "https://api.github.com/users/deershark/events{/privacy}", "received_events_url": "https://api.github.com/users/deershark/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 5680700839, "node_id": "LA_kwDOIPDwls8AAAABUpidpw", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/bug", "name": "bug", "color": "b60205", "default": true, "description": "Related to a bug, vulnerability, unexpected error with an existing feature"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2025-04-07T10:17:41Z", "updated_at": "2025-07-16T15:17:31Z", "closed_at": null, "author_association": "CONTRIBUTOR", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Checked other resources\n\n- [x] This is a bug, not a usage question. For questions, please use GitHub Discussions.\n- [x] I added a clear and detailed title that summarizes the issue.\n- [x] I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n- [x] I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\n### Example Code\n\n```python\nimport asyncio\n\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import create_react_agent\n\n\n# Create an intentionally error-prone tool\n@tool\ndef error_prone_tool(input: str) -> str:\n    \"\"\"A tool that always raises an error for demonstration purposes.\"\"\"\n    return 1 / 0\n\n\n# Set up the agent\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\ntools = [error_prone_tool]\nagent = create_react_agent(model, tools)\n\n\nasync def demonstrate_issue():\n    # Stream events to show missing tool_error event\n    async for event in agent.astream_events(\n        {\"messages\": \"Please demonstrate a tool error\"},\n        version=\"v2\",\n    ):\n        print(f\"Event type: {event['event']}, Event name: {event['name']} Event data: {event['data']}\")\n        # Notice there's no 'tool_error' event type in the output\n\n        # This is the issue - when a tool fails, we should get a 'tool_error' event\n        # but currently we don't, making error handling difficult\n\n\nif __name__ == \"__main__\":\n    asyncio.run(demonstrate_issue())\n```\n\n### Error Message and Stack Trace (if applicable)\n\n```shell\n\n```\n\n### Description\n\nI am using the `create_react_agent` method to get all output events through `astream_events`. I want to get the error event to process and display the error information to the user.\nIf an error occurs in tool, there will be no `on_tool_end` event !!!!!\nAnd, I see that the documentation of this method does not mention the `error` type of event (such as `on_tool_error`).\nIs there any way to do this? If you do not use a custom callback.\n\nHere is the output of running\uff1a\n\n```log\n......\nEvent type: on_chain_start, Event name: tools Event data: {'input': {'messages': [HumanMessage(content='Please demonstrate a tool error', additional_kwargs={}, response_metadata={}, id='51b97b0a-2f8a-45c2-b6a8-a07f3c892882'), AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_U8rGfLpsdQAoXhGZ01R4DpWd', 'function': {'arguments': '{\"input\":\"Demonstrate tool error\"}', 'name': 'error_prone_tool'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b705f0c291'}, id='run-00652a81-d2b9-469f-b413-79919c7c7839', tool_calls=[{'name': 'error_prone_tool', 'args': {'input': 'Demonstrate tool error'}, 'id': 'call_U8rGfLpsdQAoXhGZ01R4DpWd', 'type': 'tool_call'}], usage_metadata={'input_tokens': 53, 'output_tokens': 20, 'total_tokens': 66, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0}})], 'is_last_step': False, 'remaining_steps': 23}}\n\nEvent type: on_tool_start, Event name: error_prone_tool Event data: {'input': {'input': 'Demonstrate tool error'}}\n\nEvent type: on_chain_start, Event name: _write Event data: {'input': {'messages': [ToolMessage(content=\"Error: ZeroDivisionError('division by zero')\\n Please fix your mistakes.\", name='error_prone_tool', tool_call_id='call_U8rGfLpsdQAoXhGZ01R4DpWd', status='error')]}}\nEvent type: on_chain_end, Event name: _write Event data: {'output': {'messages': [ToolMessage(content=\"Error: ZeroDivisionError('division by zero')\\n Please fix your mistakes.\", name='error_prone_tool', tool_call_id='call_U8rGfLpsdQAoXhGZ01R4DpWd', status='error')]}, 'input': {'messages': [ToolMessage(content=\"Error: ZeroDivisionError('division by zero')\\n Please fix your mistakes.\", name='error_prone_tool', tool_call_id='call_U8rGfLpsdQAoXhGZ01R4DpWd', status='error')]}}\nEvent type: on_chain_start, Event name: _write Event data: {'input': {'messages': [ToolMessage(content=\"Error: ZeroDivisionError('division by zero')\\n Please fix your mistakes.\", name='error_prone_tool', tool_call_id='call_U8rGfLpsdQAoXhGZ01R4DpWd', status='error')]}}\n......\n```\n\n### System Info\n\nSystem Information\n------------------\n> OS:  Linux\n> OS Version:  langchain-ai/langgraph#1 SMP PREEMPT_DYNAMIC Fri Mar  8 11:32:16 CST 2024\n> Python Version:  3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.46\n> langsmith: 0.3.18\n> langchain_mcp_adapters: 0.0.5\n> langgraph_test: Installed. No version info available.\n\nOptional packages not installed\n-------------------------------\n> langserve\n\nOther Dependencies\n------------------\n> httpx: 0.28.1\n> jsonpatch<2.0,>=1.33: Installed. No version info available.\n> langchain-core<0.4,>=0.3.36: Installed. No version info available.\n> langsmith-pyo3: Installed. No version info available.\n> langsmith<0.4,>=0.1.125: Installed. No version info available.\n> mcp<1.5,>=1.4.1: Installed. No version info available.\n> openai-agents: Installed. No version info available.\n> opentelemetry-api: Installed. No version info available.\n> opentelemetry-exporter-otlp-proto-http: Installed. No version info available.\n> opentelemetry-sdk: Installed. No version info available.\n> orjson: 3.10.15\n> packaging: 24.2\n> packaging<25,>=23.2: Installed. No version info available.\n> pydantic: 2.10.6\n> pydantic<3.0.0,>=2.5.2;: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4;: Installed. No version info available.\n> pytest: 8.3.4\n> PyYAML>=5.3: Installed. No version info available.\n> requests: 2.31.0\n> requests-toolbelt: 1.0.0\n> rich: Installed. No version info available.\n> tenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\n> typing-extensions>=4.7: Installed. No version info available.\n> zstandard: 0.23.0", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/30708/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/30708/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/29787", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/29787/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/29787/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/29787/events", "html_url": "https://github.com/langchain-ai/langchain/issues/29787", "id": 2851850096, "node_id": "I_kwDOIPDwls6p-8dw", "number": 29787, "title": "HuggingFaceEndpointEmbeddings fails with self-hosted inference server and huggingface-hub==0.28.1", "user": {"login": "shkarupa-alex", "id": 1289725, "node_id": "MDQ6VXNlcjEyODk3MjU=", "avatar_url": "https://avatars.githubusercontent.com/u/1289725?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shkarupa-alex", "html_url": "https://github.com/shkarupa-alex", "followers_url": "https://api.github.com/users/shkarupa-alex/followers", "following_url": "https://api.github.com/users/shkarupa-alex/following{/other_user}", "gists_url": "https://api.github.com/users/shkarupa-alex/gists{/gist_id}", "starred_url": "https://api.github.com/users/shkarupa-alex/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shkarupa-alex/subscriptions", "organizations_url": "https://api.github.com/users/shkarupa-alex/orgs", "repos_url": "https://api.github.com/users/shkarupa-alex/repos", "events_url": "https://api.github.com/users/shkarupa-alex/events{/privacy}", "received_events_url": "https://api.github.com/users/shkarupa-alex/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 5680700839, "node_id": "LA_kwDOIPDwls8AAAABUpidpw", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/bug", "name": "bug", "color": "b60205", "default": true, "description": "Related to a bug, vulnerability, unexpected error with an existing feature"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2025-02-13T18:51:34Z", "updated_at": "2025-05-15T09:49:02Z", "closed_at": null, "author_association": "CONTRIBUTOR", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Checked other resources\n\n- [x] I added a very descriptive title to this issue.\n- [x] I searched the LangChain documentation with the integrated search.\n- [x] I used the GitHub search to find a similar question and didn't find it.\n- [x] I am sure that this is a bug in LangChain rather than my code.\n- [x] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\nThe following code works well with `huggingface-hub==0.27.0`, but fails with the latest one (0.28.1)\n\n```python\nfrom langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings\n\n\nembedding_url = \"http://localhost:8081\" # this a docker container with ghcr.io/huggingface/text-embeddings-inference:cpu-sha-13dddbd\n\nembeddings = HuggingFaceEndpointEmbeddings(model=embedding_url)\nprint(embeddings.embed_documents([\"dummy_text\"]))\n```\n\n### Error Message and Stack Trace (if applicable)\n\n```python\n[/Users/alex/.pyenv/versions/3.11.2/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:131](http://localhost:8888/Users/alex/.pyenv/versions/3.11.2/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py#line=130): FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n  warnings.warn(warning_message, FutureWarning)\n---------------------------------------------------------------------------\nHTTPError                                 Traceback (most recent call last)\nFile [~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:406](http://localhost:8888/~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/huggingface_hub/utils/_http.py#line=405), in hf_raise_for_status(response, endpoint_name)\n    405 try:\n--> 406     response.raise_for_status()\n    407 except HTTPError as e:\n\nFile [~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/requests/models.py:1024](http://localhost:8888/~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/requests/models.py#line=1023), in Response.raise_for_status(self)\n   1023 if http_error_msg:\n-> 1024     raise HTTPError(http_error_msg, response=self)\n\nHTTPError: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/pipeline/feature-extraction/facebook/bart-base\n\nThe above exception was the direct cause of the following exception:\n\nHfHubHTTPError                            Traceback (most recent call last)\nCell In[1], line 13\n     10 embedding_url = \"http://localhost:8081/\" # this a docker container with ghcr.io[/huggingface/text-embeddings-inference](http://localhost:8888/huggingface/text-embeddings-inference):cpu-sha-13dddbd\n     12 embeddings = HuggingFaceEndpointEmbeddings(model=embedding_url)\n---> 13 print(embeddings.embed_documents([\"dummy_text\"]))\n\nFile [~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/langchain_huggingface/embeddings/huggingface_endpoint.py:112](http://localhost:8888/~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/langchain_huggingface/embeddings/huggingface_endpoint.py#line=111), in HuggingFaceEndpointEmbeddings.embed_documents(self, texts)\n    110 _model_kwargs = self.model_kwargs or {}\n    111 #  api doc: https://huggingface.github.io/text-embeddings-inference/#/Text%20Embeddings%20Inference/embed\n--> 112 responses = self.client.post(\n    113     json={\"inputs\": texts, **_model_kwargs}, task=self.task\n    114 )\n    115 return json.loads(responses.decode())\n\nFile ~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:132, in _deprecate_method.<locals>._inner_deprecate_method.<locals>.inner_f(*args, **kwargs)\n    130     warning_message += \" \" + message\n    131 warnings.warn(warning_message, FutureWarning)\n--> 132 return f(*args, **kwargs)\n\nFile [~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:272](http://localhost:8888/~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/huggingface_hub/inference/_client.py#line=271), in InferenceClient.post(self, json, data, model, task, stream)\n    270 url = provider_helper.build_url(provider_helper.map_model(model))\n    271 headers = provider_helper.prepare_headers(headers=self.headers, api_key=self.token)\n--> 272 return self._inner_post(\n    273     request_parameters=RequestParameters(\n    274         url=url,\n    275         task=task or \"unknown\",\n    276         model=model or \"unknown\",\n    277         json=json,\n    278         data=data,\n    279         headers=headers,\n    280     ),\n    281     stream=stream,\n    282 )\n\nFile [~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:327](http://localhost:8888/~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/huggingface_hub/inference/_client.py#line=326), in InferenceClient._inner_post(self, request_parameters, stream)\n    324         raise InferenceTimeoutError(f\"Inference call timed out: {request_parameters.url}\") from error  # type: ignore\n    326 try:\n--> 327     hf_raise_for_status(response)\n    328     return response.iter_lines() if stream else response.content\n    329 except HTTPError as error:\n\nFile [~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:477](http://localhost:8888/~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/huggingface_hub/utils/_http.py#line=476), in hf_raise_for_status(response, endpoint_name)\n    473     raise _format(HfHubHTTPError, message, response) from e\n    475 # Convert `HTTPError` into a `HfHubHTTPError` to display request information\n    476 # as well (request id and[/or](http://localhost:8888/or) server error message)\n--> 477 raise _format(HfHubHTTPError, str(e), response) from e\n\nHfHubHTTPError: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/pipeline/feature-extraction/facebook/bart-base (Request ID: RzoUHJ)\n\nTooManyRequests: Please log in or use a HF access token\n```\n\n### Description\n\n`HuggingFaceEndpointEmbeddings` uses `InferenceClient` from `huggingface_hub`.\nBut since the last update, it always send embedding requests to HF and not in my local embedding server.\n\n### System Info\n\nSystem Information\n------------------\n> OS:  Darwin\n> OS Version:  Darwin Kernel Version 23.5.0: Wed May  1 20:09:52 PDT 2024; root:xnu-10063.121.3~5/RELEASE_X86_64\n> Python Version:  3.11.2 (main, Apr 17 2023, 23:33:17) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.35\n> langchain: 0.3.18\n> langchain_community: 0.3.16\n> langsmith: 0.3.3\n> langchain_experimental: 0.3.4\n> langchain_huggingface: 0.1.2\n> langchain_ollama: 0.2.3\n> langchain_openai: 0.3.3\n> langchain_qdrant: 0.2.0\n> langchain_text_splitters: 0.3.6\n> langgraph_sdk: 0.1.51\n\nOptional packages not installed\n-------------------------------\n> langserve\n\nOther Dependencies\n------------------\n> aiohttp: 3.8.4\n> aiohttp<4.0.0,>=3.8.3: Installed. No version info available.\n> async-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\n> dataclasses-json: 0.6.7\n> fastembed: Installed. No version info available.\n> httpx: 0.27.2\n> httpx-sse: 0.4.0\n> huggingface-hub: 0.28.1\n> jsonpatch<2.0,>=1.33: Installed. No version info available.\n> langchain-anthropic;: Installed. No version info available.\n> langchain-aws;: Installed. No version info available.\n> langchain-cohere;: Installed. No version info available.\n> langchain-community;: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.34: Installed. No version info available.\n> langchain-deepseek;: Installed. No version info available.\n> langchain-fireworks;: Installed. No version info available.\n> langchain-google-genai;: Installed. No version info available.\n> langchain-google-vertexai;: Installed. No version info available.\n> langchain-groq;: Installed. No version info available.\n> langchain-huggingface;: Installed. No version info available.\n> langchain-mistralai;: Installed. No version info available.\n> langchain-ollama;: Installed. No version info available.\n> langchain-openai;: Installed. No version info available.\n> langchain-text-splitters<1.0.0,>=0.3.6: Installed. No version info available.\n> langchain-together;: Installed. No version info available.\n> langsmith-pyo3: Installed. No version info available.\n> langsmith<0.4,>=0.1.125: Installed. No version info available.\n> langsmith<0.4,>=0.1.17: Installed. No version info available.\n> numpy: 1.26.4\n> numpy<2,>=1.26.4;: Installed. No version info available.\n> numpy<3,>=1.26.2;: Installed. No version info available.\n> ollama: 0.4.7\n> openai: 1.61.0\n> orjson: 3.10.15\n> packaging<25,>=23.2: Installed. No version info available.\n> pydantic: 2.10.4\n> pydantic-settings: 2.7.1\n> pydantic<3.0.0,>=2.5.2;: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4;: Installed. No version info available.\n> pytest: 7.4.0\n> PyYAML: 6.0\n> PyYAML>=5.3: Installed. No version info available.\n> qdrant-client: 1.13.2\n> requests: 2.32.3\n> requests-toolbelt: 1.0.0\n> requests<3,>=2: Installed. No version info available.\n> rich: 13.4.2\n> sentence-transformers: 3.4.1\n> SQLAlchemy: 2.0.38\n> SQLAlchemy<3,>=1.4: Installed. No version info available.\n> tenacity: 8.2.3\n> tenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\n> tenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\n> tiktoken: 0.8.0\n> tokenizers: 0.21.0\n> transformers: 4.48.3\n> typing-extensions>=4.7: Installed. No version info available.\n> zstandard: 0.23.0", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/29787/reactions", "total_count": 3, "+1": 3, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/29787/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/29153", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/29153/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/29153/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/29153/events", "html_url": "https://github.com/langchain-ai/langchain/issues/29153", "id": 2782065982, "node_id": "I_kwDOIPDwls6l0vU-", "number": 29153, "title": "Offset by 1 bug on RecursiveJsonSplitter::split_json() function", "user": {"login": "blupants", "id": 52946490, "node_id": "MDQ6VXNlcjUyOTQ2NDkw", "avatar_url": "https://avatars.githubusercontent.com/u/52946490?v=4", "gravatar_id": "", "url": "https://api.github.com/users/blupants", "html_url": "https://github.com/blupants", "followers_url": "https://api.github.com/users/blupants/followers", "following_url": "https://api.github.com/users/blupants/following{/other_user}", "gists_url": "https://api.github.com/users/blupants/gists{/gist_id}", "starred_url": "https://api.github.com/users/blupants/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/blupants/subscriptions", "organizations_url": "https://api.github.com/users/blupants/orgs", "repos_url": "https://api.github.com/users/blupants/repos", "events_url": "https://api.github.com/users/blupants/events{/privacy}", "received_events_url": "https://api.github.com/users/blupants/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 5680700839, "node_id": "LA_kwDOIPDwls8AAAABUpidpw", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/bug", "name": "bug", "color": "b60205", "default": true, "description": "Related to a bug, vulnerability, unexpected error with an existing feature"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2025-01-11T18:35:56Z", "updated_at": "2025-04-13T21:46:11Z", "closed_at": null, "author_association": "NONE", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Checked other resources\r\n\r\n- [X] I added a very descriptive title to this issue.\r\n- [X] I searched the LangChain documentation with the integrated search.\r\n- [X] I used the GitHub search to find a similar question and didn't find it.\r\n- [X] I am sure that this is a bug in LangChain rather than my code.\r\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\r\n\r\n### Example Code\r\n\r\n```python\r\nfrom langchain_text_splitters import RecursiveJsonSplitter\r\n\r\n\r\ninput_data = {\r\n  \"projects\": {\r\n    \"AS\": {\r\n      \"AS-1\": {}\r\n    },\r\n    \"DLP\": {\r\n      \"DLP-7\": {},\r\n      \"DLP-6\": {},\r\n      \"DLP-5\": {},\r\n      \"DLP-4\": {},\r\n      \"DLP-3\": {},\r\n      \"DLP-2\": {},\r\n      \"DLP-1\": {}\r\n    },\r\n    \"GTMS\": {\r\n      \"GTMS-22\": {},\r\n      \"GTMS-21\": {},\r\n      \"GTMS-20\": {},\r\n      \"GTMS-19\": {},\r\n      \"GTMS-18\": {},\r\n      \"GTMS-17\": {},\r\n      \"GTMS-16\": {},\r\n      \"GTMS-15\": {},\r\n      \"GTMS-14\": {},\r\n      \"GTMS-13\": {},\r\n      \"GTMS-12\": {},\r\n      \"GTMS-11\": {},\r\n      \"GTMS-10\": {},\r\n      \"GTMS-9\": {},\r\n      \"GTMS-8\": {},\r\n      \"GTMS-7\": {},\r\n      \"GTMS-6\": {},\r\n      \"GTMS-5\": {},\r\n      \"GTMS-4\": {},\r\n      \"GTMS-3\": {},\r\n      \"GTMS-2\": {},\r\n      \"GTMS-1\": {}\r\n    },\r\n    \"IT\": {\r\n      \"IT-3\": {},\r\n      \"IT-2\": {},\r\n      \"IT-1\": {}\r\n    },\r\n    \"ITSAMPLE\": {\r\n      \"ITSAMPLE-12\": {},\r\n      \"ITSAMPLE-11\": {},\r\n      \"ITSAMPLE-10\": {},\r\n      \"ITSAMPLE-9\": {},\r\n      \"ITSAMPLE-8\": {},\r\n      \"ITSAMPLE-7\": {},\r\n      \"ITSAMPLE-6\": {},\r\n      \"ITSAMPLE-5\": {},\r\n      \"ITSAMPLE-4\": {},\r\n      \"ITSAMPLE-3\": {},\r\n      \"ITSAMPLE-2\": {},\r\n      \"ITSAMPLE-1\": {}\r\n    },\r\n    \"MAR\": {\r\n      \"MAR-2\": {},\r\n      \"MAR-1\": {}\r\n    }\r\n  }\r\n}\r\n\r\nsplitter = RecursiveJsonSplitter(max_chunk_size=216)\r\njson_chunks = splitter.split_json(json_data=input_data)\r\n\r\ninput_data_DLP_5 = input_data.get(\"projects\", {}).get(\"DLP\", {}).get(\"DLP-5\", None)\r\ninput_data_GTMS_10 = input_data.get(\"projects\", {}).get(\"GTMS\", {}).get(\"GTMS-10\", None)\r\ninput_data_ITSAMPLE_2 = input_data.get(\"projects\", {}).get(\"ITSAMPLE\", {}).get(\"ITSAMPLE-2\", None)\r\n\r\nchunk_DLP_5 = None\r\nchunk_GTMS_10 = None\r\nchunk_ITSAMPLE_2 = None\r\n\r\nfor chunk in json_chunks:\r\n    print(chunk)\r\n    node = chunk.get(\"projects\", {}).get(\"DLP\", {}).get(\"DLP-5\", None)\r\n    if isinstance(node, dict):\r\n        chunk_DLP_5 = node\r\n    node = chunk.get(\"projects\", {}).get(\"GTMS\", {}).get(\"GTMS-10\", None)\r\n    if isinstance(node, dict):\r\n        chunk_GTMS_10 = node\r\n    node = chunk.get(\"projects\", {}).get(\"ITSAMPLE\", {}).get(\"ITSAMPLE-2\", None)\r\n    if isinstance(node, dict):\r\n        chunk_ITSAMPLE_2 = node\r\n\r\nprint(\"\\nRESULTS:\")\r\nif isinstance(chunk_DLP_5, dict):\r\n    print(f\"[PASS] - Node DLP-5 was found both in input_data and json_chunks\")\r\nelse:\r\n    print(f\"[TEST FAILED] - Node DLP-5 from input_data was NOT FOUND in json_chunks\")\r\n\r\nif isinstance(chunk_GTMS_10, dict):\r\n    print(f\"[PASS] - Node GTMS-10 was found both in input_data and json_chunks\")\r\nelse:\r\n    print(f\"[TEST FAILED] - Node GTMS-10 from input_data was NOT FOUND in json_chunks\")\r\n\r\nif isinstance(chunk_ITSAMPLE_2, dict):\r\n    print(f\"[PASS] - Node ITSAMPLE-2 was found both in input_data and json_chunks\")\r\nelse:\r\n    print(f\"[TEST FAILED] - Node ITSAMPLE-2 from input_data was NOT FOUND in json_chunks\")\r\n```\r\n\r\n### Error Message and Stack Trace (if applicable)\r\n\r\n_No response_\r\n\r\n### Description\r\n\r\nI am trying to use `langchain_text_splitters` library to split JSON content using the function `RecursiveJsonSplitter::split_json()`\r\n\r\nFor most cases it works, however I am experiencing some data being lost depending on the input JSON and the chunk size I am using.\r\n\r\nI was able to consistently replicate the issue for the input JSON provided on my sample code. I always get the nodes \"GTMS-10\" and \"ITSAMPLE-2\" discarded when I split the JSON using `max_chunk_size=216`.\r\n\r\nI noticed this issue always occurs with nodes that would be on the edge of the chunks. When you run my sample code, it will print all the 5 chunks generated:\r\n```\r\npython split_json_bug.py \r\n\r\n{'projects': {'AS': {'AS-1': {}}, 'DLP': {'DLP-7': {}, 'DLP-6': {}, 'DLP-5': {}, 'DLP-4': {}, 'DLP-3': {}, 'DLP-2': {}, 'DLP-1': {}}}}\r\n{'projects': {'GTMS': {'GTMS-22': {}, 'GTMS-21': {}, 'GTMS-20': {}, 'GTMS-19': {}, 'GTMS-18': {}, 'GTMS-17': {}, 'GTMS-16': {}, 'GTMS-15': {}, 'GTMS-14': {}, 'GTMS-13': {}, 'GTMS-12': {}, 'GTMS-11': {}}}}\r\n{'projects': {'GTMS': {'GTMS-9': {}, 'GTMS-8': {}, 'GTMS-7': {}, 'GTMS-6': {}, 'GTMS-5': {}, 'GTMS-4': {}, 'GTMS-3': {}, 'GTMS-2': {}, 'GTMS-1': {}}, 'IT': {'IT-3': {}, 'IT-2': {}, 'IT-1': {}}}}\r\n{'projects': {'ITSAMPLE': {'ITSAMPLE-12': {}, 'ITSAMPLE-11': {}, 'ITSAMPLE-10': {}, 'ITSAMPLE-9': {}, 'ITSAMPLE-8': {}, 'ITSAMPLE-7': {}, 'ITSAMPLE-6': {}, 'ITSAMPLE-5': {}, 'ITSAMPLE-4': {}, 'ITSAMPLE-3': {}}}}\r\n{'projects': {'ITSAMPLE': {'ITSAMPLE-1': {}}, 'MAR': {'MAR-2': {}, 'MAR-1': {}}}}\r\n\r\nRESULTS:\r\n[PASS] - Node DLP-5 was found both in input_data and json_chunks\r\n[TEST FAILED] - Node GTMS-10 from input_data was NOT FOUND in json_chunks\r\n[TEST FAILED] - Node ITSAMPLE-2 from input_data was NOT FOUND in json_chunks\r\n\r\n```\r\nPlease, noticed that the 2nd chunk ends with node \"GTMS-11\" and the 3rd chunk starts with \"GTMS-9\". Same thing for chunks number 4 (ends with \"ITSAMPLE-3\") and chunk number 5 (starts with \"ITSAMPLE-1\")\r\n\r\nBecause the chunks \"GTMS-10\" and \"ITSAMPLE-2\" were lost on the edges of chunks, I believe that might a case of an \"offset by 1 bug\" on the RecursiveJsonSplitter::split_json() Python function.\r\n\r\nSince I am calling it exactly how it is described in the [documentation](https://python.langchain.com/docs/how_to/recursive_json_splitter/#basic-usage) and I couldn't find any bug and discussion mentioning it, I thought I should file a bug for it.\r\n\r\n \r\n\r\n### System Info\r\n\r\n```console\r\n(.venv) user@User-MacBook-Air split_json_bug % python -m langchain_core.sys_info\r\n\r\nSystem Information\r\n------------------\r\n> OS:  Darwin\r\n> OS Version:  Darwin Kernel Version 23.6.0: Thu Sep 12 23:34:49 PDT 2024; root:xnu-10063.141.1.701.1~1/RELEASE_X86_64\r\n> Python Version:  3.11.9 (main, Apr  2 2024, 08:25:04) [Clang 15.0.0 (clang-1500.3.9.4)]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.29\r\n> langsmith: 0.2.10\r\n> langchain_text_splitters: 0.3.5\r\n\r\nOptional packages not installed\r\n-------------------------------\r\n> langserve\r\n\r\nOther Dependencies\r\n------------------\r\n> httpx: 0.28.1\r\n> jsonpatch: 1.33\r\n> langsmith-pyo3: Installed. No version info available.\r\n> orjson: 3.10.14\r\n> packaging: 24.2\r\n> pydantic: 2.10.5\r\n> PyYAML: 6.0.2\r\n> requests: 2.32.3\r\n> requests-toolbelt: 1.0.0\r\n> tenacity: 9.0.0\r\n> typing-extensions: 4.12.2\r\n> zstandard: Installed. No version info available.\r\n```\r\n\r\n```console\r\n(.venv) user@User-MacBook-Air split_json_bug % pip freeze\r\nannotated-types==0.7.0\r\nanyio==4.8.0\r\ncertifi==2024.12.14\r\ncharset-normalizer==3.4.1\r\nh11==0.14.0\r\nhttpcore==1.0.7\r\nhttpx==0.28.1\r\nidna==3.10\r\njsonpatch==1.33\r\njsonpointer==3.0.0\r\nlangchain-core==0.3.29\r\nlangchain-text-splitters==0.3.5\r\nlangsmith==0.2.10\r\norjson==3.10.14\r\npackaging==24.2\r\npydantic==2.10.5\r\npydantic_core==2.27.2\r\nPyYAML==6.0.2\r\nrequests==2.32.3\r\nrequests-toolbelt==1.0.0\r\nsniffio==1.3.1\r\ntenacity==9.0.0\r\ntyping_extensions==4.12.2\r\nurllib3==2.3.0\r\n```", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/29153/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/29153/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}]}