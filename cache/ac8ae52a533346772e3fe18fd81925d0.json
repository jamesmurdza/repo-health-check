{"total_count": 8684, "incomplete_results": false, "items": [{"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33100", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33100/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33100/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33100/events", "html_url": "https://github.com/langchain-ai/langchain/issues/33100", "id": 3451730522, "node_id": "I_kwDOIPDwls7NvTpa", "number": 33100, "title": "Support for Sarvam Chat Model Integration", "user": {"login": "Nightwing-77", "id": 232590572, "node_id": "U_kgDODd0M7A", "avatar_url": "https://avatars.githubusercontent.com/u/232590572?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Nightwing-77", "html_url": "https://github.com/Nightwing-77", "followers_url": "https://api.github.com/users/Nightwing-77/followers", "following_url": "https://api.github.com/users/Nightwing-77/following{/other_user}", "gists_url": "https://api.github.com/users/Nightwing-77/gists{/gist_id}", "starred_url": "https://api.github.com/users/Nightwing-77/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Nightwing-77/subscriptions", "organizations_url": "https://api.github.com/users/Nightwing-77/orgs", "repos_url": "https://api.github.com/users/Nightwing-77/repos", "events_url": "https://api.github.com/users/Nightwing-77/events{/privacy}", "received_events_url": "https://api.github.com/users/Nightwing-77/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 9260554836, "node_id": "LA_kwDOIPDwls8AAAACJ_jaVA", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/feature%20request", "name": "feature request", "color": "058a42", "default": false, "description": "request for an enhancement / additional functionality"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2025-09-25T03:31:50Z", "updated_at": "2025-09-25T12:39:59Z", "closed_at": null, "author_association": "NONE", "type": {"id": 18879553, "node_id": "IT_kwDOB43M6c4BIBRB", "name": "Feature", "description": "A request, idea, or new functionality", "color": "blue", "created_at": "2024-02-16T01:43:34Z", "updated_at": "2024-10-08T21:10:19Z", "is_enabled": true}, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Checked other resources\n\n- [x] This is a feature request, not a bug report or usage question.\n- [x] I added a clear and descriptive title that summarizes the feature request.\n- [x] I used the GitHub search to find a similar feature request and didn't find it.\n- [x] I checked the LangChain documentation and API reference to see if this feature already exists.\n- [x] This is not related to the langchain-community package.\n\n### Feature Description\n\nIt would be great if LangChain could add support for Sarvam AI\n chat models. Sarvam provides LLMs optimized for Indian Languages understanding and efficiency, which could be valuable for a wide range of LangChain applications.\n\n\n### Use Case\n\nSarvam offers strong multilingual capabilities (especially for Indic and low-resource languages).\n\nHaving it as a first-class ChatModel in LangChain would enable developers to use it seamlessly within chains, agents, and RAG pipelines.\n\nThis would expand the ecosystem\u2019s coverage of open/accessible models.\n\n### Proposed Solution\n\nAdd a new integration in langchain/chat_models similar to existing providers.\n\nImplement SarvamChat class following the BaseChatModel interface.\n\nSupport configuration via API key and model parameters (temperature, max tokens, etc.).\n\n#### Also, if this feature would be appreciated, I\u2019d be happy to work on the implementation and open a PR.\n \n\n### Alternatives Considered\n\nRight now, developers would need to write custom wrappers outside LangChain. An official integration would standardize usage and improve developer experience\n\n### Additional Context\n\n_No response_", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33100/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33100/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33092", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33092/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33092/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33092/events", "html_url": "https://github.com/langchain-ai/langchain/issues/33092", "id": 3450785704, "node_id": "I_kwDOIPDwls7Nrs-o", "number": 33092, "title": "Support for decentralized agent-to-agent communication (HyperCortex Mesh Protocol, HMP)", "user": {"login": "kagvi13", "id": 219128991, "node_id": "U_kgDODQ-knw", "avatar_url": "https://avatars.githubusercontent.com/u/219128991?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kagvi13", "html_url": "https://github.com/kagvi13", "followers_url": "https://api.github.com/users/kagvi13/followers", "following_url": "https://api.github.com/users/kagvi13/following{/other_user}", "gists_url": "https://api.github.com/users/kagvi13/gists{/gist_id}", "starred_url": "https://api.github.com/users/kagvi13/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kagvi13/subscriptions", "organizations_url": "https://api.github.com/users/kagvi13/orgs", "repos_url": "https://api.github.com/users/kagvi13/repos", "events_url": "https://api.github.com/users/kagvi13/events{/privacy}", "received_events_url": "https://api.github.com/users/kagvi13/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 9260554836, "node_id": "LA_kwDOIPDwls8AAAACJ_jaVA", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/feature%20request", "name": "feature request", "color": "058a42", "default": false, "description": "request for an enhancement / additional functionality"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2025-09-24T20:01:11Z", "updated_at": "2025-09-24T20:01:11Z", "closed_at": null, "author_association": "NONE", "type": {"id": 18879553, "node_id": "IT_kwDOB43M6c4BIBRB", "name": "Feature", "description": "A request, idea, or new functionality", "color": "blue", "created_at": "2024-02-16T01:43:34Z", "updated_at": "2024-10-08T21:10:19Z", "is_enabled": true}, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Checked other resources\n\n- [x] This is a feature request, not a bug report or usage question.\n- [x] I added a clear and descriptive title that summarizes the feature request.\n- [x] I used the GitHub search to find a similar feature request and didn't find it.\n- [x] I checked the LangChain documentation and API reference to see if this feature already exists.\n- [x] This is not related to the langchain-community package.\n\n### Feature Description\n\nLangChain currently focuses on connecting LLMs via chains and tools for single-agent workflows.\nWe propose adding support for **decentralized multi-agent communication**, enabling agents to exchange knowledge, maintain cognitive diaries, and validate decisions in a distributed way via [HyperCortex Mesh Protocol (HMP)](https://github.com/kagvi13/HMP).\n\n### Use Case\n\n* Agents in different LangChain workflows could discover each other and share processed knowledge, memory traces, and reasoning strategies.\n* Example: one chain performing legal analysis could query another chain\u2019s reasoning history to improve accuracy.\n* HMP enables **persistent semantic memory, cognitive journaling, and collaboration**, creating a mesh of interoperable agents.\n\n### Proposed Solution\n\n* Implement optional HMP support as a plugin or extension layer.\n* Provide interfaces for chains and tools to register as HMP nodes (either as Cognitive Core or Cognitive Shell).\n* Integrate shared cognitive diaries and semantic graphs for interoperability between agents.\n\n### Alternatives Considered\n\n* Using only embeddings or raw outputs: limited interoperability and knowledge sharing.\n* Custom peer-to-peer messaging between chains: lacks standardized cognitive structures and ethics framework.\n\n### Additional Context\n\n* HMP is already prototyped with reference docs:\n\n  * [HMP-0004-v4.1.md](https://github.com/kagvi13/HMP/blob/main/docs/HMP-0004-v4.1.md) \u2014 protocol specification\n  * [HMP-Ethics.md](https://github.com/kagvi13/HMP/blob/main/docs/HMP-Ethics.md) \u2014 ethics and validation\n  * [dht\\_protocol.md](https://github.com/kagvi13/HMP/blob/main/docs/dht_protocol.md) \u2014 P2P discovery\n  * [HMP-agent-REPL-cycle.md](https://github.com/kagvi13/HMP/blob/main/docs/HMP-agent-REPL-cycle.md) \u2014 example agent cycle", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33092/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33092/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33083", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33083/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33083/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33083/events", "html_url": "https://github.com/langchain-ai/langchain/issues/33083", "id": 3448538800, "node_id": "I_kwDOIPDwls7NjIaw", "number": 33083, "title": "llama_decode returns -1 when trying to add documents to a PGVector (store)", "user": {"login": "sidietz", "id": 31989328, "node_id": "MDQ6VXNlcjMxOTg5MzI4", "avatar_url": "https://avatars.githubusercontent.com/u/31989328?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sidietz", "html_url": "https://github.com/sidietz", "followers_url": "https://api.github.com/users/sidietz/followers", "following_url": "https://api.github.com/users/sidietz/following{/other_user}", "gists_url": "https://api.github.com/users/sidietz/gists{/gist_id}", "starred_url": "https://api.github.com/users/sidietz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sidietz/subscriptions", "organizations_url": "https://api.github.com/users/sidietz/orgs", "repos_url": "https://api.github.com/users/sidietz/repos", "events_url": "https://api.github.com/users/sidietz/events{/privacy}", "received_events_url": "https://api.github.com/users/sidietz/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 5680700839, "node_id": "LA_kwDOIPDwls8AAAABUpidpw", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/bug", "name": "bug", "color": "b60205", "default": true, "description": "Related to a bug, vulnerability, unexpected error with an existing feature"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2025-09-24T09:27:54Z", "updated_at": "2025-09-24T09:50:25Z", "closed_at": null, "author_association": "NONE", "type": {"id": 18879550, "node_id": "IT_kwDOB43M6c4BIBQ-", "name": "Bug", "description": "An unexpected problem or behavior", "color": "red", "created_at": "2024-02-16T01:43:33Z", "updated_at": "2024-07-26T15:00:33Z", "is_enabled": true}, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Checked other resources\n\n- [x] This is a bug, not a usage question.\n- [x] I added a clear and descriptive title that summarizes this issue.\n- [x] I used the GitHub search to find a similar question and didn't find it.\n- [x] I am sure that this is a bug in LangChain rather than my code.\n- [x] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n- [x] This is not related to the langchain-community package.\n- [x] I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n- [x] I posted a self-contained, minimal, reproducible example. A maintainer can copy it and run it AS IS.\n\n### Example Code\n\nExample:\n```\nfrom langchain_community.embeddings import LlamaCppEmbeddings\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_postgres import PGVector\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nimport bs4\nfrom langchain import hub\nfrom langchain_community.document_loaders import WebBaseLoader\n\nllama = LlamaCppEmbeddings(model_path=\"/path/to/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-Q4_K_M.gguf\", n_gpu_layers=99)\n\nloader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),   \nbs_kwargs=dict(        parse_only=bs4.SoupStrainer(            class_=(\"post-content\", \"post-title\", \"post-header\")        )    ),) \ndocs = loader.load() # from the rag example [1]\n\nvector_store = PGVector(embeddings=llama, collection_name=\"test\", connection=\"postgresql+psycopg://rag:PW@localhost:5432\", create_extension=False)\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)\nall_splits = text_splitter.split_documents(docs)\nids = vector_store.add_documents(documents=all_splits)\n```\n\n[1]\nhttps://python.langchain.com/docs/tutorials/rag/\n\n### Error Message and Stack Trace (if applicable)\n\n```\ninit: invalid seq_id[204][0] = 1 >= 1\ndecode: failed to initialize batch\nllama_decode: failed to decode, ret = -1\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[15], line 3\n      1 text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)\n      2 all_splits = text_splitter.split_documents(docs)\n----> 3 ids = vector_store.add_documents(documents=all_splits)\n\nFile [/opt/anaconda3/envs/rag2/lib/python3.13/site-packages/langchain_core/vectorstores/base.py:279](http://localhost:8888/opt/anaconda3/envs/rag2/lib/python3.13/site-packages/langchain_core/vectorstores/base.py#line=278), in VectorStore.add_documents(self, documents, **kwargs)\n    277     texts = [doc.page_content for doc in documents]\n    278     metadatas = [doc.metadata for doc in documents]\n--> 279     return self.add_texts(texts, metadatas, **kwargs)\n    280 msg = (\n    281     f\"`add_documents` and `add_texts` has not been implemented \"\n    282     f\"for {self.__class__.__name__} \"\n    283 )\n    284 raise NotImplementedError(msg)\n\nFile [/opt/anaconda3/envs/rag2/lib/python3.13/site-packages/langchain_postgres/vectorstores.py:885](http://localhost:8888/opt/anaconda3/envs/rag2/lib/python3.13/site-packages/langchain_postgres/vectorstores.py#line=884), in PGVector.add_texts(self, texts, metadatas, ids, **kwargs)\n    883 assert not self._async_engine, \"This method must be called without async_mode\"\n    884 texts_ = list(texts)\n--> 885 embeddings = self.embedding_function.embed_documents(texts_)\n    886 return self.add_embeddings(\n    887     texts=texts_,\n    888     embeddings=list(embeddings),\n   (...)    891     **kwargs,\n    892 )\n\nFile [/opt/anaconda3/envs/rag2/lib/python3.13/site-packages/langchain_community/embeddings/llamacpp.py:119](http://localhost:8888/opt/anaconda3/envs/rag2/lib/python3.13/site-packages/langchain_community/embeddings/llamacpp.py#line=118), in LlamaCppEmbeddings.embed_documents(self, texts)\n    110 def embed_documents(self, texts: List[str]) -> List[List[float]]:\n    111     \"\"\"Embed a list of documents using the Llama model.\n    112 \n    113     Args:\n   (...)    117         List of embeddings, one for each text.\n    118     \"\"\"\n--> 119     embeddings = self.client.create_embedding(texts)\n    120     final_embeddings = []\n    121     for e in embeddings[\"data\"]:\n\nFile [/opt/anaconda3/envs/rag2/lib/python3.13/site-packages/llama_cpp/llama.py:980](http://localhost:8888/opt/anaconda3/envs/rag2/lib/python3.13/site-packages/llama_cpp/llama.py#line=979), in Llama.create_embedding(self, input, model)\n    978 embeds: Union[List[List[float]], List[List[List[float]]]]\n    979 total_tokens: int\n--> 980 embeds, total_tokens = self.embed(input, return_count=True)  # type: ignore\n    982 # convert to CreateEmbeddingResponse\n    983 data: List[Embedding] = [\n    984     {\n    985         \"object\": \"embedding\",\n   (...)    989     for idx, emb in enumerate(embeds)\n    990 ]\n\nFile [/opt/anaconda3/envs/rag2/lib/python3.13/site-packages/llama_cpp/llama.py:1094](http://localhost:8888/opt/anaconda3/envs/rag2/lib/python3.13/site-packages/llama_cpp/llama.py#line=1093), in Llama.embed(self, input, normalize, truncate, return_count)\n   1092 # time to eval batch\n   1093 if t_batch + n_tokens > n_batch:\n-> 1094     decode_batch(s_batch)\n   1095     s_batch = []\n   1096     t_batch = 0\n\nFile [/opt/anaconda3/envs/rag2/lib/python3.13/site-packages/llama_cpp/llama.py:1045](http://localhost:8888/opt/anaconda3/envs/rag2/lib/python3.13/site-packages/llama_cpp/llama.py#line=1044), in Llama.embed.<locals>.decode_batch(seq_sizes)\n   1043 def decode_batch(seq_sizes: List[int]):\n   1044     llama_cpp.llama_kv_self_clear(self._ctx.ctx)\n-> 1045     self._ctx.decode(self._batch)\n   1046     self._batch.reset()\n   1048     # store embeddings\n\nFile [/opt/anaconda3/envs/rag2/lib/python3.13/site-packages/llama_cpp/_internals.py:327](http://localhost:8888/opt/anaconda3/envs/rag2/lib/python3.13/site-packages/llama_cpp/_internals.py#line=326), in LlamaContext.decode(self, batch)\n    322 return_code = llama_cpp.llama_decode(\n    323     self.ctx,\n    324     batch.batch,\n    325 )\n    326 if return_code != 0:\n--> 327     raise RuntimeError(f\"llama_decode returned {return_code}\")\n\nRuntimeError: llama_decode returned -1\n```\n\n### Description\n\nI am trying to use pgvector as vector store and embed the example data from [1] into a local gpt-oss:20b model. However this fails with a runtime error. I expect the embedding to work without an error.\n\nCalculating the embeddings seems to work.\n```\nvector_1 = embeddings.embed_query(all_splits[0].page_content)\nllama_perf_context_print:        load time =   63623.89 ms\nllama_perf_context_print: prompt eval time =   62139.37 ms [/](http://localhost:8888/)   208 tokens (  298.75 ms per token,     3.35 tokens per second)\nllama_perf_context_print:        eval time =       0.00 ms [/](http://localhost:8888/)     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_perf_context_print:       total time =   68931.87 ms [/](http://localhost:8888/)   209 tokens\nllama_perf_context_print:    graphs reused =          0\nlen(vector_1)\n2880\n```\n\n[1] https://python.langchain.com/docs/tutorials/rag/\n\n### System Info\n\n```\nfrom langchain_core import sys_info\nsys_info.print_sys_info()\nSystem Information\n------------------\n> OS:  Linux\n> OS Version:  #1 SMP PREEMPT_DYNAMIC Mon, 22 Sep 2025 22:08:35 +0000\n> Python Version:  3.13.7 | packaged by Anaconda, Inc. | (main, Sep  9 2025, 19:59:03) [GCC 11.2.0]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.76\n> langchain: 0.3.27\n> langchain_community: 0.3.29\n> langsmith: 0.4.30\n> langchain_ollama: 0.3.8\n> langchain_postgres: 0.0.15\n> langchain_text_splitters: 0.3.11\n\nOptional packages not installed\n-------------------------------\n> langserve\n\nOther Dependencies\n------------------\n> aiohttp<4.0.0,>=3.8.3: Installed. No version info available.\n> async-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\n> asyncpg>=0.30.0: Installed. No version info available.\n> dataclasses-json<0.7,>=0.6.7: Installed. No version info available.\n> httpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\n> httpx<1,>=0.23.0: Installed. No version info available.\n> jsonpatch<2.0,>=1.33: Installed. No version info available.\n> langchain-anthropic;: Installed. No version info available.\n> langchain-aws;: Installed. No version info available.\n> langchain-azure-ai;: Installed. No version info available.\n> langchain-cohere;: Installed. No version info available.\n> langchain-community;: Installed. No version info available.\n> langchain-core<0.4.0,>=0.2.13: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.72: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.76: Installed. No version info available.\n> langchain-core<2.0.0,>=0.3.75: Installed. No version info available.\n> langchain-deepseek;: Installed. No version info available.\n> langchain-fireworks;: Installed. No version info available.\n> langchain-google-genai;: Installed. No version info available.\n> langchain-google-vertexai;: Installed. No version info available.\n> langchain-groq;: Installed. No version info available.\n> langchain-huggingface;: Installed. No version info available.\n> langchain-mistralai;: Installed. No version info available.\n> langchain-ollama;: Installed. No version info available.\n> langchain-openai;: Installed. No version info available.\n> langchain-perplexity;: Installed. No version info available.\n> langchain-text-splitters<1.0.0,>=0.3.9: Installed. No version info available.\n> langchain-together;: Installed. No version info available.\n> langchain-xai;: Installed. No version info available.\n> langchain<2.0.0,>=0.3.27: Installed. No version info available.\n> langsmith-pyo3>=0.1.0rc2;: Installed. No version info available.\n> langsmith>=0.1.125: Installed. No version info available.\n> langsmith>=0.1.17: Installed. No version info available.\n> langsmith>=0.3.45: Installed. No version info available.\n> numpy<3,>=1.21: Installed. No version info available.\n> numpy>=1.26.2;: Installed. No version info available.\n> numpy>=2.1.0;: Installed. No version info available.\n> ollama<1.0.0,>=0.5.3: Installed. No version info available.\n> openai-agents>=0.0.3;: Installed. No version info available.\n> opentelemetry-api>=1.30.0;: Installed. No version info available.\n> opentelemetry-exporter-otlp-proto-http>=1.30.0;: Installed. No version info available.\n> opentelemetry-sdk>=1.30.0;: Installed. No version info available.\n> orjson>=3.9.14;: Installed. No version info available.\n> packaging>=23.2: Installed. No version info available.\n> pgvector<0.4,>=0.2.5: Installed. No version info available.\n> psycopg-pool<4,>=3.2.1: Installed. No version info available.\n> psycopg<4,>=3: Installed. No version info available.\n> pydantic-settings<3.0.0,>=2.10.1: Installed. No version info available.\n> pydantic<3,>=1: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4: Installed. No version info available.\n> pydantic>=2.7.4: Installed. No version info available.\n> pytest>=7.0.0;: Installed. No version info available.\n> PyYAML>=5.3: Installed. No version info available.\n> requests-toolbelt>=1.0.0: Installed. No version info available.\n> requests<3,>=2: Installed. No version info available.\n> requests<3,>=2.32.5: Installed. No version info available.\n> requests>=2.0.0: Installed. No version info available.\n> rich>=13.9.4;: Installed. No version info available.\n> SQLAlchemy<3,>=1.4: Installed. No version info available.\n> sqlalchemy<3,>=2: Installed. No version info available.\n> tenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\n> tenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\n> typing-extensions>=4.7: Installed. No version info available.\n> vcrpy>=7.0.0;: Installed. No version info available.\n> zstandard>=0.23.0: Installed. No version info available.\n\n```", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33083/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33083/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33082", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33082/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33082/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33082/events", "html_url": "https://github.com/langchain-ai/langchain/issues/33082", "id": 3448245312, "node_id": "I_kwDOIPDwls7NiAxA", "number": 33082, "title": "create_agent support fallback", "user": {"login": "tornadotang", "id": 45265934, "node_id": "MDQ6VXNlcjQ1MjY1OTM0", "avatar_url": "https://avatars.githubusercontent.com/u/45265934?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tornadotang", "html_url": "https://github.com/tornadotang", "followers_url": "https://api.github.com/users/tornadotang/followers", "following_url": "https://api.github.com/users/tornadotang/following{/other_user}", "gists_url": "https://api.github.com/users/tornadotang/gists{/gist_id}", "starred_url": "https://api.github.com/users/tornadotang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tornadotang/subscriptions", "organizations_url": "https://api.github.com/users/tornadotang/orgs", "repos_url": "https://api.github.com/users/tornadotang/repos", "events_url": "https://api.github.com/users/tornadotang/events{/privacy}", "received_events_url": "https://api.github.com/users/tornadotang/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 9260554836, "node_id": "LA_kwDOIPDwls8AAAACJ_jaVA", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/feature%20request", "name": "feature request", "color": "058a42", "default": false, "description": "request for an enhancement / additional functionality"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2025-09-24T08:06:38Z", "updated_at": "2025-09-25T12:45:09Z", "closed_at": null, "author_association": "NONE", "type": {"id": 18879553, "node_id": "IT_kwDOB43M6c4BIBRB", "name": "Feature", "description": "A request, idea, or new functionality", "color": "blue", "created_at": "2024-02-16T01:43:34Z", "updated_at": "2024-10-08T21:10:19Z", "is_enabled": true}, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Checked other resources\n\n- [x] This is a feature request, not a bug report or usage question.\n- [x] I added a clear and descriptive title that summarizes the feature request.\n- [x] I used the GitHub search to find a similar feature request and didn't find it.\n- [x] I checked the LangChain documentation and API reference to see if this feature already exists.\n- [x] This is not related to the langchain-community package.\n\n### Feature Description\n\nTo better support structured output, create_agent no longer supports pre-bound models with tools or configuration:\n\nBUT w/o pre-bound models, we cannot pass model.with_fallback to create_agent, that means we lost the capability of failover when openai outage\n\n### Use Case\n\nI want to build agent support failover when openai outage, w/ create_react_agent it can support pre-bound model.with_fallback().\n\nbut langchain.agents create_agent disabled it.\n\n### Proposed Solution\n\nI request create_agent support pre-bound model.with_fallback()\n\n### Alternatives Considered\n\n_No response_\n\n### Additional Context\n\n_No response_", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33082/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33082/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33081", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33081/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33081/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33081/events", "html_url": "https://github.com/langchain-ai/langchain/issues/33081", "id": 3448220675, "node_id": "I_kwDOIPDwls7Nh6wD", "number": 33081, "title": "FalkorDB uses Euclidean distance instead of Cosine despite default and explicit settings", "user": {"login": "Akanksha-turinton", "id": 215996941, "node_id": "U_kgDODN_aDQ", "avatar_url": "https://avatars.githubusercontent.com/u/215996941?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Akanksha-turinton", "html_url": "https://github.com/Akanksha-turinton", "followers_url": "https://api.github.com/users/Akanksha-turinton/followers", "following_url": "https://api.github.com/users/Akanksha-turinton/following{/other_user}", "gists_url": "https://api.github.com/users/Akanksha-turinton/gists{/gist_id}", "starred_url": "https://api.github.com/users/Akanksha-turinton/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Akanksha-turinton/subscriptions", "organizations_url": "https://api.github.com/users/Akanksha-turinton/orgs", "repos_url": "https://api.github.com/users/Akanksha-turinton/repos", "events_url": "https://api.github.com/users/Akanksha-turinton/events{/privacy}", "received_events_url": "https://api.github.com/users/Akanksha-turinton/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 5680700839, "node_id": "LA_kwDOIPDwls8AAAABUpidpw", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/bug", "name": "bug", "color": "b60205", "default": true, "description": "Related to a bug, vulnerability, unexpected error with an existing feature"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2025-09-24T07:59:34Z", "updated_at": "2025-09-25T09:56:48Z", "closed_at": null, "author_association": "NONE", "type": {"id": 18879550, "node_id": "IT_kwDOB43M6c4BIBQ-", "name": "Bug", "description": "An unexpected problem or behavior", "color": "red", "created_at": "2024-02-16T01:43:33Z", "updated_at": "2024-07-26T15:00:33Z", "is_enabled": true}, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Checked other resources\n\n- [x] This is a bug, not a usage question.\n- [x] I added a clear and descriptive title that summarizes this issue.\n- [x] I used the GitHub search to find a similar question and didn't find it.\n- [x] I am sure that this is a bug in LangChain rather than my code.\n- [x] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n- [ ] This is not related to the langchain-community package.\n- [x] I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n- [x] I posted a self-contained, minimal, reproducible example. A maintainer can copy it and run it AS IS.\n\n### Example Code\n\nfrom langchain_community.vectorstores.falkordb_vector import FalkorDBVector\nfrom langchain_community.embeddings import SentenceTransformerEmbeddings\nfrom langchain.schema import Document  \n\n# Initialize free embeddings\nembeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n\n# Example documents (must be Document objects)\ndocs = [\n    Document(page_content=\"Roti Batch\", metadata={\"id\": 1}),\n    Document(page_content=\"Roti\", metadata={\"id\": 2}),\n    Document(page_content=\"Recipe\", metadata={\"id\": 3})\n]\n\n# Initialize FalkorDB vector store\nvector_store = FalkorDBVector.from_documents(\n    documents=docs,\n    embedding=embeddings,\n    host=\"localhost\",\n    port=6379\n)\n\n# Perform similarity search with scores\nquery = \"Roti\"\nresults = vector_store.similarity_search_with_score(query=query, k=3)\n\nfor doc, score in results:\n    print(f\"Doc: {doc.page_content}, Score: {score}\")\n\n\n# Results \nDoc: Roti, Score: 1.0\nDoc: Roti Batch, Score: 0.650620520114899\nDoc: Recipe, Score: 0.390799462795258\n\n### Error Message and Stack Trace (if applicable)\n\n_No response_\n\n### Description\n\nI am experiencing an issue with Falkor where it appears to use **Euclidean distance** instead of **Cosine distance**, even though:\n\n1. The default distance strategy is explicitly set to Cosine:\n\n```python\nDEFAULT_DISTANCE_STRATEGY = DistanceStrategy.COSINE\n```\n\n2. The distance strategy is explicitly passed as Cosine in parameters.\n\nHowever, when running vector queries, the distance computation still seems to behave like Euclidean. For example, in the `_get_search_index_query` function:\n\n```python\nDISTANCE_MAPPING = {\n    DistanceStrategy.EUCLIDEAN_DISTANCE: \"euclidean\",\n    DistanceStrategy.COSINE: \"cosine\",\n}\n\ndef _get_search_index_query(search_type: SearchType, index_type: IndexType = DEFAULT_INDEX_TYPE) -> str:\n    if index_type == IndexType.NODE:\n        if search_type == SearchType.VECTOR:\n            return (\n                \"CALL db.idx.vector.queryNodes($entity_label, \"\n                \"$entity_property, $k, vecf32($embedding)) \"\n                \"YIELD node, score \"\n                \"WITH node, (2 - score)/2 as score\"\n            )\n```\n\nIt seems like the distance type mapping (`DISTANCE_MAPPING`) is not being respected, and the query is returning results consistent with **Euclidean distance** rather than **Cosine similarity**.\n\n**Expected Behavior:**\n\n* When `DistanceStrategy.COSINE` is set as default or explicitly passed, the query should compute **Cosine similarity**.\n\n**Actual Behavior:**\n\n* The query appears to use **Euclidean distance** regardless of the strategy setting.\n\n**Environment:**\n\n* Falkor version: latest\n* Python version: 3.13\n\n**Additional Context:**\n\n* This behavior affects similarity searches and scoring in ways that are inconsistent with the intended Cosine distance strategy.\n\n\n### System Info\n\n(.venv) akankshapalve@Akankshas-MacBook-Air insights-falkordb-service % python -m langchain_core.sys_info\n\nSystem Information\n------------------\n> OS:  Darwin\n> OS Version:  Darwin Kernel Version 24.6.0: Mon Jul 14 11:30:40 PDT 2025; root:xnu-11417.140.69~1/RELEASE_ARM64_T8132\n> Python Version:  3.13.5 (main, Jun 12 2025, 21:50:42) [Clang 16.0.0 (clang-1600.0.26.6)]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.76\n> langchain: 0.3.27\n> langchain_community: 0.3.29\n> langsmith: 0.4.29\n> langchain_falkordb: 0.1.2\n> langchain_openai: 0.3.33\n> langchain_text_splitters: 0.3.11\n\nOptional packages not installed\n-------------------------------\n> langserve\n\nOther Dependencies\n------------------\n> aiohttp<4.0.0,>=3.8.3: Installed. No version info available.\n> async-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\n> dataclasses-json<0.7,>=0.6.7: Installed. No version info available.\n> falkordb: 1.2.0\n> httpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\n> httpx<1,>=0.23.0: Installed. No version info available.\n> jsonpatch<2.0,>=1.33: Installed. No version info available.\n> langchain-anthropic;: Installed. No version info available.\n> langchain-aws;: Installed. No version info available.\n> langchain-azure-ai;: Installed. No version info available.\n> langchain-cohere;: Installed. No version info available.\n> langchain-community;: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.72: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.76: Installed. No version info available.\n> langchain-core<2.0.0,>=0.3.75: Installed. No version info available.\n> langchain-deepseek;: Installed. No version info available.\n> langchain-fireworks;: Installed. No version info available.\n> langchain-google-genai;: Installed. No version info available.\n> langchain-google-vertexai;: Installed. No version info available.\n> langchain-groq;: Installed. No version info available.\n> langchain-huggingface;: Installed. No version info available.\n> langchain-mistralai;: Installed. No version info available.\n> langchain-ollama;: Installed. No version info available.\n> langchain-openai;: Installed. No version info available.\n> langchain-perplexity;: Installed. No version info available.\n> langchain-text-splitters<1.0.0,>=0.3.9: Installed. No version info available.\n> langchain-together;: Installed. No version info available.\n> langchain-xai;: Installed. No version info available.\n> langchain<2.0.0,>=0.3.27: Installed. No version info available.\n> langsmith-pyo3>=0.1.0rc2;: Installed. No version info available.\n> langsmith>=0.1.125: Installed. No version info available.\n> langsmith>=0.1.17: Installed. No version info available.\n> langsmith>=0.3.45: Installed. No version info available.\n> numpy>=1.26.2;: Installed. No version info available.\n> numpy>=2.1.0;: Installed. No version info available.\n> openai-agents>=0.0.3;: Installed. No version info available.\n> openai<2.0.0,>=1.104.2: Installed. No version info available.\n> opentelemetry-api>=1.30.0;: Installed. No version info available.\n> opentelemetry-exporter-otlp-proto-http>=1.30.0;: Installed. No version info available.\n> opentelemetry-sdk>=1.30.0;: Installed. No version info available.\n> orjson>=3.9.14;: Installed. No version info available.\n> packaging>=23.2: Installed. No version info available.\n> pydantic-settings<3.0.0,>=2.10.1: Installed. No version info available.\n> pydantic<3,>=1: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4: Installed. No version info available.\n> pydantic>=2.7.4: Installed. No version info available.\n> pytest>=7.0.0;: Installed. No version info available.\n> PyYAML>=5.3: Installed. No version info available.\n> requests-toolbelt>=1.0.0: Installed. No version info available.\n> requests<3,>=2: Installed. No version info available.\n> requests<3,>=2.32.5: Installed. No version info available.\n> requests>=2.0.0: Installed. No version info available.\n> rich>=13.9.4;: Installed. No version info available.\n> SQLAlchemy<3,>=1.4: Installed. No version info available.\n> tenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\n> tenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\n> tiktoken<1,>=0.7: Installed. No version info available.\n> typing-extensions>=4.7: Installed. No version info available.\n> vcrpy>=7.0.0;: Installed. No version info available.\n> zstandard>=0.23.0: Installed. No version info available.\n", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33081/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33081/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33080", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33080/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33080/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33080/events", "html_url": "https://github.com/langchain-ai/langchain/issues/33080", "id": 3447938608, "node_id": "I_kwDOIPDwls7Ng14w", "number": 33080, "title": "```RunnableConfig``` not passed into tools when using ```AgentExecutor```", "user": {"login": "Sibinraj06", "id": 127471275, "node_id": "U_kgDOB5kOqw", "avatar_url": "https://avatars.githubusercontent.com/u/127471275?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Sibinraj06", "html_url": "https://github.com/Sibinraj06", "followers_url": "https://api.github.com/users/Sibinraj06/followers", "following_url": "https://api.github.com/users/Sibinraj06/following{/other_user}", "gists_url": "https://api.github.com/users/Sibinraj06/gists{/gist_id}", "starred_url": "https://api.github.com/users/Sibinraj06/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Sibinraj06/subscriptions", "organizations_url": "https://api.github.com/users/Sibinraj06/orgs", "repos_url": "https://api.github.com/users/Sibinraj06/repos", "events_url": "https://api.github.com/users/Sibinraj06/events{/privacy}", "received_events_url": "https://api.github.com/users/Sibinraj06/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 5680700839, "node_id": "LA_kwDOIPDwls8AAAABUpidpw", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/bug", "name": "bug", "color": "b60205", "default": true, "description": "Related to a bug, vulnerability, unexpected error with an existing feature"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2025-09-24T06:46:57Z", "updated_at": "2025-09-24T06:52:53Z", "closed_at": null, "author_association": "NONE", "type": {"id": 18879550, "node_id": "IT_kwDOB43M6c4BIBQ-", "name": "Bug", "description": "An unexpected problem or behavior", "color": "red", "created_at": "2024-02-16T01:43:33Z", "updated_at": "2024-07-26T15:00:33Z", "is_enabled": true}, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "\n\n### Example Code\n\nWhen running tools through an `AgentExecutor`, the `RunnableConfig` is not being propagated to the tool \u2014 both when using `.with_config()` and when passing `config` directly to `.invoke()` / `.ainvoke()`.\n\nInside the tool, `config` is always empty or `None`.\n\n```\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain.agents import AgentExecutor\nfrom langchain_core.tools import tool\n\n# Example tool\n@tool\ndef list_projects_detailed(project: str, config: RunnableConfig):\n    print(\"Config inside tool:\", config)  # always None\n    return {\"projects\": []}\n\n# Agent execution\nasync def run_agent(user_query: str, user_id: str):\n    config = {\"user_id\": user_id}\n\n    agent_executor = AgentExecutor(\n        agent=agent,\n        tools=[list_projects_detailed],\n        verbose=True,\n        handle_parsing_errors=True,\n    )\n\n    # Case 1: `with_config`\n    agent_executor = agent_executor.with_config(config=config)\n    response = await agent_executor.ainvoke({\"input\": user_query})\n    print(\"response (with_config):\", response)\n\n    # Case 2: passing config directly\n    response2 = await agent_executor.ainvoke({\"input\": user_query}, config=config)\n    print(\"response (ainvoke with config):\", response2)\n```\n\n\n### Error Message and Stack Trace (if applicable)\n\n_No response_\n\n### Description\n\n* Tools should receive the `RunnableConfig` with `user_id` inside their `config` parameter.\n\n\n* In both cases, `config` is `None` inside the tool.\n\n\n\n### System Info\n\n\n#### Environment\n\n* **LangChain version:** 0.3.27\n* **langchain-core version:** 0.3.75\n* **Python version:** 3.12\n* **Backend:** OpenAI\n* **OS:** (Ubuntu 22.04)\n\nI also checked [[discussion #28694](https://github.com/langchain-ai/langchain/discussions/28694)](https://github.com/langchain-ai/langchain/discussions/28694), which seems related. It looks like `RunnableConfig` is not being passed into tools from `AgentExecutor`.", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33080/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33080/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33056", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33056/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33056/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33056/events", "html_url": "https://github.com/langchain-ai/langchain/issues/33056", "id": 3446051859, "node_id": "I_kwDOIPDwls7NZpQT", "number": 33056, "title": "Richer Interface for Multimodal Data and BaseChat Interaction", "user": {"login": "mprudra", "id": 161220745, "node_id": "U_kgDOCZwIiQ", "avatar_url": "https://avatars.githubusercontent.com/u/161220745?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mprudra", "html_url": "https://github.com/mprudra", "followers_url": "https://api.github.com/users/mprudra/followers", "following_url": "https://api.github.com/users/mprudra/following{/other_user}", "gists_url": "https://api.github.com/users/mprudra/gists{/gist_id}", "starred_url": "https://api.github.com/users/mprudra/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mprudra/subscriptions", "organizations_url": "https://api.github.com/users/mprudra/orgs", "repos_url": "https://api.github.com/users/mprudra/repos", "events_url": "https://api.github.com/users/mprudra/events{/privacy}", "received_events_url": "https://api.github.com/users/mprudra/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 9260554836, "node_id": "LA_kwDOIPDwls8AAAACJ_jaVA", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/feature%20request", "name": "feature request", "color": "058a42", "default": false, "description": "request for an enhancement / additional functionality"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2025-09-23T17:05:46Z", "updated_at": "2025-09-23T17:05:46Z", "closed_at": null, "author_association": "NONE", "type": {"id": 18879553, "node_id": "IT_kwDOB43M6c4BIBRB", "name": "Feature", "description": "A request, idea, or new functionality", "color": "blue", "created_at": "2024-02-16T01:43:34Z", "updated_at": "2024-10-08T21:10:19Z", "is_enabled": true}, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Checked other resources\n\n- [x] This is a feature request, not a bug report or usage question.\n- [x] I added a clear and descriptive title that summarizes the feature request.\n- [x] I used the GitHub search to find a similar feature request and didn't find it.\n- [x] I checked the LangChain documentation and API reference to see if this feature already exists.\n- [x] This is not related to the langchain-community package.\n\n### Feature Description\n\nBased on my experience with Langchain, I find the `langchain.Document` interface to be too generic\u2014perhaps intentionally so, to support a wide range of use cases. However, as LLMs continue to evolve, especially with growing multimodal capabilities, the need for a richer, more expressive interface for data parsing, storage, retrieval, LLM interaction, and citation becomes increasingly critical.\nCurrently, `Langchain.Document` is not expressive enough to represent complex or structured information beyond plain text. I also explored `Langchain.BaseMedia`, which partially addresses multimodality (e.g., image/audio), but it still lacks the granularity required for representing tables, formulas, and other structured formats. I\u2019m expanding the concept of \"modality\" here to include more sophisticated constructs for representing information.\n\n### Use Case\n\n### Existing Gaps and Observations\nThere are interfaces in other systems that attempt to support richer representations, but many are tightly coupled with specific document parsers. For example:\n* **[Docling](https://docling-project.github.io/docling/concepts/docling_document/#example-document-structures)** provides its own schema for structured outputs from parsed documents (text, tables, images, etc.).\n* Other parser APIs like **LandingAI** or **LlamaParse** also return structured outputs (e.g., in JSON, HTML, Markdown).\n\nThis reveals a broader opportunity: to define a **common interface** in Langchain that can represent different modalities - `TextElement`, `ImageElement`, `TableElement`, `AudioElement`, etc. - as part of a unified `MultimodalDocument`, which could be implemented as something like `List[Element]`.\nSuch a structure could assume vertically stacked elements by default, but it could also support layout-aware metadata (e.g., page number, bounding boxes) if the parser is capable of providing it.\nCurrently, most Langchain chains/runnables (e.g., for Q\\&A) are text-based. They expect retrievers to return a `List[Document]` where the content is textual. These documents are typically concatenated into a prompt string passed to the LLM.\nHowever, in the multimodal setting, this approach doesn\u2019t scale well. You need to interact with the LLM using `HumanMessage` objects that might represent different modalities. The current workaround - dumping everything into `Document.metadata` and reconstructing `HumanMessage` types (text, image, etc.) from there - is hacky, hard to reuse, and error-prone.\n\nThere\u2019s a clear **design gap** here:\n* Langchain expects retrievers to return `List[Document]`\n* LLMs (via `BaseChat`) expect a `List[HumanMessage]`\n* But there\u2019s no clear or clean path for transforming rich, structured documents into appropriate multimodal messages\n* This missing piece is a kind of `(Retriever | Lambda | LLM)` pipeline where the lambda function is not straightforward to implement due to the limitations of `Document`\n\n### Proposed Solution\n\n### Suggested Improvements\n\n1. **Define a richer interface** for representing structured, multimodal information. This could be implemented via Pydantic models:\n   * `TextElement`, `TableElement`, `ImageElement`, `FormulaElement`, etc.\n   * Tables can have nested granularity: headers, rows, and cell-level metadata\n   * This would allow for more granular RAG operations and citations downstream\n2. **Multimodal Loaders and Retrievers** concepts:\n   * Parser authors could fill in detailed, structured elements\n   * These could later support fine-grained citations in LLM response\n   * Enable clean identification and handling of different modalities in retrieval\n   * This would also help in routing different types of `Element` to appropriate LLM input handlers (text, image, table, etc.)\n\n[GOOD TO HAVE]\n1. **Support for over-the-wire serialization**, e.g., via Pydantic-compatible formats (JSON, etc.)\n2. **Support for serialization/deserialization** to/from file systems or object storage:\n   * Should support selective loading, since RAG workflows typically need only a subset of the full document\n\n### Alternatives Considered\n\n_No response_\n\n### Additional Context\n\n### Question\n- Is this current behavior and limitation **intentional**?\n- Do you agree with the overall proposal?\n- Would this be something worth considering and taking up in Langchain?", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33056/reactions", "total_count": 2, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 1, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33056/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33051", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33051/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33051/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33051/events", "html_url": "https://github.com/langchain-ai/langchain/issues/33051", "id": 3443007799, "node_id": "I_kwDOIPDwls7NOCE3", "number": 33051, "title": "ChatOpenAI can now accept a callable for api_key, not just a string", "user": {"login": "pamelafox", "id": 297042, "node_id": "MDQ6VXNlcjI5NzA0Mg==", "avatar_url": "https://avatars.githubusercontent.com/u/297042?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pamelafox", "html_url": "https://github.com/pamelafox", "followers_url": "https://api.github.com/users/pamelafox/followers", "following_url": "https://api.github.com/users/pamelafox/following{/other_user}", "gists_url": "https://api.github.com/users/pamelafox/gists{/gist_id}", "starred_url": "https://api.github.com/users/pamelafox/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pamelafox/subscriptions", "organizations_url": "https://api.github.com/users/pamelafox/orgs", "repos_url": "https://api.github.com/users/pamelafox/repos", "events_url": "https://api.github.com/users/pamelafox/events{/privacy}", "received_events_url": "https://api.github.com/users/pamelafox/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 5680700839, "node_id": "LA_kwDOIPDwls8AAAABUpidpw", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/bug", "name": "bug", "color": "b60205", "default": true, "description": "Related to a bug, vulnerability, unexpected error with an existing feature"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2025-09-22T22:47:11Z", "updated_at": "2025-09-24T02:11:22Z", "closed_at": null, "author_association": "CONTRIBUTOR", "type": {"id": 18879550, "node_id": "IT_kwDOB43M6c4BIBQ-", "name": "Bug", "description": "An unexpected problem or behavior", "color": "red", "created_at": "2024-02-16T01:43:33Z", "updated_at": "2024-07-26T15:00:33Z", "is_enabled": true}, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Checked other resources\n\n- [x] This is a bug, not a usage question.\n- [x] I added a clear and descriptive title that summarizes this issue.\n- [x] I used the GitHub search to find a similar question and didn't find it.\n- [x] I am sure that this is a bug in LangChain rather than my code.\n- [x] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n- [x] This is not related to the langchain-community package.\n- [x] I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n- [x] I posted a self-contained, minimal, reproducible example. A maintainer can copy it and run it AS IS.\n\n### Example Code\n\nRecently, the openai package changed so that api_key can accept either a string OR a callable. The callable is used to generate a token, for use with APIs like Azure OpenAI that generate bearer tokens. \n\nHere's the start of __init__ from the openai package:\n\n```\n    def __init__(\n        self,\n        *,\n        api_key: str | None | Callable[[], str] = None,\n```\n\n\nHowever, when I try to pass a callable into the langchain-openai wrapper, I get a type validation error, since that wrapper still says that api_key must be a string.\nCould it be extended to allow for the Callable as well?\n\nHere's full code to replicate, with an Azure account:\n\n```\nimport os\n\nimport azure.identity\nfrom dotenv import load_dotenv\nfrom langchain.agents import create_agent\nfrom langchain_openai import ChatOpenAI\nfrom rich import print\n\nload_dotenv(override=True)\n\ntoken_provider = azure.identity.get_bearer_token_provider(\n    azure.identity.DefaultAzureCredential(),\n    \"https://cognitiveservices.azure.com/.default\",\n)\nmodel = ChatOpenAI(\n    model=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT\"],\n    base_url=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    api_key=token_provider,\n)\n\nagent = create_agent(model=model, prompt=\"You are a helpful assistant that makes lots of cat references and uses emojis.\", tools=[])\nresponse = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"write a haiku about a hungry cat that wants tuna\"}]})\nlatest_message = response[\"messages\"][-1]\nprint(latest_message.content)\n```\n\n\n### Error Message and Stack Trace (if applicable)\n\n(.venv) python-openai-demos % /Users/pamelafox/python-openai-demos/.venv/bin/python /Users/pamelafox/python-openai-demos/chat_langchain.py\nTraceback (most recent call last):\n  File \"/Users/pamelafox/python-openai-demos/chat_langchain.py\", line 17, in <module>\n    model = ChatOpenAI(\n            ^^^^^^^^^^^\n  File \"/Users/pamelafox/python-openai-demos/.venv/lib/python3.11/site-packages/langchain_core/load/serializable.py\", line 115, in __init__\n    super().__init__(*args, **kwargs)\n  File \"/Users/pamelafox/python-openai-demos/.venv/lib/python3.11/site-packages/pydantic/main.py\", line 253, in __init__\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\npydantic_core._pydantic_core.ValidationError: 1 validation error for ChatOpenAI\napi_key\n  Input should be a valid string [type=string_type, input_value=<function get_bearer_toke....wrapper at 0x17bd558a0>, input_type=function]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\n\n### Description\n\nI need my code to pass type validation\n\n### System Info\n\n\nSystem Information\n------------------\n> OS:  Darwin\n> OS Version:  Darwin Kernel Version 23.6.0: Wed May 14 13:52:22 PDT 2025; root:xnu-10063.141.1.705.2~2/RELEASE_ARM64_T6000\n> Python Version:  3.11.13 (main, Jun  3 2025, 18:38:25) [Clang 16.0.0 (clang-1600.0.26.6)]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.76\n> langchain: 1.0.0a6\n> langchain_community: 0.0.33\n> langsmith: 0.4.30\n> langchain_openai: 0.3.33\n> langchain_text_splitters: 0.3.11\n> langgraph_sdk: 0.2.9\n\nOptional packages not installed\n-------------------------------\n> langserve\n\nOther Dependencies\n------------------\n> aiohttp: 3.9.5\n> aiosqlite: Installed. No version info available.\n> aleph-alpha-client: Installed. No version info available.\n> anthropic: 0.68.0\n> arxiv: Installed. No version info available.\n> assemblyai: Installed. No version info available.\n> atlassian-python-api: Installed. No version info available.\n> azure-ai-documentintelligence: Installed. No version info available.\n> beautifulsoup4: Installed. No version info available.\n> bibtexparser: Installed. No version info available.\n> cassio: Installed. No version info available.\n> chardet: Installed. No version info available.\n> cloudpickle: Installed. No version info available.\n> cohere: 5.18.0\n> databricks-vectorsearch: Installed. No version info available.\n> dataclasses-json: 0.6.4\n> datasets: Installed. No version info available.\n> dgml-utils: Installed. No version info available.\n> elasticsearch: Installed. No version info available.\n> esprima: Installed. No version info available.\n> faiss-cpu: Installed. No version info available.\n> feedparser: Installed. No version info available.\n> fireworks-ai: Installed. No version info available.\n> friendli-client: Installed. No version info available.\n> geopandas: Installed. No version info available.\n> gitpython: Installed. No version info available.\n> google-cloud-documentai: Installed. No version info available.\n> gql: Installed. No version info available.\n> gradientai: Installed. No version info available.\n> hdbcli: Installed. No version info available.\n> hologres-vector: Installed. No version info available.\n> html2text: Installed. No version info available.\n> httpx: 0.28.1\n> httpx-sse: 0.4.0\n> httpx<1,>=0.23.0: Installed. No version info available.\n> httpx>=0.25.2: Installed. No version info available.\n> javelin-sdk: Installed. No version info available.\n> jinja2: 3.1.6\n> jq: Installed. No version info available.\n> jsonpatch<2.0,>=1.33: Installed. No version info available.\n> jsonschema: 4.25.1\n> langchain-anthropic;: Installed. No version info available.\n> langchain-aws;: Installed. No version info available.\n> langchain-azure-ai;: Installed. No version info available.\n> langchain-cohere;: Installed. No version info available.\n> langchain-community;: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.76: Installed. No version info available.\n> langchain-core<2.0.0,>=0.3.75: Installed. No version info available.\n> langchain-deepseek;: Installed. No version info available.\n> langchain-fireworks;: Installed. No version info available.\n> langchain-google-genai;: Installed. No version info available.\n> langchain-google-vertexai;: Installed. No version info available.\n> langchain-groq;: Installed. No version info available.\n> langchain-huggingface;: Installed. No version info available.\n> langchain-mistralai;: Installed. No version info available.\n> langchain-ollama;: Installed. No version info available.\n> langchain-openai;: Installed. No version info available.\n> langchain-perplexity;: Installed. No version info available.\n> langchain-text-splitters<1.0.0,>=0.3.11: Installed. No version info available.\n> langchain-together;: Installed. No version info available.\n> langchain-xai;: Installed. No version info available.\n> langgraph>=0.6.7: Installed. No version info available.\n> langsmith-pyo3>=0.1.0rc2;: Installed. No version info available.\n> langsmith>=0.3.45: Installed. No version info available.\n> lxml: Installed. No version info available.\n> markdownify: Installed. No version info available.\n> motor: Installed. No version info available.\n> msal: 1.28.0\n> mwparserfromhell: Installed. No version info available.\n> mwxml: Installed. No version info available.\n> newspaper3k: Installed. No version info available.\n> numexpr: Installed. No version info available.\n> numpy: 1.26.4\n> nvidia-riva-client: Installed. No version info available.\n> oci: Installed. No version info available.\n> openai: 1.108.1\n> openai-agents>=0.0.3;: Installed. No version info available.\n> openai<2.0.0,>=1.104.2: Installed. No version info available.\n> openapi-pydantic: Installed. No version info available.\n> opentelemetry-api>=1.30.0;: Installed. No version info available.\n> opentelemetry-exporter-otlp-proto-http>=1.30.0;: Installed. No version info available.\n> opentelemetry-sdk>=1.30.0;: Installed. No version info available.\n> oracle-ads: Installed. No version info available.\n> orjson>=3.10.1: Installed. No version info available.\n> orjson>=3.9.14;: Installed. No version info available.\n> packaging>=23.2: Installed. No version info available.\n> pandas: 2.2.2\n> pdfminer-six: Installed. No version info available.\n> pgvector: Installed. No version info available.\n> praw: Installed. No version info available.\n> premai: Installed. No version info available.\n> psychicapi: Installed. No version info available.\n> py-trello: Installed. No version info available.\n> pydantic<3,>=1: Installed. No version info available.\n> pydantic>=2.7.4: Installed. No version info available.\n> pyjwt: 2.8.0\n> pymupdf: 1.25.5\n> pypdf: Installed. No version info available.\n> pypdfium2: Installed. No version info available.\n> pyspark: Installed. No version info available.\n> pytest>=7.0.0;: Installed. No version info available.\n> PyYAML: 6.0.2\n> PyYAML>=5.3: Installed. No version info available.\n> rank-bm25: Installed. No version info available.\n> rapidfuzz: Installed. No version info available.\n> rapidocr-onnxruntime: Installed. No version info available.\n> rdflib: Installed. No version info available.\n> requests: 2.32.3\n> requests-toolbelt: 1.0.0\n> requests-toolbelt>=1.0.0: Installed. No version info available.\n> requests>=2.0.0: Installed. No version info available.\n> rich>=13.9.4;: Installed. No version info available.\n> rspace_client: Installed. No version info available.\n> scikit-learn: 1.6.1\n> SQLAlchemy: 2.0.29\n> sqlite-vss: Installed. No version info available.\n> streamlit: Installed. No version info available.\n> sympy: 1.14.0\n> telethon: Installed. No version info available.\n> tenacity: 8.2.3\n> tenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\n> tidb-vector: Installed. No version info available.\n> tiktoken<1,>=0.7: Installed. No version info available.\n> timescale-vector: Installed. No version info available.\n> tqdm: 4.66.2\n> tree-sitter: Installed. No version info available.\n> tree-sitter-languages: Installed. No version info available.\n> typer: Installed. No version info available.\n> typing-extensions>=4.7: Installed. No version info available.\n> upstash-redis: Installed. No version info available.\n> vcrpy>=7.0.0;: Installed. No version info available.\n> vdms: Installed. No version info available.\n> xata: Installed. No version info available.\n> xmltodict: Installed. No version info available.\n> zstandard>=0.23.0: Installed. No version info available.", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33051/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33051/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33049", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33049/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33049/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33049/events", "html_url": "https://github.com/langchain-ai/langchain/issues/33049", "id": 3442255708, "node_id": "I_kwDOIPDwls7NLKdc", "number": 33049, "title": "Feature: Support GoogleGenerativeAIEmbeddings using application default credentials", "user": {"login": "bkarduck", "id": 98230692, "node_id": "U_kgDOBdrhpA", "avatar_url": "https://avatars.githubusercontent.com/u/98230692?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bkarduck", "html_url": "https://github.com/bkarduck", "followers_url": "https://api.github.com/users/bkarduck/followers", "following_url": "https://api.github.com/users/bkarduck/following{/other_user}", "gists_url": "https://api.github.com/users/bkarduck/gists{/gist_id}", "starred_url": "https://api.github.com/users/bkarduck/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bkarduck/subscriptions", "organizations_url": "https://api.github.com/users/bkarduck/orgs", "repos_url": "https://api.github.com/users/bkarduck/repos", "events_url": "https://api.github.com/users/bkarduck/events{/privacy}", "received_events_url": "https://api.github.com/users/bkarduck/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 9260554836, "node_id": "LA_kwDOIPDwls8AAAACJ_jaVA", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/feature%20request", "name": "feature request", "color": "058a42", "default": false, "description": "request for an enhancement / additional functionality"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2025-09-22T18:45:07Z", "updated_at": "2025-09-25T12:52:11Z", "closed_at": null, "author_association": "NONE", "type": {"id": 18879553, "node_id": "IT_kwDOB43M6c4BIBRB", "name": "Feature", "description": "A request, idea, or new functionality", "color": "blue", "created_at": "2024-02-16T01:43:34Z", "updated_at": "2024-10-08T21:10:19Z", "is_enabled": true}, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Checked other resources\n\n- [x] This is a feature request, not a bug report or usage question.\n- [x] I added a clear and descriptive title that summarizes the feature request.\n- [x] I used the GitHub search to find a similar feature request and didn't find it.\n- [x] I checked the LangChain documentation and API reference to see if this feature already exists.\n- [x] This is not related to the langchain-community package.\n\n### Feature Description\n\nI would like langchain to support the use of the Application Default Credentials json as the access pattern for google models when using GoogleGenerativeAI features. It currently only accepts an API key.\n\n### Use Case\n\nI have an application through my work that uses an Application Default Credentials json file to access the google endpoint for embeddings. As vertex gets officially phased out, we're moving to using all of the google genai capabilities and while we have been using langchain's VertexAIEmbeddings wrapper to handle embedding and storing docs in elasticsearch, we want to use GoogleGenerativeAIEmbeddings instead. We found today that it's not supported unless we pass in an API key, which doesn't align with the access pattern we've been using. \n\n### Proposed Solution\n\nI think this could be fixed by allowing the user to pass in the client they are using that has the appropriate google access with the ADC. \n\n### Alternatives Considered\n\nThe alternative would be effectively copy/pasting the class and using the same features with the correct access pattern.\n\n### Additional Context\n\n_No response_", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33049/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33049/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33046", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33046/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33046/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33046/events", "html_url": "https://github.com/langchain-ai/langchain/issues/33046", "id": 3438946169, "node_id": "I_kwDOIPDwls7M-id5", "number": 33046, "title": "AddableUpdatesDict does not support right-side addition", "user": {"login": "HERIUN", "id": 25131767, "node_id": "MDQ6VXNlcjI1MTMxNzY3", "avatar_url": "https://avatars.githubusercontent.com/u/25131767?v=4", "gravatar_id": "", "url": "https://api.github.com/users/HERIUN", "html_url": "https://github.com/HERIUN", "followers_url": "https://api.github.com/users/HERIUN/followers", "following_url": "https://api.github.com/users/HERIUN/following{/other_user}", "gists_url": "https://api.github.com/users/HERIUN/gists{/gist_id}", "starred_url": "https://api.github.com/users/HERIUN/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/HERIUN/subscriptions", "organizations_url": "https://api.github.com/users/HERIUN/orgs", "repos_url": "https://api.github.com/users/HERIUN/repos", "events_url": "https://api.github.com/users/HERIUN/events{/privacy}", "received_events_url": "https://api.github.com/users/HERIUN/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 5680700839, "node_id": "LA_kwDOIPDwls8AAAABUpidpw", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/bug", "name": "bug", "color": "b60205", "default": true, "description": "Related to a bug, vulnerability, unexpected error with an existing feature"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2025-09-22T02:28:11Z", "updated_at": "2025-09-24T12:27:24Z", "closed_at": null, "author_association": "NONE", "type": {"id": 18879550, "node_id": "IT_kwDOB43M6c4BIBQ-", "name": "Bug", "description": "An unexpected problem or behavior", "color": "red", "created_at": "2024-02-16T01:43:33Z", "updated_at": "2024-07-26T15:00:33Z", "is_enabled": true}, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Checked other resources\n\n- [x] This is a bug, not a usage question.\n- [x] I added a clear and descriptive title that summarizes this issue.\n- [x] I used the GitHub search to find a similar question and didn't find it.\n- [x] I am sure that this is a bug in LangChain rather than my code.\n- [x] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n- [x] This is not related to the langchain-community package.\n- [x] I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n- [x] I posted a self-contained, minimal, reproducible example. A maintainer can copy it and run it AS IS.\n\n### Example Code\n\n```python\nfrom dotenv import load_dotenv\nfrom mcp import ClientSession, StdioServerParameters, types\nfrom mcp.client.stdio import stdio_client\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain.agents import AgentExecutor\nfrom langchain.memory import ConversationSummaryMemory\nfrom langchain_mcp_adapters.tools import load_mcp_tools\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langgraph.prebuilt import create_react_agent\nfrom contextlib import AsyncExitStack\nimport json\nimport asyncio\nfrom module.llm.gpt_oss_20B import GPT_OSS_20B\nimport os\nfrom typing import List\n\nload_dotenv()\n\nfrom pydantic import BaseModel, Field\n\n\nclass Onpremise_RAG_Agent:\n    def __init__(self):\n        self.exit_stack = AsyncExitStack()\n        self.llm = GPT_OSS_20B(\n            model_path = os.getenv(\"llm_model_path\"),\n            host=\"localhost\",\n            port=os.getenv(\"llm_port\"),\n            gpu_devices=[int(x) for x in os.getenv(\"llm_gpu_devices\",\"\").split(\",\") if x != ''],\n        )\n        # Tools list\n        self.available_tools = []\n        # Prompts list\n        self.available_prompts = []\n        # Sessions dict maps tool/prompt names or resource URIs to MCP client sessions\n        self.messages = [] # \uc138\uc158\ubcc4 \uba54\uc2dc\uc9c0\n        self.agent = None\n        self.prompt = (\n            \"\ub2f9\uc2e0\uc740 \uc720\uc6a9\ud55c \uc5b4\uc2dc\uc2a4\ud134\ud2b8\uc785\ub2c8\ub2e4. \ub2e4\uc74c\uc758 \uaddc\uce59\uc744 \uc5c4\uaca9\ud558\uac8c \uc900\uc218\ud558\uc5ec \ub2f5\ubcc0\ud558\uc138\uc694:\\n\"\n            \"- \uac80\uc0c9\ub41c \ubb38\uc11c\uc5d0 \uc788\ub294 \uc815\ubcf4\ub9cc \uc0ac\uc6a9\ud558\uc5ec \ub2f5\ubcc0\ud558\uc138\uc694. \ubb38\uc11c\uc5d0 \uc5c6\ub294 \ub0b4\uc6a9\uc740 \ucd94\uce21\ud558\uc9c0 \ub9c8\uc138\uc694.\\n\"\n            \"- \ub2f5\ubcc0\uc758 \ub05d\uc5d0\ub294 \ubc18\ub4dc\uc2dc \uc0ac\uc6a9\ud55c \ubb38\uc11c\uc758 \ucd9c\ucc98(\ud30c\uc77c \uc774\ub984, \ud398\uc774\uc9c0 \ubc88\ud638 \ub610\ub294 \ud589 \ubc88\ud638)\ub97c \uba85\ud655\ud788 \uba85\uc2dc\ud558\uc138\uc694.\\n\"\n            \"- \ub2f5\ubcc0\uc740 \uba85\ud655\ud558\uace0 \uac04\uacb0\ud558\uac8c \uc791\uc131\ud558\uc138\uc694.\"\n            \"- \uc0dd\uc131\ub41c \ub2f5\ubcc0\uc774 \uc0ac\uc6a9\uc790\uc758 \uc9c8\ubb38\uc5d0 \uc801\uc808\ud55c \ub2f5\uc774 \ub418\ub294\uc9c0 \ub17c\ub9ac\uc801\uc73c\ub85c \uc0dd\uac01\ud558\uc138\uc694.\"\n        )\n        self.memory = ConversationSummaryMemory(\n            llm = self.llm,\n            memory_key=\"chat_history\",\n            )\n\n    async def connect_to_mcp_servers(self):\n        try:\n            with open(\"server_config.json\", \"r\") as file:\n                data = json.load(file)\n            \n            client = MultiServerMCPClient(data)\n            self.available_tools.extend(await client.get_tools())\n            \n            self.agent = create_react_agent(\n                self.llm, \n                tools = self.available_tools, \n                prompt=self.prompt,\n                )\n                #,response_format=AgentAnswerFormat)\n            self.agent_ex = AgentExecutor(\n                agent= self.agent, \n                tools= self.available_tools, \n                memory = self.memory\n                )\n        except Exception as e:\n            print(f\"Error loading server config: {e}\")\n            raise\n\n    async def process_query(self, query):\n        self.messages.append(HumanMessage(content=query))\n\n        while True:\n            # response = await self.agent.ainvoke(input={\"messages\" : self.messages})\n            response = await self.agent_ex.ainvoke(input={\"messages\" : self.messages})\n\n            # \uba54\uc2dc\uc9c0\uc758 \ub9c8\uc9c0\ub9c9 AIMessage\ub97c \ucc3e\uc544 \ucd9c\ub825\n            for message in reversed(response[\"messages\"]):\n                if isinstance(message, AIMessage):\n                    # Handle AIMessage content\n                    if isinstance(message.content, str) and message.content.strip():\n                        print(f\"Assistant: {message.content}\")\n                        self.messages.append(message)\n                        self.messages = await self._summarize_messages()\n                        return\n\n    # API \ud658\uacbd\uc5d0 \ub9de\uac8c \uc218\uc815\ub41c \uba54\uc11c\ub4dc\n    async def invoke_agent(self, query: str, chat_history: List = None) -> dict:\n        \"\"\"\ub2e8\uc77c \uc9c8\ubb38\uc5d0 \ub300\ud574 \uc5d0\uc774\uc804\ud2b8\ub97c \uc2e4\ud589\ud558\uace0 \uad6c\uc870\ud654\ub41c \ub2f5\ubcc0\uc744 \ubc18\ud658\ud569\ub2c8\ub2e4.\"\"\"\n        if chat_history is None:\n            chat_history = []\n        \n        messages = chat_history + [HumanMessage(content=query)]\n        \n        response = await self.agent.ainvoke(input={\"messages\": messages})\n\n        if response:\n            return response\n        else:\n            # \uc5d0\ub7ec \ub610\ub294 \uc608\uc0c1\uce58 \ubabb\ud55c \uc751\ub2f5 \ud615\uc2dd \ucc98\ub9ac\n            return {\"answer\": \"\uc5d0\uc774\uc804\ud2b8\uac00 \ub2f5\ubcc0\uc744 \uc0dd\uc131\ud558\uc9c0 \ubabb\ud588\uc2b5\ub2c8\ub2e4.\"}\n\n\n    async def _summarize_messages(self):\n        \"\"\"\ub300\ud654 \uae30\ub85d\uc744 \uc694\uc57d\ud569\ub2c8\ub2e4.\"\"\"\n        # \uc774\uc804 \uba54\uc2dc\uc9c0\ub4e4\uc744 HumanMessage\uc640 AIMessage\ub85c \uacb0\ud569\n\n        conversation_history = \"\"\n        for msg in self.messages:\n            if isinstance(msg, HumanMessage):\n                conversation_history += f\"user: {msg.content}\\n\"\n            elif isinstance(msg, AIMessage):\n                conversation_history += f\"assistant: {msg.content}\\n\"\n            elif isinstance(msg, SystemMessage):\n                conversation_history += f\"history : {msg.content}\\n\"\n            \n        summary_prompt = (\n            f\"\ub2e4\uc74c \ub300\ud654 \ub0b4\uc6a9\uc744 1000\uc790 \uc774\ub0b4\ub85c \uc694\uc57d\ud558\uc5ec \uc911\uc694\ud55c \uc815\ubcf4\uc640 \ub9e5\ub77d\uc744 \uc720\uc9c0\ud574 \uc8fc\uc138\uc694.\\n\\n\"\n            f\"\ub300\ud654:\\n{conversation_history}\\n\\n\"\n            f\"\uc694\uc57d:\"\n        )\n        try:\n            summary = await self.llm.ainvoke(input=[HumanMessage(content=summary_prompt)])\n            messages = [SystemMessage(content=summary.content)]\n        except Exception as e:\n            # \uc694\uc57d \uc2e4\ud328 \uc2dc \uba54\uc2dc\uc9c0 \ub9ac\uc2a4\ud2b8\ub97c \ucd08\uae30\ud654\ud558\uc5ec \ud1a0\ud070 \ucd08\uacfc \uc624\ub958 \ubc29\uc9c0\n            messages = []\n\n        return messages\n\n\n    async def chat_loop(self):\n        print(\"\\nMCP Chatbot Started!\")\n        print(\"Type your queries or 'quit' to exit.\")\n        \n        while True:\n            try:\n                query = input(\"\\nQuery: \").strip()\n                if not query:\n                    continue\n        \n                if query.lower() == 'quit':\n                    break\n                \n                await self.process_query(query)\n                    \n            except Exception as e:\n                print(f\"\\nError: {str(e)}\")\n\n    async def cleanup(self):\n        await self.exit_stack.aclose()\n    \n\nasync def main():\n    chatbot = Onpremise_RAG_Agent()\n    try:\n        await chatbot.connect_to_mcp_servers()\n        await chatbot.chat_loop()\n    finally:\n        await chatbot.cleanup()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n### Error Message and Stack Trace (if applicable)\n```python\nTraceback (most recent call last):\n  File \"/mnt/data1/anaconda3/envs/lit_gpt/lib/python3.12/site-packages/langchain/chains/base.py\", line 209, in ainvoke\n    await self._acall(inputs, run_manager=run_manager)\n  File \"/mnt/data1/anaconda3/envs/lit_gpt/lib/python3.12/site-packages/langchain/agents/agent.py\", line 1669, in _acall\n    next_step_output = await self._atake_next_step(\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/data1/anaconda3/envs/lit_gpt/lib/python3.12/site-packages/langchain/agents/agent.py\", line 1463, in _atake_next_step\n    [\n  File \"/mnt/data1/anaconda3/envs/lit_gpt/lib/python3.12/site-packages/langchain/agents/agent.py\", line 1491, in _aiter_next_step\n    output = await self._action_agent.aplan(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/data1/anaconda3/envs/lit_gpt/lib/python3.12/site-packages/langchain/agents/agent.py\", line 506, in aplan\n    final_output += chunk\n  File \"/mnt/data1/anaconda3/envs/lit_gpt/lib/python3.12/site-packages/langgraph/pregel/io.py\", line 133, in __radd__\n    raise TypeError(\"AddableUpdatesDict does not support right-side addition\")\nTypeError: AddableUpdatesDict does not support right-side addition\n```\n### Description\n\nI'm trying to use AgentExecutor.\n\nbut something is wrong.. It's ok when i use ```create_react_agent``` instance\n\n\n### System Info\n\nSystem Information\n------------------\n> OS:  Linux\n> OS Version:  #15~24.04.1-Ubuntu SMP Fri Jul 25 23:26:08 UTC 2025\n> Python Version:  3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:56:27) [GCC 11.2.0]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.70\n> langchain: 0.3.26\n> langchain_community: 0.3.27\n> langsmith: 0.3.45\n> langchain_chroma: 0.2.4\n> langchain_cohere: 0.3.5\n> langchain_docling: 1.0.0\n> langchain_experimental: 0.3.4\n> langchain_google_genai: 2.1.7\n> langchain_groq: 0.3.4\n> langchain_huggingface: 0.3.1\n> langchain_mcp_adapters: 0.1.9\n> langchain_ollama: 0.3.6\n> langchain_openai: 0.2.14\n> langchain_pymupdf4llm: 0.4.1\n> langchain_text_splitters: 0.3.8\n> langchain_unstructured: 0.1.2\n> langgraph_sdk: 0.1.70\n\nOptional packages not installed\n-------------------------------\n> langserve\n\nOther Dependencies\n------------------\n> aiohttp<4.0.0,>=3.8.3: Installed. No version info available.\n> async-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\n> chromadb>=1.0.9: Installed. No version info available.\n> cohere: 5.16.1\n> dataclasses-json<0.7,>=0.5.7: Installed. No version info available.\n> docling~=2.18: Installed. No version info available.\n> filetype: 1.2.0\n> google-ai-generativelanguage: 0.6.18\n> groq<1,>=0.28.0: Installed. No version info available.\n> httpx: 0.28.1\n> httpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\n> httpx>=0.25.2: Installed. No version info available.\n> huggingface-hub>=0.33.4: Installed. No version info available.\n> jsonpatch<2.0,>=1.33: Installed. No version info available.\n> langchain-anthropic;: Installed. No version info available.\n> langchain-aws;: Installed. No version info available.\n> langchain-azure-ai;: Installed. No version info available.\n> langchain-cohere;: Installed. No version info available.\n> langchain-community;: Installed. No version info available.\n> langchain-core<0.4,>=0.3.36: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.51: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.66: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.70: Installed. No version info available.\n> langchain-core>=0.3.60: Installed. No version info available.\n> langchain-core~=0.3.19: Installed. No version info available.\n> langchain-deepseek;: Installed. No version info available.\n> langchain-fireworks;: Installed. No version info available.\n> langchain-google-genai;: Installed. No version info available.\n> langchain-google-vertexai;: Installed. No version info available.\n> langchain-groq;: Installed. No version info available.\n> langchain-huggingface;: Installed. No version info available.\n> langchain-mistralai;: Installed. No version info available.\n> langchain-ollama;: Installed. No version info available.\n> langchain-openai;: Installed. No version info available.\n> langchain-perplexity;: Installed. No version info available.\n> langchain-text-splitters<1.0.0,>=0.3.8: Installed. No version info available.\n> langchain-together;: Installed. No version info available.\n> langchain-xai;: Installed. No version info available.\n> langchain<1.0.0,>=0.3.26: Installed. No version info available.\n> langsmith-pyo3: Installed. No version info available.\n> langsmith>=0.1.125: Installed. No version info available.\n> langsmith>=0.1.17: Installed. No version info available.\n> langsmith>=0.3.45: Installed. No version info available.\n> mcp>=1.9.2: Installed. No version info available.\n> numpy>=1.26.0;: Installed. No version info available.\n> numpy>=1.26.2;: Installed. No version info available.\n> numpy>=2.1.0;: Installed. No version info available.\n> ollama<1.0.0,>=0.5.1: Installed. No version info available.\n> openai: 1.90.0\n> openai-agents: Installed. No version info available.\n> opentelemetry-api: 1.36.0\n> opentelemetry-exporter-otlp-proto-http: 1.36.0\n> opentelemetry-sdk: 1.36.0\n> orjson: 3.10.18\n> orjson>=3.10.1: Installed. No version info available.\n> packaging: 24.2\n> packaging>=23.2: Installed. No version info available.\n> pandas: 2.2.3\n> pydantic: 2.11.7\n> pydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4: Installed. No version info available.\n> pydantic>=2.7.4: Installed. No version info available.\n> pymupdf4llm: 0.0.24\n> pytest: 8.3.5\n> PyYAML>=5.3: Installed. No version info available.\n> requests: 2.32.3\n> requests-toolbelt: 1.0.0\n> requests<3,>=2: Installed. No version info available.\n> rich: 13.9.4\n> sentence-transformers>=2.6.0;: Installed. No version info available.\n> SQLAlchemy<3,>=1.4: Installed. No version info available.\n> tabulate: 0.9.0\n> tenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\n> tenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\n> tiktoken: 0.9.0\n> tokenizers>=0.19.1: Installed. No version info available.\n> transformers>=4.39.0;: Installed. No version info available.\n> typing-extensions>=4.14.0: Installed. No version info available.\n> typing-extensions>=4.7: Installed. No version info available.\n> unstructured-client: 0.24.1\n> unstructured[all-docs]: Installed. No version info available.\n> zstandard: 0.23.0", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33046/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33046/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33041", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33041/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33041/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33041/events", "html_url": "https://github.com/langchain-ai/langchain/issues/33041", "id": 3438046477, "node_id": "I_kwDOIPDwls7M7G0N", "number": 33041, "title": "reasoning parameter not working as expected in chatollama", "user": {"login": "mudassir206", "id": 195078458, "node_id": "U_kgDOC6CpOg", "avatar_url": "https://avatars.githubusercontent.com/u/195078458?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mudassir206", "html_url": "https://github.com/mudassir206", "followers_url": "https://api.github.com/users/mudassir206/followers", "following_url": "https://api.github.com/users/mudassir206/following{/other_user}", "gists_url": "https://api.github.com/users/mudassir206/gists{/gist_id}", "starred_url": "https://api.github.com/users/mudassir206/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mudassir206/subscriptions", "organizations_url": "https://api.github.com/users/mudassir206/orgs", "repos_url": "https://api.github.com/users/mudassir206/repos", "events_url": "https://api.github.com/users/mudassir206/events{/privacy}", "received_events_url": "https://api.github.com/users/mudassir206/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 5680700839, "node_id": "LA_kwDOIPDwls8AAAABUpidpw", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/bug", "name": "bug", "color": "b60205", "default": true, "description": "Related to a bug, vulnerability, unexpected error with an existing feature"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2025-09-21T08:55:18Z", "updated_at": "2025-09-22T16:45:08Z", "closed_at": null, "author_association": "NONE", "type": {"id": 18879550, "node_id": "IT_kwDOB43M6c4BIBQ-", "name": "Bug", "description": "An unexpected problem or behavior", "color": "red", "created_at": "2024-02-16T01:43:33Z", "updated_at": "2024-07-26T15:00:33Z", "is_enabled": true}, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Checked other resources\n\n- [x] This is a bug, not a usage question.\n- [x] I added a clear and descriptive title that summarizes this issue.\n- [x] I used the GitHub search to find a similar question and didn't find it.\n- [x] I am sure that this is a bug in LangChain rather than my code.\n- [x] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n- [x] This is not related to the langchain-community package.\n- [x] I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n- [x] I posted a self-contained, minimal, reproducible example. A maintainer can copy it and run it AS IS.\n\n### Example Code\n\nif model_cfg.SUPPORTS_THINKING:\n                return ChatOllama(\n                    model=llm_model_name,\n                    temperature=temperature,\n                    api_key=api_key,\n                    base_url=api_url,\n                    disable_streaming=not streaming,\n                    reasoning=enable_think,\n                    keep_alive=0,\n                )\n            else:\n                return ChatOllama(\n                    model=llm_model_name,\n                    temperature=temperature,\n                    api_key=api_key,\n                    base_url=api_url,\n                    disable_streaming=not streaming,\n                    keep_alive=0,\n                )\n\n### Error Message and Stack Trace (if applicable)\n\n_No response_\n\n### Description\n\nwe have been uisng chatollama while invoking the ollama models.\nhere we are facing one issue.whenever we want to disable think content in the response body we are passing false flag to reasoning parameter,even though we are getting think content in the response body.\n\n### System Info\n\nSystem Information\n------------------\n> OS:  Linux\n> OS Version:  #29~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Aug 14 16:52:50 UTC 2\n> Python Version:  3.11.11 (main, Dec 11 2024, 16:28:39) [GCC 11.2.0]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.72\n> langsmith: 0.4.9\n> langgraph_sdk: 0.2.0\n\nOptional packages not installed\n-------------------------------\n> langserve\n\nOther Dependencies\n------------------\n> httpx: 0.28.1\n> httpx>=0.25.2: Installed. No version info available.\n> jsonpatch<2.0,>=1.33: Installed. No version info available.\n> langsmith-pyo3: Installed. No version info available.\n> langsmith>=0.3.45: Installed. No version info available.\n> openai-agents: Installed. No version info available.\n> opentelemetry-api: Installed. No version info available.\n> opentelemetry-exporter-otlp-proto-http: Installed. No version info available.\n> opentelemetry-sdk: Installed. No version info available.\n> orjson: 3.11.1\n> orjson>=3.10.1: Installed. No version info available.\n> packaging: 24.2\n> packaging>=23.2: Installed. No version info available.\n> pydantic: 2.10.3\n> pydantic>=2.7.4: Installed. No version info available.\n> pytest: 8.3.4\n> PyYAML>=5.3: Installed. No version info available.\n> requests: 2.32.3\n> requests-toolbelt: 1.0.0\n> rich: 13.9.4\n> tenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\n> typing-extensions>=4.7: Installed. No version info available.\n> zstandard: 0.23.0", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33041/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33041/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33019", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33019/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33019/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33019/events", "html_url": "https://github.com/langchain-ai/langchain/issues/33019", "id": 3434404587, "node_id": "I_kwDOIPDwls7MtNrr", "number": 33019, "title": "pylance type checking error when importing tool decorator", "user": {"login": "le-codeur-rapide", "id": 166131032, "node_id": "U_kgDOCeb1WA", "avatar_url": "https://avatars.githubusercontent.com/u/166131032?v=4", "gravatar_id": "", "url": "https://api.github.com/users/le-codeur-rapide", "html_url": "https://github.com/le-codeur-rapide", "followers_url": "https://api.github.com/users/le-codeur-rapide/followers", "following_url": "https://api.github.com/users/le-codeur-rapide/following{/other_user}", "gists_url": "https://api.github.com/users/le-codeur-rapide/gists{/gist_id}", "starred_url": "https://api.github.com/users/le-codeur-rapide/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/le-codeur-rapide/subscriptions", "organizations_url": "https://api.github.com/users/le-codeur-rapide/orgs", "repos_url": "https://api.github.com/users/le-codeur-rapide/repos", "events_url": "https://api.github.com/users/le-codeur-rapide/events{/privacy}", "received_events_url": "https://api.github.com/users/le-codeur-rapide/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 5680700839, "node_id": "LA_kwDOIPDwls8AAAABUpidpw", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/bug", "name": "bug", "color": "b60205", "default": true, "description": "Related to a bug, vulnerability, unexpected error with an existing feature"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2025-09-19T13:34:28Z", "updated_at": "2025-09-20T20:42:15Z", "closed_at": null, "author_association": "NONE", "type": {"id": 18879550, "node_id": "IT_kwDOB43M6c4BIBQ-", "name": "Bug", "description": "An unexpected problem or behavior", "color": "red", "created_at": "2024-02-16T01:43:33Z", "updated_at": "2024-07-26T15:00:33Z", "is_enabled": true}, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Checked other resources\n\n- [x] This is a bug, not a usage question.\n- [x] I added a clear and descriptive title that summarizes this issue.\n- [x] I used the GitHub search to find a similar question and didn't find it.\n- [x] I am sure that this is a bug in LangChain rather than my code.\n- [x] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n- [x] This is not related to the langchain-community package.\n- [x] I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n- [x] I posted a self-contained, minimal, reproducible example. A maintainer can copy it and run it AS IS.\n\n### Example Code\n\nThe following code:\n\n```python\nfrom langchain_core.tools import tool\n```\n\n### Error Message and Stack Trace (if applicable)\n\nType checker pylance: strict mode) display the error message:\n```\nType of \"tool\" is partially unknown\nType of \"tool\" is \"Overload[(*, description: str | None = None, return_direct: bool = False, args_schema: type[BaseModel] | dict[str, Any] | None = None, infer_schema: bool = True, response_format: Literal['content', 'content_and_artifact'] = \"content\", parse_docstring: bool = False, error_on_invalid_docstring: bool = True) -> (((((...) -> Any) | Runnable[Any, Any])) -> BaseTool), (name_or_callable: str, runnable: Runnable[Unknown, Unknown], *, description: str | None = None, return_direct: bool = False, args_schema: type[BaseModel] | dict[str, Any] | None = None, infer_schema: bool = True, response_format: Literal['content', 'content_and_artifact'] = \"content\", parse_docstring: bool = False, error_on_invalid_docstring: bool = True) -> BaseTool, (name_or_callable: (...) -> Unknown, *, description: str | None = None, return_direct: bool = False, args_schema: type[BaseModel] | dict[str, Any] | None = None, infer_schema: bool = True, response_format: Literal['content', 'content_and_artifact'] = \"content\", parse_docstring: bool = False, error_on_invalid_docstring: bool = True) -> BaseTool, (name_or_callable: str, *, description: str | None = None, return_direct: bool = False, args_schema: type[BaseModel] | dict[str, Any] | None = None, infer_schema: bool = True, response_format: Literal['content', 'content_and_artifact'] = \"content\", parse_docstring: bool = False, error_on_invalid_docstring: bool = True) -> (((((...) -> Any) | Runnable[Any, Any])) -> BaseTool)]\"Pylance[reportUnknownVariableType](https://github.com/microsoft/pylance-release/blob/main/docs/diagnostics/reportUnknownVariableType.md)\n````\n\n### Description\n\nI want to fix the tool decorator function type hints so it does not cause a type error when I import it using strict mode.\n\n### System Info\n\nlangchain_core: 0.3.69", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33019/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33019/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33018", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33018/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33018/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33018/events", "html_url": "https://github.com/langchain-ai/langchain/issues/33018", "id": 3433264894, "node_id": "I_kwDOIPDwls7Mo3b-", "number": 33018, "title": "Can't use LangFuse with LangChain v1-alpha", "user": {"login": "SAIL-Fang", "id": 135190100, "node_id": "U_kgDOCA7WVA", "avatar_url": "https://avatars.githubusercontent.com/u/135190100?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SAIL-Fang", "html_url": "https://github.com/SAIL-Fang", "followers_url": "https://api.github.com/users/SAIL-Fang/followers", "following_url": "https://api.github.com/users/SAIL-Fang/following{/other_user}", "gists_url": "https://api.github.com/users/SAIL-Fang/gists{/gist_id}", "starred_url": "https://api.github.com/users/SAIL-Fang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SAIL-Fang/subscriptions", "organizations_url": "https://api.github.com/users/SAIL-Fang/orgs", "repos_url": "https://api.github.com/users/SAIL-Fang/repos", "events_url": "https://api.github.com/users/SAIL-Fang/events{/privacy}", "received_events_url": "https://api.github.com/users/SAIL-Fang/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 5680700839, "node_id": "LA_kwDOIPDwls8AAAABUpidpw", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/bug", "name": "bug", "color": "b60205", "default": true, "description": "Related to a bug, vulnerability, unexpected error with an existing feature"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2025-09-19T07:41:07Z", "updated_at": "2025-09-19T08:02:21Z", "closed_at": "2025-09-19T08:02:21Z", "author_association": "NONE", "type": {"id": 18879550, "node_id": "IT_kwDOB43M6c4BIBQ-", "name": "Bug", "description": "An unexpected problem or behavior", "color": "red", "created_at": "2024-02-16T01:43:33Z", "updated_at": "2024-07-26T15:00:33Z", "is_enabled": true}, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Checked other resources\n\n- [x] This is a bug, not a usage question.\n- [x] I added a clear and descriptive title that summarizes this issue.\n- [x] I used the GitHub search to find a similar question and didn't find it.\n- [x] I am sure that this is a bug in LangChain rather than my code.\n- [ ] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n- [x] This is not related to the langchain-community package.\n- [x] I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n- [x] I posted a self-contained, minimal, reproducible example. A maintainer can copy it and run it AS IS.\n\n### Example Code\n\nI added Langfuse to the example on the official website to observe the operation of the agent, and then an error occurred.\n\nThe code as follow:\n```python\nfrom pydantic import BaseModel\nfrom langchain.agents import create_agent\nfrom pydantic import Field\nfrom langchain_openai import ChatOpenAI\nfrom langfuse import Langfuse\nfrom langfuse.langchain import CallbackHandler\n\nlangfuse = Langfuse(\n    public_key=\"pk-lf-e809c2d1-06bd-43f9-9355-f83679487e20\",\n    secret_key=\"sk-lf-49a3b6fd-ac1e-4330-a2af-d2a2895687eb\",\n    host=\"http://192.168.4.88:3000\"\n)\n\nlangfuse_handler = CallbackHandler()\n\nclass ContactInfo(BaseModel):\n    \"\"\"Contact information for a person.\"\"\"\n    name: str = Field(description=\"The name of the person\")\n    email: str = Field(description=\"The email address of the person\")\n    phone: str = Field(description=\"The phone number of the person\")\n\nmodel = ChatOpenAI(\n    model=\"deepseek-v3-1-250821\",\n    openai_api_key=\"462e555e-e16d-47e7-ba14-5088cffed156\",\n    openai_api_base=\"https://ark.cn-beijing.volces.com/api/v3\",\n    timeout=1800,\n    extra_body={\"thinking\": {\"type\": \"enabled\"}},\n)\n\n\nagent = create_agent(\n    model=model,\n    tools=[],\n    response_format=ContactInfo  # Auto-selects ProviderStrategy\n)\n\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}]\n}, config={\"callbacks\": [langfuse_handler]})\n\nresult[\"structured_response\"]\n# ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567')\n```\n\n\n\n### Error Message and Stack Trace (if applicable)\n\n```\nModuleNotFoundError: Please install langchain to use the Langfuse langchain integration: 'pip install langchain'\n```\n\n\n### Description\n\nThe problem lies in the fact that the CallbackHandler.py between LangFuse groups has not yet been adapted to the latest langchain v1 version. I also have no idea where the 'BaseCallbackHandler' has been moved and cannot be retrieved.\n```\n    from langchain.callbacks.base import (\n        BaseCallbackHandler as LangchainBaseCallbackHandler,\n    )\n```\n\n### System Info\n\n\nSystem Information\n------------------\n> OS:  Windows\n> OS Version:  10.0.22631\n> Python Version:  3.10.11 | packaged by Anaconda, Inc. | (main, May 16 2023, 00:55:32) [MSC v.1916 64 bit (AMD64)]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.76\n> langchain: 1.0.0a5\n> langsmith: 0.4.6\n> langchain_anthropic: 0.3.20\n> langchain_openai: 0.3.33\n> langchain_text_splitters: 0.3.11\n> langgraph_sdk: 0.2.6\n> langgraph_supervisor: 0.0.29\n\nOptional packages not installed\n-------------------------------\n> langserve\n\nOther Dependencies\n------------------\n> anthropic<1,>=0.67.0: Installed. No version info available.\n> httpx: 0.27.0\n> httpx>=0.25.2: Installed. No version info available.\n> jsonpatch<2.0,>=1.33: Installed. No version info available.\n> langchain-anthropic;: Installed. No version info available.\n> langchain-aws;: Installed. No version info available.\n> langchain-azure-ai;: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.76: Installed. No version info available.\n> langchain-core<2.0.0,>=0.3.75: Installed. No version info available.\n> langchain-core>=0.3.40: Installed. No version info available.\n> langchain-deepseek;: Installed. No version info available.\n> langchain-fireworks;: Installed. No version info available.\n> langchain-google-genai;: Installed. No version info available.\n> langchain-google-vertexai;: Installed. No version info available.\n> langchain-groq;: Installed. No version info available.\n> langchain-huggingface;: Installed. No version info available.\n> langchain-mistralai;: Installed. No version info available.\n> langchain-ollama;: Installed. No version info available.\n> langchain-openai;: Installed. No version info available.\n> langchain-perplexity;: Installed. No version info available.\n> langchain-text-splitters<1.0.0,>=0.3.11: Installed. No version info available.\n> langchain-together;: Installed. No version info available.\n> langchain-xai;: Installed. No version info available.\n> langgraph<0.7.0,>=0.6.0: Installed. No version info available.\n> langgraph>=0.6.7: Installed. No version info available.\n> langsmith-pyo3: Installed. No version info available.\n> langsmith>=0.3.45: Installed. No version info available.\n> openai-agents: Installed. No version info available.\n> openai<2.0.0,>=1.104.2: Installed. No version info available.\n> opentelemetry-api: 1.35.0\n> opentelemetry-exporter-otlp-proto-http: 1.35.0\n> opentelemetry-sdk: 1.35.0\n> orjson: 3.10.16\n> orjson>=3.10.1: Installed. No version info available.\n> packaging: 23.2\n> packaging>=23.2: Installed. No version info available.\n> pydantic: 2.10.6\n> pydantic<3.0.0,>=2.7.4: Installed. No version info available.\n> pydantic>=2.7.4: Installed. No version info available.\n> pytest: Installed. No version info available.\n> PyYAML>=5.3: Installed. No version info available.\n> requests: 2.32.3\n> requests-toolbelt: 1.0.0\n> rich: Installed. No version info available.\n> tenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\n> tiktoken<1,>=0.7: Installed. No version info available.\n> typing-extensions>=4.7: Installed. No version info available.\n> zstandard: 0.23.0", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33018/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33018/timeline", "performed_via_github_app": null, "state_reason": "completed", "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33016", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33016/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33016/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33016/events", "html_url": "https://github.com/langchain-ai/langchain/issues/33016", "id": 3432353756, "node_id": "I_kwDOIPDwls7MlY_c", "number": 33016, "title": "OutputParserException when using Ollama instead of Gemini", "user": {"login": "philippe-lavoie", "id": 5925321, "node_id": "MDQ6VXNlcjU5MjUzMjE=", "avatar_url": "https://avatars.githubusercontent.com/u/5925321?v=4", "gravatar_id": "", "url": "https://api.github.com/users/philippe-lavoie", "html_url": "https://github.com/philippe-lavoie", "followers_url": "https://api.github.com/users/philippe-lavoie/followers", "following_url": "https://api.github.com/users/philippe-lavoie/following{/other_user}", "gists_url": "https://api.github.com/users/philippe-lavoie/gists{/gist_id}", "starred_url": "https://api.github.com/users/philippe-lavoie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/philippe-lavoie/subscriptions", "organizations_url": "https://api.github.com/users/philippe-lavoie/orgs", "repos_url": "https://api.github.com/users/philippe-lavoie/repos", "events_url": "https://api.github.com/users/philippe-lavoie/events{/privacy}", "received_events_url": "https://api.github.com/users/philippe-lavoie/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 5680700839, "node_id": "LA_kwDOIPDwls8AAAABUpidpw", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/bug", "name": "bug", "color": "b60205", "default": true, "description": "Related to a bug, vulnerability, unexpected error with an existing feature"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2025-09-19T01:30:06Z", "updated_at": "2025-09-19T01:30:06Z", "closed_at": null, "author_association": "NONE", "type": {"id": 18879550, "node_id": "IT_kwDOB43M6c4BIBQ-", "name": "Bug", "description": "An unexpected problem or behavior", "color": "red", "created_at": "2024-02-16T01:43:33Z", "updated_at": "2024-07-26T15:00:33Z", "is_enabled": true}, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Checked other resources\n\n- [x] This is a bug, not a usage question.\n- [x] I added a clear and descriptive title that summarizes this issue.\n- [x] I used the GitHub search to find a similar question and didn't find it.\n- [x] I am sure that this is a bug in LangChain rather than my code.\n- [x] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n- [x] This is not related to the langchain-community package.\n- [x] I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n- [x] I posted a self-contained, minimal, reproducible example. A maintainer can copy it and run it AS IS.\n\n### Example Code\n\nThe following can be used to see that the tutorial example works with Gemini but not with Ollama\n\n```javascript\nimport { ChatOllama } from \"@langchain/ollama\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { z } from \"zod\";\n//import { ChatGoogleGenerativeAI } from \"@langchain/google-genai\";\n\n// const llm = new ChatGoogleGenerativeAI({\n//   model: \"gemini-2.0-flash\",\n//   temperature: 0,\n//   apiKey: \"Use your own key\",\n// });\n\nconst llm = new ChatOllama({\n  model: \"llama3.2:3b\",\n  temperature: 0,\n  baseUrl: \"http://localhost:11434\"\n})\n\n\n\nconst classificationSchema2 = z.object({\n  sentiment: z\n    .enum([\"happy\", \"neutral\", \"sad\"])\n    .describe(\"The sentiment of the text\"),\n  aggressiveness: z\n    .number()\n    .int()\n    .describe(\n      \"describes how aggressive the statement is on a scale from 1 to 5. The higher the number the more aggressive\"\n    ),\n  language: z\n    .enum([\"spanish\", \"english\", \"french\", \"german\", \"italian\"])\n    .describe(\"The language the text is written in\"),\n});\n\nconst taggingPrompt2 = ChatPromptTemplate.fromTemplate(\n  `Extract the desired information from the following passage.\n\nPassage:\n{input}\n`\n);\n\nconst llmWithStructuredOutput2 = llm.withStructuredOutput(\n  classificationSchema2,\n  { name: \"extractor\" }\n);\n\nconst prompt2 = await taggingPrompt2.invoke({\n  input:\n    \"Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!\"\n});\n\nconst result = await llmWithStructuredOutput2.invoke(prompt2);\n\nconsole.log(\"Result:\", result);\n```\n\n\n### Error Message and Stack Trace (if applicable)\n\n```\n throw new OutputParserException(`Failed to parse. Text: \"${text}\". Error: ${e}`, text);\n                  ^\n\nOutputParserException [Error]: Failed to parse. Text: \"\". Error: SyntaxError: Unexpected end of JSON input\n\nTroubleshooting URL: https://js.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/\n```\n\n### Description\n\nI'm trying to follow the tutorials but it's not working with Ollama. The integration seems to have a bug.  The example code can be used to easily switch between Ollama and Gemini, just modify the commented lines.\n\n### System Info\n\nI'm using javascript with the following dependencies\n\n  \"dependencies\": {\n    \"@langchain/community\": \"^0.3.54\",\n    \"@langchain/core\": \"^0.3.73\",\n    \"@langchain/google-genai\": \"^0.2.18\",\n    \"@langchain/ollama\": \"^0.2.4\",\n    \"ollama\": \"^0.5.18\",\n    \"pdf-parse\": \"^1.1.1\",\n    \"zod-to-json-schema\": \"^3.24.6\"\n  },\n  ", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33016/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33016/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33014", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33014/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33014/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33014/events", "html_url": "https://github.com/langchain-ai/langchain/issues/33014", "id": 3432232656, "node_id": "I_kwDOIPDwls7Mk7bQ", "number": 33014, "title": "Streaming inactivity timeout incorrectly aborts after total timeout ( @langchain/openai\": \"^1.0.0-alpha.1\" )", "user": {"login": "mlibre", "id": 8473036, "node_id": "MDQ6VXNlcjg0NzMwMzY=", "avatar_url": "https://avatars.githubusercontent.com/u/8473036?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mlibre", "html_url": "https://github.com/mlibre", "followers_url": "https://api.github.com/users/mlibre/followers", "following_url": "https://api.github.com/users/mlibre/following{/other_user}", "gists_url": "https://api.github.com/users/mlibre/gists{/gist_id}", "starred_url": "https://api.github.com/users/mlibre/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mlibre/subscriptions", "organizations_url": "https://api.github.com/users/mlibre/orgs", "repos_url": "https://api.github.com/users/mlibre/repos", "events_url": "https://api.github.com/users/mlibre/events{/privacy}", "received_events_url": "https://api.github.com/users/mlibre/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 9260554836, "node_id": "LA_kwDOIPDwls8AAAACJ_jaVA", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/feature%20request", "name": "feature request", "color": "058a42", "default": false, "description": "request for an enhancement / additional functionality"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2025-09-18T23:56:16Z", "updated_at": "2025-09-24T13:32:13Z", "closed_at": null, "author_association": "NONE", "type": {"id": 18879553, "node_id": "IT_kwDOB43M6c4BIBRB", "name": "Feature", "description": "A request, idea, or new functionality", "color": "blue", "created_at": "2024-02-16T01:43:34Z", "updated_at": "2024-10-08T21:10:19Z", "is_enabled": true}, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Checked other resources\n\n- [x] This is a feature request, not a bug report or usage question.\n- [x] I added a clear and descriptive title that summarizes the feature request.\n- [x] I used the GitHub search to find a similar feature request and didn't find it.\n- [x] I checked the LangChain documentation and API reference to see if this feature already exists.\n- [x] This is not related to the langchain-community package.\n\n### Feature Description\n\nWhen using the streaming API, the stream is aborted after X seconds total, regardless of whether chunks are still arriving. I need an inactivity timeout: the stream should abort only if no chunk arrives for X seconds, not after X seconds total request time.\n\nin this example \n```js\nawait llm.invoke( messages, { timeout: 3000 });\n```\n\nthe output is something like this:\n\n```bash\nnode usage.js  \nQuantum computing is a new type of computing that uses the principles of quantum mechanics to process information in a fundamentally different way from classical computers. Here\u2019s a simple breakdown:\n\n1. **Qubits vs. Bits**:  \n   - Classical computers use **bits**, which can be either 0 or 1.  \n   - Quantum computers use **qubits** (quantum bits), which can be 0, 1, or both at the same time. This is called **superposition**.  \n\n2. **Superposition**:  \n   Imagine a coin spinning in the air\u2014it\u2019s neither fully heads nor tails until it lands. Qubits work similarly, existing in multiple states simultaneously, allowing quantum computers to process many possibilities at once.  \n\n3. **Entanglement**:  \n   Qubits can be **entangled**, meaning the state of one qubit instantly affects the state of another, no matter how far apart they are. This creates powerful connections that speed up calculations.  \n\n4. **Parallel Processing**:  \n   Because of superposition, quantum computers can explore many solutions toAll providers failed: The operation was aborted due to timeout\n```\n\nthe connection gets closed after 3 second. the message in above log:\n\n`The operation was aborted due to timeout`\n\n### Use Case\n\nit helps for load balancing between providers and ....\n\n### Proposed Solution\n\nadd an option:\n\n```js\nawait llm.invoke( messages, { stream_timeout: 3000 });\n```\n\n### Alternatives Considered\n\n_No response_\n\n### Additional Context\n\n_No response_", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33014/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33014/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33005", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33005/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33005/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33005/events", "html_url": "https://github.com/langchain-ai/langchain/issues/33005", "id": 3430578037, "node_id": "I_kwDOIPDwls7Mend1", "number": 33005, "title": "feat(langchain): consider adding `llm` as alias for `model` in `create_agent`", "user": {"login": "mdrxy", "id": 61371264, "node_id": "MDQ6VXNlcjYxMzcxMjY0", "avatar_url": "https://avatars.githubusercontent.com/u/61371264?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mdrxy", "html_url": "https://github.com/mdrxy", "followers_url": "https://api.github.com/users/mdrxy/followers", "following_url": "https://api.github.com/users/mdrxy/following{/other_user}", "gists_url": "https://api.github.com/users/mdrxy/gists{/gist_id}", "starred_url": "https://api.github.com/users/mdrxy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mdrxy/subscriptions", "organizations_url": "https://api.github.com/users/mdrxy/orgs", "repos_url": "https://api.github.com/users/mdrxy/repos", "events_url": "https://api.github.com/users/mdrxy/events{/privacy}", "received_events_url": "https://api.github.com/users/mdrxy/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2025-09-18T14:13:24Z", "updated_at": "2025-09-21T00:59:05Z", "closed_at": "2025-09-21T00:59:05Z", "author_association": "COLLABORATOR", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Privileged issue\n\n- [x] I am a LangChain maintainer, or was asked directly by a LangChain maintainer to create an issue here.\n\n### Issue Content\n\n_No response_", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33005/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33005/timeline", "performed_via_github_app": null, "state_reason": "not_planned", "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33003", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33003/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33003/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33003/events", "html_url": "https://github.com/langchain-ai/langchain/issues/33003", "id": 3430396681, "node_id": "I_kwDOIPDwls7Md7MJ", "number": 33003, "title": "BurnCloud seeks to contribute enhancements - Permission to submit PR", "user": {"login": "zuiyue-com", "id": 134291161, "node_id": "U_kgDOCAEe2Q", "avatar_url": "https://avatars.githubusercontent.com/u/134291161?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zuiyue-com", "html_url": "https://github.com/zuiyue-com", "followers_url": "https://api.github.com/users/zuiyue-com/followers", "following_url": "https://api.github.com/users/zuiyue-com/following{/other_user}", "gists_url": "https://api.github.com/users/zuiyue-com/gists{/gist_id}", "starred_url": "https://api.github.com/users/zuiyue-com/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zuiyue-com/subscriptions", "organizations_url": "https://api.github.com/users/zuiyue-com/orgs", "repos_url": "https://api.github.com/users/zuiyue-com/repos", "events_url": "https://api.github.com/users/zuiyue-com/events{/privacy}", "received_events_url": "https://api.github.com/users/zuiyue-com/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 9260554836, "node_id": "LA_kwDOIPDwls8AAAACJ_jaVA", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/feature%20request", "name": "feature request", "color": "058a42", "default": false, "description": "request for an enhancement / additional functionality"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2025-09-18T13:28:42Z", "updated_at": "2025-09-18T13:36:28Z", "closed_at": "2025-09-18T13:36:27Z", "author_association": "NONE", "type": {"id": 18879553, "node_id": "IT_kwDOB43M6c4BIBRB", "name": "Feature", "description": "A request, idea, or new functionality", "color": "blue", "created_at": "2024-02-16T01:43:34Z", "updated_at": "2024-10-08T21:10:19Z", "is_enabled": true}, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Checked other resources\n\n- [x] This is a feature request, not a bug report or usage question.\n- [x] I added a clear and descriptive title that summarizes the feature request.\n- [x] I used the GitHub search to find a similar feature request and didn't find it.\n- [x] I checked the LangChain documentation and API reference to see if this feature already exists.\n- [x] This is not related to the langchain-community package.\n\n### Feature Description\n\n## Hello! \ud83d\udc4b \n\ufeff \nWe are **BurnCloud**, a large language model provider, and we're very interested in contributing to your excellent open-source project. \n\ufeff \n### About Us \n- Company: BURNCLOUD LTD \n- Focus: Large Language Model services and solutions \n- Website: https://www.burncloud.com\n- API: https://ai.burncloud.com\n\n\n### Contact Us\n- Email: huangwei@burncloud.com\n\n\n### Use Case\n\n### What we can offer:\n\n- Free API access for open source projects\n- Support for 100+ models including Claude, GPT, Gemini\n- Stable infrastructure with competitive pricing\n\n\n### Proposed Solution\n\n_No response_\n\n### Alternatives Considered\n\n_No response_\n\n### Additional Context\n\n_No response_", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33003/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33003/timeline", "performed_via_github_app": null, "state_reason": "completed", "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33002", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33002/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33002/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33002/events", "html_url": "https://github.com/langchain-ai/langchain/issues/33002", "id": 3430324712, "node_id": "I_kwDOIPDwls7Mdpno", "number": 33002, "title": "`StringPromptTemplate` is missing implementation of abstract base method `format`", "user": {"login": "Simon-Stone", "id": 18614423, "node_id": "MDQ6VXNlcjE4NjE0NDIz", "avatar_url": "https://avatars.githubusercontent.com/u/18614423?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Simon-Stone", "html_url": "https://github.com/Simon-Stone", "followers_url": "https://api.github.com/users/Simon-Stone/followers", "following_url": "https://api.github.com/users/Simon-Stone/following{/other_user}", "gists_url": "https://api.github.com/users/Simon-Stone/gists{/gist_id}", "starred_url": "https://api.github.com/users/Simon-Stone/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Simon-Stone/subscriptions", "organizations_url": "https://api.github.com/users/Simon-Stone/orgs", "repos_url": "https://api.github.com/users/Simon-Stone/repos", "events_url": "https://api.github.com/users/Simon-Stone/events{/privacy}", "received_events_url": "https://api.github.com/users/Simon-Stone/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 5680700839, "node_id": "LA_kwDOIPDwls8AAAABUpidpw", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/bug", "name": "bug", "color": "b60205", "default": true, "description": "Related to a bug, vulnerability, unexpected error with an existing feature"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2025-09-18T13:11:57Z", "updated_at": "2025-09-18T13:14:18Z", "closed_at": "2025-09-18T13:14:18Z", "author_association": "CONTRIBUTOR", "type": {"id": 18879550, "node_id": "IT_kwDOB43M6c4BIBQ-", "name": "Bug", "description": "An unexpected problem or behavior", "color": "red", "created_at": "2024-02-16T01:43:33Z", "updated_at": "2024-07-26T15:00:33Z", "is_enabled": true}, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Checked other resources\n\n- [x] This is a bug, not a usage question.\n- [x] I added a clear and descriptive title that summarizes this issue.\n- [x] I used the GitHub search to find a similar question and didn't find it.\n- [x] I am sure that this is a bug in LangChain rather than my code.\n- [x] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n- [x] This is not related to the langchain-community package.\n- [x] I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n- [x] I posted a self-contained, minimal, reproducible example. A maintainer can copy it and run it AS IS.\n\n### Example Code\n\n```python\nfrom langchain_core.prompts import StringPromptTemplate\n\nprompt = StringPromptTemplate(\"Tell me a joke about {topic}\", input_variables=[\"topic\"])\n\nprint(prompt.format_prompt(topic=\"dogs\"))\n```\n\n### Error Message and Stack Trace (if applicable)\n\n```\nTraceback (most recent call last):\n  File \"/Users/f006pfk/source/bug.py\", line 4, in <module>\n    prompt = StringPromptTemplate(\"Tell me a joke about {topic}\", input_variables=[\"topic\"])\nTypeError: Can't instantiate abstract class StringPromptTemplate without an implementation for abstract method 'format'\n```\n\n### Description\n\nThe class `StringPromptTemplate` as a subclass of `BasePromptTemplate` has to implement the abstract method `format`, but that is not currently the case. \n\nLine 291 in `langchain_core.prompts.string.py` even tries to call the abstract base method.\n\n### System Info\n\nSystem Information\n------------------\n> OS:  Darwin\n> OS Version:  Darwin Kernel Version 24.6.0: Mon Jul 14 11:30:29 PDT 2025; root:xnu-11417.140.69~1/RELEASE_ARM64_T6000\n> Python Version:  3.13.2 (main, Feb  4 2025, 14:51:09) [Clang 16.0.0 (clang-1600.0.26.6)]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.76\n> langchain: 0.3.24", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33002/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33002/timeline", "performed_via_github_app": null, "state_reason": "completed", "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33059", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33059/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33059/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33059/events", "html_url": "https://github.com/langchain-ai/langchain/issues/33059", "id": 3446304590, "node_id": "I_kwDOIPDwls7Nam9O", "number": 33059, "title": "ChatBedrockConverse does not emit on_chat_model_stream events in LangGraph astream_events()", "user": {"login": "jabrPromtior", "id": 165715814, "node_id": "U_kgDOCeCfZg", "avatar_url": "https://avatars.githubusercontent.com/u/165715814?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jabrPromtior", "html_url": "https://github.com/jabrPromtior", "followers_url": "https://api.github.com/users/jabrPromtior/followers", "following_url": "https://api.github.com/users/jabrPromtior/following{/other_user}", "gists_url": "https://api.github.com/users/jabrPromtior/gists{/gist_id}", "starred_url": "https://api.github.com/users/jabrPromtior/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jabrPromtior/subscriptions", "organizations_url": "https://api.github.com/users/jabrPromtior/orgs", "repos_url": "https://api.github.com/users/jabrPromtior/repos", "events_url": "https://api.github.com/users/jabrPromtior/events{/privacy}", "received_events_url": "https://api.github.com/users/jabrPromtior/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 5680700839, "node_id": "LA_kwDOIPDwls8AAAABUpidpw", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/bug", "name": "bug", "color": "b60205", "default": true, "description": "Related to a bug, vulnerability, unexpected error with an existing feature"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2025-09-18T12:30:56Z", "updated_at": "2025-09-23T18:41:00Z", "closed_at": null, "author_association": "NONE", "type": null, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Checked other resources\n\n- [x] This is a bug, not a usage question. For questions, please use the LangChain Forum (https://forum.langchain.com/).\n- [x] I added a clear and detailed title that summarizes the issue.\n- [x] I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n- [x] I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\n### Example Code\n\n```python\n#!/usr/bin/env python3\n\"\"\"\nMinimal Reproducible Example (MRE) for ChatBedrock vs ChatBedrockConverse streaming event issue.\n\nThis script demonstrates the difference in event emission between ChatBedrock and ChatBedrockConverse\nwhen used with LangGraph's astream_events().\n\nExpected behavior:\n- ChatBedrock should emit 'on_chat_model_stream' events during streaming\n- ChatBedrockConverse may emit different events or no streaming events\n\nRun with: python debug_bedrock_streaming.py\n\"\"\"\n\nimport asyncio\nimport os\nfrom typing import Dict, Any, List\n\nfrom langchain_aws import ChatBedrock, ChatBedrockConverse\nfrom langchain_core.messages import HumanMessage\nfrom langgraph.graph import StateGraph, MessagesState\nfrom langgraph.prebuilt import create_react_agent\n\n\ndef create_react_agent_with_bedrock(model_client):\n    \"\"\"Create a LangGraph agent using create_react_agent (like the actual guest agent).\"\"\"\n\n    # Simple system prompt\n    system_prompt = \"\"\"You are a helpful assistant. Answer questions concisely.\"\"\"\n\n    # Create agent using create_react_agent (same as guest agent)\n    graph = create_react_agent(\n        model=model_client,\n        tools=[],  # No tools, just conversation\n        prompt=system_prompt,\n        name=\"test_agent\",\n    )\n\n    return graph\n\n\nasync def test_streaming_events(graph, test_name: str) -> List[Dict[str, Any]]:\n    \"\"\"Test streaming events from a LangGraph and collect all events.\"\"\"\n    print(f\"\\n=== Testing {test_name} ===\")\n\n    events_collected = []\n    test_message = \"Hello, can you tell me about personal finance in one sentence?\"\n\n    try:\n        async for event in graph.astream_events(\n            {\"messages\": [HumanMessage(content=test_message)]},\n            version=\"v1\"\n        ):\n            event_type = event.get(\"event\", \"unknown\")\n            name = event.get(\"name\", \"unknown\")\n\n            print(f\"Event: {event_type} | Name: {name}\")\n\n            # Collect all events for analysis\n            events_collected.append({\n                \"event\": event_type,\n                \"name\": name,\n                \"data_keys\": list(event.get(\"data\", {}).keys()) if event.get(\"data\") else []\n            })\n\n            # Specifically check for streaming events\n            if event_type == \"on_chat_model_stream\":\n                print(\"  \u2705 FOUND: on_chat_model_stream event!\")\n                # Try to extract streaming content\n                data = event.get(\"data\", {})\n                if \"chunk\" in data:\n                    chunk = data[\"chunk\"]\n                    if hasattr(chunk, \"content\"):\n                        print(f\"    Content: {chunk.content}\")\n            elif event_type == \"on_chat_model_end\":\n                print(\"  \ud83d\udcdd FOUND: on_chat_model_end event (alternative to streaming)\")\n                # Try to extract final content\n                data = event.get(\"data\", {})\n                if \"output\" in data:\n                    output = data[\"output\"]\n                    if hasattr(output, \"content\"):\n                        print(f\"    Final content: {output.content}\")\n            elif event_type == \"on_chain_stream\":\n                print(\"  \ud83d\udd04 FOUND: on_chain_stream event\")\n                # This might contain streaming data too\n                data = event.get(\"data\", {})\n                print(f\"    Stream data keys: {list(data.keys())}\")\n\n                # Check if this contains the actual streaming content\n                if \"chunk\" in data:\n                    chunk = data[\"chunk\"]\n                    print(f\"    Chunk type: {type(chunk)}\")\n                    print(f\"    Chunk keys (if dict): {list(chunk.keys()) if isinstance(chunk, dict) else 'N/A'}\")\n\n                    # Deep inspection of chunk content\n                    if isinstance(chunk, dict):\n                        for key, value in chunk.items():\n                            print(f\"      {key}: {type(value)} = {repr(value)}\")\n                    elif hasattr(chunk, \"content\"):\n                        content = chunk.content\n                        print(f\"    Chunk content: {repr(content)}\")\n                        if isinstance(content, list) and content:\n                            # Handle structured content like in the guest service\n                            for item in content:\n                                if isinstance(item, dict) and \"text\" in item:\n                                    text = item[\"text\"]\n                                    print(f\"    Extracted text: '{text}'\")\n                        elif isinstance(content, str):\n                            print(f\"    String content: '{content}'\")\n                    else:\n                        print(f\"    Chunk repr: {repr(chunk)}\")\n\n            elif event_type == \"on_chat_model_stream\":\n                print(\"  \u2705 FOUND: on_chat_model_stream event!\")\n                # Compare with chat model stream content\n                data = event.get(\"data\", {})\n                print(f\"    Chat model stream data keys: {list(data.keys())}\")\n                if \"chunk\" in data:\n                    chunk = data[\"chunk\"]\n                    print(f\"    Chat model chunk type: {type(chunk)}\")\n                    if hasattr(chunk, \"content\"):\n                        content = chunk.content\n                        print(f\"    Chat model chunk content: {repr(content)}\")\n                    else:\n                        print(f\"    Chat model chunk repr: {repr(chunk)}\")\n\n    except Exception as e:\n        print(f\"\u274c Error during streaming: {e}\")\n        import traceback\n        print(f\"Traceback: {traceback.format_exc()}\")\n        return []\n\n    return events_collected\n\n\nasync def test_direct_streaming():\n    \"\"\"Test direct streaming from ChatBedrockConverse without LangGraph.\"\"\"\n    from langchain_aws import ChatBedrockConverse\n    from langchain_core.messages import HumanMessage\n\n    print(\"\\n\ud83d\udd2c TESTING DIRECT STREAMING (bypassing LangGraph)\")\n    print(\"=\" * 50)\n\n    model_id = \"openai.gpt-oss-120b-1:0\"\n    region = \"us-west-2\"\n\n    try:\n        # Test with default parameters (no streaming parameter needed)\n        chat_model = ChatBedrockConverse(\n            model_id=model_id,\n            region_name=region,\n            temperature=0.4,\n        )\n\n        print(\"Testing direct astream() method...\")\n        messages = [HumanMessage(content=\"Hello, tell me about personal finance in one sentence\")]\n\n        chunks_received = 0\n        full_content = \"\"\n        async for chunk in chat_model.astream(messages):\n            chunks_received += 1\n            print(f\"Chunk {chunks_received}: {type(chunk)}\")\n\n            # Extract content from chunk\n            if hasattr(chunk, 'content'):\n                content = chunk.content\n                if isinstance(content, list):\n                    for item in content:\n                        if isinstance(item, dict) and 'text' in item:\n                            text = item.get('text', '')\n                            if text:\n                                print(f\"  Text: '{text}'\")\n                                full_content += text\n                        elif isinstance(item, str):\n                            print(f\"  String: '{item}'\")\n                            full_content += item\n                elif isinstance(content, str):\n                    print(f\"  Content: '{content}'\")\n                    full_content += content\n                else:\n                    print(f\"  Content: {repr(content)}\")\n\n            # Show response metadata if available\n            if hasattr(chunk, 'response_metadata') and chunk.response_metadata:\n                print(f\"  Metadata: {chunk.response_metadata}\")\n\n            print(f\"  Accumulated so far: '{full_content}'\\n\")\n\n        print(f\"\\n\ud83d\udcca DIRECT STREAMING RESULTS:\")\n        print(f\"  - Chunks received: {chunks_received}\")\n        if chunks_received > 0:\n            print(\"  \u2705 SUCCESS: Direct streaming works!\")\n        else:\n            print(\"  \u274c FAILED: No chunks received\")\n\n    except Exception as e:\n        print(f\"\u274c DIRECT STREAMING ERROR: {e}\")\n        import traceback\n        print(f\"Traceback: {traceback.format_exc()}\")\n\n\nasync def main():\n    \"\"\"Main test function comparing both Bedrock clients.\"\"\"\n\n    # Configuration - CURRENT setup that you're actually using\n    model_id = \"openai.gpt-oss-120b-1:0\"  # Current OpenAI model (only works with ChatBedrockConverse)\n    region = \"us-west-2\"  # Current region\n\n    print(f\"Testing with model: {model_id}\")\n    print(f\"Testing with region: {region}\")\n    print(\"NOTE: ChatBedrock doesn't support this OpenAI model, so we'll see AccessDenied for it\")\n    print()\n\n    print(\"\ud83d\udd0d Testing Bedrock Streaming Event Emission\")\n    print(\"=\" * 50)\n\n    # Test 1: ChatBedrock (old implementation)\n    print(\"\\n--- Setting up ChatBedrock (old) ---\")\n    bedrock_old = ChatBedrock(\n        model_id=model_id,\n        region_name=region,\n        temperature=0.4,\n        streaming=True  # Explicitly enable streaming\n    )\n\n    graph_old = create_react_agent_with_bedrock(bedrock_old)\n    events_old = await test_streaming_events(graph_old, \"ChatBedrock (Old)\")\n\n    # Test 2: ChatBedrockConverse (new implementation)\n    print(\"\\n--- Setting up ChatBedrockConverse (new) ---\")\n    bedrock_new = ChatBedrockConverse(\n        model_id=model_id,\n        region_name=region,\n        temperature=0.4,\n        # Note: ChatBedrockConverse may handle streaming differently\n    )\n\n    graph_new = create_react_agent_with_bedrock(bedrock_new)\n    events_new = await test_streaming_events(graph_new, \"ChatBedrockConverse (New)\")\n\n    # Analysis\n    print(\"\\n\" + \"=\" * 50)\n    print(\"\ud83d\udcca ANALYSIS\")\n    print(\"=\" * 50)\n\n    # Check for streaming events in both tests\n    old_stream_events = [e for e in events_old if e[\"event\"] == \"on_chat_model_stream\"]\n    new_stream_events = [e for e in events_new if e[\"event\"] == \"on_chat_model_stream\"]\n\n    old_end_events = [e for e in events_old if e[\"event\"] == \"on_chat_model_end\"]\n    new_end_events = [e for e in events_new if e[\"event\"] == \"on_chat_model_end\"]\n\n    print(f\"\\nChatBedrock (Old):\")\n    print(f\"  - Total events: {len(events_old)}\")\n    print(f\"  - on_chat_model_stream events: {len(old_stream_events)}\")\n    print(f\"  - on_chat_model_end events: {len(old_end_events)}\")\n\n    print(f\"\\nChatBedrockConverse (New):\")\n    print(f\"  - Total events: {len(events_new)}\")\n    print(f\"  - on_chat_model_stream events: {len(new_stream_events)}\")\n    print(f\"  - on_chat_model_end events: {len(new_end_events)}\")\n\n    # Analysis based on actual usage scenario\n    print(\"\\n\ud83d\udd0d ANALYSIS OF YOUR MIGRATION SCENARIO:\")\n    print(\"=\" * 50)\n\n    if len(old_stream_events) == 0 and len(new_stream_events) == 0:\n        print(\"\ud83d\udccb WHAT'S HAPPENING:\")\n        print(\"  - ChatBedrock doesn't support the OpenAI model you're now using\")\n        print(\"  - ChatBedrockConverse DOES support it, but emits events differently\")\n        print(\"  - Your guest service expects 'on_chat_model_stream' events\")\n        print(\"  - ChatBedrockConverse may emit streaming data via 'on_chain_stream' events\")\n\n    if len(new_end_events) > 0:\n        print(\"\\n\ud83d\udca1 SOLUTION APPROACH:\")\n        print(\"  Update your streaming logic to handle BOTH event types:\")\n        print(\"  1. Listen for 'on_chat_model_stream' (for backward compatibility)\")\n        print(\"  2. Listen for 'on_chain_stream' (for ChatBedrockConverse)\")\n        print(\"  3. Extract content from both event types consistently\")\n\n    print(\"\\n\ud83c\udfaf KEY INSIGHT:\")\n    print(\"  This isn't a 'bug' - it's a difference in how the two clients\")\n    print(\"  emit streaming events for different model types.\")\n\n    # Show all unique event types for debugging\n    print(\"\\n\ud83d\udccb UNIQUE EVENT TYPES:\")\n    print(f\"  ChatBedrock: {set(e['event'] for e in events_old)}\")\n    print(f\"  ChatBedrockConverse: {set(e['event'] for e in events_new)}\")\n\n    # Test direct streaming to see if the model supports it\n    await test_direct_streaming()\n\n\nif __name__ == \"__main__\":\n    # Set up basic logging to see any warnings/errors\n    import logging\n    logging.basicConfig(level=logging.INFO)\n\n    # Run the test\n    asyncio.run(main())\n```\n\n### Error Message and Stack Trace (if applicable)\n\n```shell\n\n```\n\n### Description\n\nWhen using ChatBedrockConverse with OpenAI models in LangGraph's astream_events(), the expected on_chat_model_stream events are not emitted. Instead, complete messages are delivered via on_chain_stream events.\n\nExpected Behavior:\nBoth ChatBedrock and ChatBedrockConverse should emit on_chat_model_stream events for consistency\n\nActual Behavior:\nChatBedrock (Anthropic models): \u2705 Emits on_chat_model_stream events\nChatBedrockConverse (OpenAI models): \u274c Emits on_chain_stream events instead\nImpact: Applications relying on on_chat_model_stream events lose streaming functionality when migrating from Anthropic to OpenAI models.\n\n### System Info\n\nMy app is running a dockerfile with the minimum packages, and this is reproducible in this enviroment:\nSystem Information\n------------------\n> OS:  Windows\n> OS Version:  10.0.26100\n> Python Version:  3.13.3 (tags/v3.13.3:6280bb5, Apr  8 2025, 14:47:33) [MSC v.1943 64 bit (AMD64)]                                                             \n\nPackage Information\n-------------------\n> langchain_core: 0.3.76\n> langchain: 0.3.25\n> langsmith: 0.3.45\n> langchain_aws: 0.2.31\n> langchain_openai: 0.3.24\n> langchain_text_splitters: 0.3.8\n> langgraph_sdk: 0.1.69\n\nOptional packages not installed\n-------------------------------\n> langserve\n\nOther Dependencies\n------------------\n> async-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\n> beautifulsoup4: 4.13.4\n> bedrock-agentcore: Installed. No version info available.\n> boto3: 1.40.27\n> httpx: 0.28.1\n> jsonpatch<2.0,>=1.33: Installed. No version info available.\n> langchain-anthropic;: Installed. No version info available.\n> langchain-aws;: Installed. No version info available.\n> langchain-azure-ai;: Installed. No version info available.\n> langchain-cohere;: Installed. No version info available.\n> langchain-community;: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.51: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.58: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.65: Installed. No version info available.\n> langchain-deepseek;: Installed. No version info available.\n> langchain-fireworks;: Installed. No version info available.\n> langchain-google-genai;: Installed. No version info available.\n> langchain-google-vertexai;: Installed. No version info available.\n> langchain-groq;: Installed. No version info available.\n> langchain-huggingface;: Installed. No version info available.\n> langchain-mistralai;: Installed. No version info available.\n> langchain-ollama;: Installed. No version info available.\n> langchain-openai;: Installed. No version info available.\n> langchain-perplexity;: Installed. No version info available.\n> langchain-text-splitters<1.0.0,>=0.3.8: Installed. No version info available. \n> langchain-together;: Installed. No version info available.\n> langchain-xai;: Installed. No version info available.\n> langsmith-pyo3: Installed. No version info available.\n> langsmith<0.4,>=0.1.17: Installed. No version info available.\n> langsmith>=0.3.45: Installed. No version info available.\n> numpy: 2.3.2\n> openai-agents: Installed. No version info available.\n> openai<2.0.0,>=1.86.0: Installed. No version info available.\n> opentelemetry-api: 1.34.1\n> opentelemetry-exporter-otlp-proto-http: 1.34.1\n> opentelemetry-sdk: 1.34.1\n> orjson: 3.10.18\n> packaging: 24.2\n> packaging>=23.2: Installed. No version info available.\n> playwright: Installed. No version info available.\n> pydantic: 2.11.4\n> pydantic<3.0.0,>=2.7.4: Installed. No version info available.\n> pydantic>=2.7.4: Installed. No version info available.\n> pytest: 8.4.1\n> PyYAML>=5.3: Installed. No version info available.\n> requests: 2.32.3\n> requests-toolbelt: 1.0.0\n> requests<3,>=2: Installed. No version info available.\n> rich: 14.0.0\n> SQLAlchemy<3,>=1.4: Installed. No version info available.\n> tenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.        \n> tiktoken<1,>=0.7: Installed. No version info available.\n> typing-extensions>=4.7: Installed. No version info available.\n> zstandard: 0.23.0", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33059/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33059/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33000", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33000/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33000/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33000/events", "html_url": "https://github.com/langchain-ai/langchain/issues/33000", "id": 3429444681, "node_id": "I_kwDOIPDwls7MaSxJ", "number": 33000, "title": "AIMLAPI: support Function Calling", "user": {"login": "Muhammadzainattiq", "id": 142875994, "node_id": "U_kgDOCIQdWg", "avatar_url": "https://avatars.githubusercontent.com/u/142875994?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Muhammadzainattiq", "html_url": "https://github.com/Muhammadzainattiq", "followers_url": "https://api.github.com/users/Muhammadzainattiq/followers", "following_url": "https://api.github.com/users/Muhammadzainattiq/following{/other_user}", "gists_url": "https://api.github.com/users/Muhammadzainattiq/gists{/gist_id}", "starred_url": "https://api.github.com/users/Muhammadzainattiq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Muhammadzainattiq/subscriptions", "organizations_url": "https://api.github.com/users/Muhammadzainattiq/orgs", "repos_url": "https://api.github.com/users/Muhammadzainattiq/repos", "events_url": "https://api.github.com/users/Muhammadzainattiq/events{/privacy}", "received_events_url": "https://api.github.com/users/Muhammadzainattiq/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 9260554836, "node_id": "LA_kwDOIPDwls8AAAACJ_jaVA", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/feature%20request", "name": "feature request", "color": "058a42", "default": false, "description": "request for an enhancement / additional functionality"}], "state": "open", "locked": false, "assignee": {"login": "sagnik3788", "id": 116512372, "node_id": "U_kgDOBvHWdA", "avatar_url": "https://avatars.githubusercontent.com/u/116512372?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sagnik3788", "html_url": "https://github.com/sagnik3788", "followers_url": "https://api.github.com/users/sagnik3788/followers", "following_url": "https://api.github.com/users/sagnik3788/following{/other_user}", "gists_url": "https://api.github.com/users/sagnik3788/gists{/gist_id}", "starred_url": "https://api.github.com/users/sagnik3788/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sagnik3788/subscriptions", "organizations_url": "https://api.github.com/users/sagnik3788/orgs", "repos_url": "https://api.github.com/users/sagnik3788/repos", "events_url": "https://api.github.com/users/sagnik3788/events{/privacy}", "received_events_url": "https://api.github.com/users/sagnik3788/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "assignees": [{"login": "sagnik3788", "id": 116512372, "node_id": "U_kgDOBvHWdA", "avatar_url": "https://avatars.githubusercontent.com/u/116512372?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sagnik3788", "html_url": "https://github.com/sagnik3788", "followers_url": "https://api.github.com/users/sagnik3788/followers", "following_url": "https://api.github.com/users/sagnik3788/following{/other_user}", "gists_url": "https://api.github.com/users/sagnik3788/gists{/gist_id}", "starred_url": "https://api.github.com/users/sagnik3788/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sagnik3788/subscriptions", "organizations_url": "https://api.github.com/users/sagnik3788/orgs", "repos_url": "https://api.github.com/users/sagnik3788/repos", "events_url": "https://api.github.com/users/sagnik3788/events{/privacy}", "received_events_url": "https://api.github.com/users/sagnik3788/received_events", "type": "User", "user_view_type": "public", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2025-09-18T09:14:39Z", "updated_at": "2025-09-24T03:53:34Z", "closed_at": null, "author_association": "NONE", "type": {"id": 18879553, "node_id": "IT_kwDOB43M6c4BIBRB", "name": "Feature", "description": "A request, idea, or new functionality", "color": "blue", "created_at": "2024-02-16T01:43:34Z", "updated_at": "2024-10-08T21:10:19Z", "is_enabled": true}, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Checked other resources\n\n- [x] This is a feature request, not a bug report or usage question.\n- [x] I added a clear and descriptive title that summarizes the feature request.\n- [x] I used the GitHub search to find a similar feature request and didn't find it.\n- [x] I checked the LangChain documentation and API reference to see if this feature already exists.\n- [x] This is not related to the langchain-community package.\n\n### Feature Description\n\nThe AimlapiLLM connects several different llms by AIML API platform and most of those llm supports function calling. But the bind_tools method isn't defined for this class.\n\n### Use Case\n\nIt would enable to use function calling/ tool calling with ai ml api models\n\n### Proposed Solution\n\n_No response_\n\n### Alternatives Considered\n\n_No response_\n\n### Additional Context\n\n_No response_", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/33000/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/33000/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/32999", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32999/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32999/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32999/events", "html_url": "https://github.com/langchain-ai/langchain/issues/32999", "id": 3429373524, "node_id": "I_kwDOIPDwls7MaBZU", "number": 32999, "title": "Receive BadRequest 400 when invoking GPT-OSS-20B", "user": {"login": "LIN-Yu-Ting", "id": 62418760, "node_id": "MDQ6VXNlcjYyNDE4NzYw", "avatar_url": "https://avatars.githubusercontent.com/u/62418760?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LIN-Yu-Ting", "html_url": "https://github.com/LIN-Yu-Ting", "followers_url": "https://api.github.com/users/LIN-Yu-Ting/followers", "following_url": "https://api.github.com/users/LIN-Yu-Ting/following{/other_user}", "gists_url": "https://api.github.com/users/LIN-Yu-Ting/gists{/gist_id}", "starred_url": "https://api.github.com/users/LIN-Yu-Ting/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LIN-Yu-Ting/subscriptions", "organizations_url": "https://api.github.com/users/LIN-Yu-Ting/orgs", "repos_url": "https://api.github.com/users/LIN-Yu-Ting/repos", "events_url": "https://api.github.com/users/LIN-Yu-Ting/events{/privacy}", "received_events_url": "https://api.github.com/users/LIN-Yu-Ting/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 5680700839, "node_id": "LA_kwDOIPDwls8AAAABUpidpw", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/bug", "name": "bug", "color": "b60205", "default": true, "description": "Related to a bug, vulnerability, unexpected error with an existing feature"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2025-09-18T08:52:29Z", "updated_at": "2025-09-18T15:03:30Z", "closed_at": null, "author_association": "NONE", "type": {"id": 18879550, "node_id": "IT_kwDOB43M6c4BIBQ-", "name": "Bug", "description": "An unexpected problem or behavior", "color": "red", "created_at": "2024-02-16T01:43:33Z", "updated_at": "2024-07-26T15:00:33Z", "is_enabled": true}, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Checked other resources\n\n- [x] This is a bug, not a usage question.\n- [x] I added a clear and descriptive title that summarizes this issue.\n- [x] I used the GitHub search to find a similar question and didn't find it.\n- [x] I am sure that this is a bug in LangChain rather than my code.\n- [x] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n- [x] This is not related to the langchain-community package.\n- [x] I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n- [x] I posted a self-contained, minimal, reproducible example. A maintainer can copy it and run it AS IS.\n\n### Example Code\n\nWe have a local GPT-OSS-20B model run on vLLM. \n\nWe first load it in ChatOpenAI model.\n```python\nfrom langchain_openai import ChatOpenAI\nllm = ChatOpenAI(\n    model=\"openai/gpt-oss-20b\",\n    base_url=\"http://localhost/v1\",\n    api_key=\"EMPTY\",\n    temperature=0.0,\n    max_tokens=10000,\n    timeout=180,\n    max_retries=2,\n    use_responses_api=True\n)\n```\nNote: We are able to invoke this llm with simple HumanMessage.\n\nWe then define a very simple tool as following.\n\n```python\nfrom langchain_core.tools import tool\n \n@tool\ndef get_weather(location: str) -> str:\n    \"\"\"\n    Get the current weather for a location.\n    Args:\n        location: name of location that you want to check.\n    return:\n        return general weather and temperature\n    \"\"\"\n    if location.lower() == \"paris\":\n        return \"Sunny, 25\u00b0C\"\n    elif location.lower() == \"london\":\n        return \"Cloudy, 18\u00b0C\"\n    else:\n        return f\"No weather data for {location}\"\n```\n\nWe are able to obtain a tool_call response with\n```python\nfrom langchain_core.messages import HumanMessage\nllm_with_tools = llm.bind_tools([get_weather])\nmessages = [HumanMessage(content=\"How is the weather in Paris?\")]\nmessage = llm_with_tools.invoke(messages)\nmessage\n```\n\nwhere message looks like this:\n```\nAIMessage(content=[], additional_kwargs={'reasoning': {'id': 'rs_5390005f10d84a2ead227a8931a6ec15', 'summary': [], 'type': 'reasoning', 'content': [{'text': 'We need to use the get_weather function.', 'type': 'reasoning_text'}]}, '__openai_function_call_ids__': {'call_9b56ba57d2f54bfe9e974ddca1241199': 'ft_9b56ba57d2f54bfe9e974ddca1241199'}}, response_metadata={'id': 'resp_0eb4781a55174b179f6a3431fcbfaccb', 'created_at': 1758171845.0, 'model': 'openai/gpt-oss-20b', 'object': 'response', 'service_tier': 'auto', 'status': 'completed', 'model_name': 'openai/gpt-oss-20b'}, id='run--2593891c-28d2-4c90-84b2-0123e2e56f34-0', tool_calls=[{'name': 'get_weather', 'args': {'location': 'Paris'}, 'id': 'call_9b56ba57d2f54bfe9e974ddca1241199', 'type': 'tool_call'}], usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})\n```\n\nBy executing the tool, we obtain the ToolMessage\n```\nToolMessage(content='Sunny, 25\u00b0C', name='get_weather', id='3e6108e8-96f4-4c28-a17a-f85aefe51cbb', tool_call_id='call_9b56ba57d2f54bfe9e974ddca1241199')]}\n```\n\nIf we append the AIMessage and ToolMessage with the original messages and invoke another time the llm_with_tools. Unfortunately, we received the following exception.\n\n\n\n### Error Message and Stack Trace (if applicable)\n\n```\n---------------------------------------------------------------------------BadRequestError                           Traceback (most recent call last)\nCell In[12], line 2      \n          1 llm_with_tools = llm.bind_tools(tools=[get_weather], strict=True)\n----> 2 message = llm_with_tools.invoke(messages)\n          3 message\nFile /usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py:5710, in RunnableBindingBase.invoke(self, input, config, **kwargs)\n   5703 @override\n   5704 def invoke(\n   5705     self,\n   (...)\n   5708     **kwargs: Optional[Any],\n   5709 ) -> Output:\n-> 5710     return self.bound.invoke(\n   5711         input,\n   5712         self._merge_configs(config),\n   5713         **{**self.kwargs, **kwargs},\n   5714     )\nFile /usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py:395, in BaseChatModel.invoke(self, input, config, stop, **kwargs)\n    383 @override\n    384 def invoke(\n    385     self,\n   (...)\n    390     **kwargs: Any,\n    391 ) -> BaseMessage:\n    392     config = ensure_config(config)\n    393     return cast(\n    394         \"ChatGeneration\",\n--> 395         self.generate_prompt(\n    396             [self._convert_input(input)],\n    397             stop=stop,\n    398             callbacks=config.get(\"callbacks\"),\n    399             tags=config.get(\"tags\"),\n    400             metadata=config.get(\"metadata\"),\n    401             run_name=config.get(\"run_name\"),\n    402             run_id=config.pop(\"run_id\", None),\n    403             **kwargs,\n    404         ).generations[0][0],\n    405     ).message\nFile /usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py:1023, in BaseChatModel.generate_prompt(self, prompts, stop, callbacks, **kwargs)\n   1014 @override\n   1015 def generate_prompt(\n   1016     self,\n   (...)\n   1020     **kwargs: Any,\n   1021 ) -> LLMResult:\n   1022     prompt_messages = [p.to_messages() for p in prompts]\n-> 1023     return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\nFile /usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py:840, in BaseChatModel.generate(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\n    837 for i, m in enumerate(input_messages):\n    838     try:\n    839         results.append(\n--> 840             self._generate_with_cache(\n    841                 m,\n    842                 stop=stop,\n    843                 run_manager=run_managers[i] if run_managers else None,\n    844                 **kwargs,\n    845             )\n    846         )\n    847     except BaseException as e:\n    848         if run_managers:\nFile /usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py:1089, in BaseChatModel._generate_with_cache(self, messages, stop, run_manager, **kwargs)\n   1087     result = generate_from_stream(iter(chunks))\n   1088 elif inspect.signature(self._generate).parameters.get(\"run_manager\"):\n-> 1089     result = self._generate(\n   1090         messages, stop=stop, run_manager=run_manager, **kwargs\n   1091     )\n   1092 else:\n   1093     result = self._generate(messages, stop=stop, **kwargs)\nFile /usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py:1184, in BaseChatOpenAI._generate(self, messages, stop, run_manager, **kwargs)\n   1182     if raw_response is not None and hasattr(raw_response, \"http_response\"):\n   1183         e.response = raw_response.http_response  # type: ignore[attr-defined]\n-> 1184     raise e\n   1185 if (\n   1186     self.include_response_headers\n   1187     and raw_response is not None\n   1188     and hasattr(raw_response, \"headers\")\n   1189 ):\n   1190     generation_info = {\"headers\": dict(raw_response.headers)}\nFile /usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py:1166, in BaseChatOpenAI._generate(self, messages, stop, run_manager, **kwargs)\n   1162     raw_response = self.root_client.responses.with_raw_response.parse(\n   1163         **payload\n   1164     )\n   1165 else:-\n> 1166     raw_response = self.root_client.responses.with_raw_response.create(\n   1167         **payload\n   1168     )\n   1169 response = raw_response.parse()\n   1170 if self.include_response_headers:\nFile /usr/local/lib/python3.10/dist-packages/openai/_legacy_response.py:364, in to_raw_response_wrapper.<locals>.wrapped(*args, **kwargs)\n    360 extra_headers[RAW_RESPONSE_HEADER] = \"true\"\n    362 kwargs[\"extra_headers\"] = extra_headers\n--> 364 return cast(LegacyAPIResponse[R], func(*args, **kwargs))\nFile /usr/local/lib/python3.10/dist-packages/openai/resources/responses/responses.py:828, in Responses.create(self, background, conversation, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, prompt_cache_key, reasoning, safety_identifier, service_tier, store, stream, stream_options, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\n    791 def create(\n    792     self,\n    793     *,\n   (...)\n    826     timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN,\n    827 ) -> Response | Stream[ResponseStreamEvent]:\n--> 828     return self._post(\n    829         \"/responses\",\n    830         body=maybe_transform(\n    831             {\n    832                 \"background\": background,\n    833                 \"conversation\": conversation,\n    834                 \"include\": include,\n    835                 \"input\": input,\n    836                 \"instructions\": instructions,\n    837                 \"max_output_tokens\": max_output_tokens,\n    838                 \"max_tool_calls\": max_tool_calls,\n    839                 \"metadata\": metadata,\n    840                 \"model\": model,\n    841                 \"parallel_tool_calls\": parallel_tool_calls,\n    842                 \"previous_response_id\": previous_response_id,\n    843                 \"prompt\": prompt,\n    844                 \"prompt_cache_key\": prompt_cache_key,\n    845                 \"reasoning\": reasoning,\n    846                 \"safety_identifier\": safety_identifier,\n    847                 \"service_tier\": service_tier,\n    848                 \"store\": store,\n    849                 \"stream\": stream,\n    850                 \"stream_options\": stream_options,\n    851                 \"temperature\": temperature,\n    852                 \"text\": text,\n    853                 \"tool_choice\": tool_choice,\n    854                 \"tools\": tools,\n    855                 \"top_logprobs\": top_logprobs,\n    856                 \"top_p\": top_p,\n    857                 \"truncation\": truncation,\n    858                 \"user\": user,\n    859             },\n    860             response_create_params.ResponseCreateParamsStreaming\n    861             if stream\n    862             else response_create_params.ResponseCreateParamsNonStreaming,\n    863         ),\n    864         options=make_request_options(\n    865             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n    866         ),\n    867         cast_to=Response,\n    868         stream=stream or False,\n    869         stream_cls=Stream[ResponseStreamEvent],\n    870     )\nFile /usr/local/lib/python3.10/dist-packages/openai/_base_client.py:1259, in SyncAPIClient.post(self, path, cast_to, body, options, files, stream, stream_cls)\n   1245 def post(\n   1246     self,\n   1247     path: str,\n   (...)\n   1254     stream_cls: type[_StreamT] | None = None,\n   1255 ) -> ResponseT | _StreamT:\n   1256     opts = FinalRequestOptions.construct(\n   1257         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1258     )\n-> 1259     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\nFile /usr/local/lib/python3.10/dist-packages/openai/_base_client.py:1047, in SyncAPIClient.request(self, cast_to, options, stream, stream_cls)\n   1044             err.response.read()\n   1046         log.debug(\"Re-raising status error\")\n-> 1047         raise self._make_status_error_from_response(err.response) from None\n   1049     break\n   1051 assert response is not None, \"could not resolve response (should never happen)\"\nBadRequestError: Error code: 400 - {'object': 'error', 'message': \"object of type 'pydantic_core._pydantic_core.ValidatorIterator' has no len() None\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n ```\n\n### Description\n\nI would like to use local GPT-OSS-20B model as my llm for tool_calling. It does not generate correct AIMessage with tool_calls, if I do not specify `use_responses_api=True`. However, if I provide this argument then I will encounter the above exception.\n\nWe try to discuss this in https://github.com/langchain-ai/langchain/issues/32885. However, I use another approach and I believe this is not related to langgraph as I do not use any create_react_agent function here.\n\n### System Info\n\nSystem Information\n------------------\n> OS:  Linux\n> OS Version:  #1 SMP PREEMPT_DYNAMIC Wed May 1 15:46:25 EDT 2024\n> Python Version:  3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.76\n> langchain: 0.3.27\n> langchain_community: 0.3.25\n> langsmith: 0.3.45\n> langchain_chroma: 0.2.4\n> langchain_google_genai: 2.1.6\n> langchain_openai: 0.3.33\n> langchain_text_splitters: 0.3.11\n> langgraph_sdk: 0.1.74\n\nOptional packages not installed\n-------------------------------\n> langserve\n\nOther Dependencies\n------------------\n> aiohttp<4.0.0,>=3.8.3: Installed. No version info available.\n> async-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\n> chromadb>=1.0.9: Installed. No version info available.\n> dataclasses-json<0.7,>=0.5.7: Installed. No version info available.\n> filetype: 1.2.0\n> google-ai-generativelanguage: 0.6.18\n> httpx: 0.28.1\n> httpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\n> httpx>=0.25.2: Installed. No version info available.\n> jsonpatch<2.0,>=1.33: Installed. No version info available.\n> langchain-anthropic;: Installed. No version info available.\n> langchain-aws;: Installed. No version info available.\n> langchain-azure-ai;: Installed. No version info available.\n> langchain-cohere;: Installed. No version info available.\n> langchain-community;: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.65: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.72: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.76: Installed. No version info available.\n> langchain-core<2.0.0,>=0.3.75: Installed. No version info available.\n> langchain-core>=0.3.60: Installed. No version info available.\n> langchain-deepseek;: Installed. No version info available.\n> langchain-fireworks;: Installed. No version info available.\n> langchain-google-genai;: Installed. No version info available.\n> langchain-google-vertexai;: Installed. No version info available.\n> langchain-groq;: Installed. No version info available.\n> langchain-huggingface;: Installed. No version info available.\n> langchain-mistralai;: Installed. No version info available.\n> langchain-ollama;: Installed. No version info available.\n> langchain-openai;: Installed. No version info available.\n> langchain-perplexity;: Installed. No version info available.\n> langchain-text-splitters<1.0.0,>=0.3.9: Installed. No version info available.\n> langchain-together;: Installed. No version info available.\n> langchain-xai;: Installed. No version info available.\n> langchain<1.0.0,>=0.3.25: Installed. No version info available.\n> langsmith-pyo3: Installed. No version info available.\n> langsmith<0.4,>=0.1.125: Installed. No version info available.\n> langsmith>=0.1.17: Installed. No version info available.\n> langsmith>=0.3.45: Installed. No version info available.\n> numpy>=1.26.0;: Installed. No version info available.\n> numpy>=1.26.2;: Installed. No version info available.\n> numpy>=2.1.0;: Installed. No version info available.\n> openai-agents: Installed. No version info available.\n> openai<2.0.0,>=1.104.2: Installed. No version info available.\n> opentelemetry-api: 1.37.0\n> opentelemetry-exporter-otlp-proto-http: Installed. No version info available.\n> opentelemetry-sdk: 1.37.0\n> orjson: 3.10.18\n> orjson>=3.10.1: Installed. No version info available.\n> packaging: 25.0\n> packaging>=23.2: Installed. No version info available.\n> pydantic: 2.11.9\n> pydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4: Installed. No version info available.\n> pydantic>=2.7.4: Installed. No version info available.\n> pytest: Installed. No version info available.\n> PyYAML>=5.3: Installed. No version info available.\n> requests: 2.31.0\n> requests-toolbelt: 1.0.0\n> requests<3,>=2: Installed. No version info available.\n> rich: 14.1.0\n> SQLAlchemy<3,>=1.4: Installed. No version info available.\n> tenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\n> tenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\n> tiktoken<1,>=0.7: Installed. No version info available.\n> typing-extensions>=4.7: Installed. No version info available.\n> zstandard: 0.23.0", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/32999/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32999/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/32998", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32998/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32998/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32998/events", "html_url": "https://github.com/langchain-ai/langchain/issues/32998", "id": 3429085554, "node_id": "I_kwDOIPDwls7MY7Fy", "number": 32998, "title": "chat_ollama ResourceWarning: unclosed socket.socket", "user": {"login": "suckseed5", "id": 76991214, "node_id": "MDQ6VXNlcjc2OTkxMjE0", "avatar_url": "https://avatars.githubusercontent.com/u/76991214?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suckseed5", "html_url": "https://github.com/suckseed5", "followers_url": "https://api.github.com/users/suckseed5/followers", "following_url": "https://api.github.com/users/suckseed5/following{/other_user}", "gists_url": "https://api.github.com/users/suckseed5/gists{/gist_id}", "starred_url": "https://api.github.com/users/suckseed5/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suckseed5/subscriptions", "organizations_url": "https://api.github.com/users/suckseed5/orgs", "repos_url": "https://api.github.com/users/suckseed5/repos", "events_url": "https://api.github.com/users/suckseed5/events{/privacy}", "received_events_url": "https://api.github.com/users/suckseed5/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 5680700839, "node_id": "LA_kwDOIPDwls8AAAABUpidpw", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/bug", "name": "bug", "color": "b60205", "default": true, "description": "Related to a bug, vulnerability, unexpected error with an existing feature"}, {"id": 6718231201, "node_id": "LA_kwDOIPDwls8AAAABkHASoQ", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/unable-to-reproduce", "name": "unable-to-reproduce", "color": "ea4c8e", "default": false, "description": "LangChain's maintainers are not able to reproduce this issue and consequently cannot work on it"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2025-09-18T07:38:59Z", "updated_at": "2025-09-18T13:59:03Z", "closed_at": null, "author_association": "NONE", "type": {"id": 18879550, "node_id": "IT_kwDOB43M6c4BIBQ-", "name": "Bug", "description": "An unexpected problem or behavior", "color": "red", "created_at": "2024-02-16T01:43:33Z", "updated_at": "2024-07-26T15:00:33Z", "is_enabled": true}, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Checked other resources\n\n- [x] This is a bug, not a usage question.\n- [x] I added a clear and descriptive title that summarizes this issue.\n- [x] I used the GitHub search to find a similar question and didn't find it.\n- [x] I am sure that this is a bug in LangChain rather than my code.\n- [x] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n- [x] This is not related to the langchain-community package.\n- [x] I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n- [x] I posted a self-contained, minimal, reproducible example. A maintainer can copy it and run it AS IS.\n\n### Example Code\n\n```\nfrom langchain_ollama import ChatOllama\n\nllm = ChatOllama(\n    model=\"xxx\",\n    temperature=0,\n    base_url=\"xxx\"\n    # other params...\n)\n\nmessages = [\n    (\n        \"system\",\n        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n    ),\n    (\"human\", \"I love programming.\"),\n]\nai_msg = llm.invoke(messages)\n\nprint(ai_msg.content)\n```\n\n \n\n### Error Message and Stack Trace (if applicable)\n\nResourceWarning: unclosed <socket.socket fd=19, family=2, type=1, proto=6, laddr=('127.0.0.1', 41080), raddr=('127.0.0.1', 11434)> _chain_future(future, new_future)\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n\n### Description\n\nIs the session not closed?\n\n### System Info\n\nSystem Information\n------------------\n> OS:  Linux\n> OS Version:  #115~20.04.1-Ubuntu SMP Mon Apr 15 17:33:04 UTC 2024\n> Python Version:  3.11.0 (main, Mar  1 2023, 18:26:19) [GCC 11.2.0]", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/32998/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32998/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/32988", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32988/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32988/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32988/events", "html_url": "https://github.com/langchain-ai/langchain/issues/32988", "id": 3424587928, "node_id": "I_kwDOIPDwls7MHxCY", "number": 32988, "title": "Human in loop middleware's edit argument passing issue", "user": {"login": "MissLostCodes", "id": 137595846, "node_id": "U_kgDOCDOLxg", "avatar_url": "https://avatars.githubusercontent.com/u/137595846?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MissLostCodes", "html_url": "https://github.com/MissLostCodes", "followers_url": "https://api.github.com/users/MissLostCodes/followers", "following_url": "https://api.github.com/users/MissLostCodes/following{/other_user}", "gists_url": "https://api.github.com/users/MissLostCodes/gists{/gist_id}", "starred_url": "https://api.github.com/users/MissLostCodes/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MissLostCodes/subscriptions", "organizations_url": "https://api.github.com/users/MissLostCodes/orgs", "repos_url": "https://api.github.com/users/MissLostCodes/repos", "events_url": "https://api.github.com/users/MissLostCodes/events{/privacy}", "received_events_url": "https://api.github.com/users/MissLostCodes/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 5680700839, "node_id": "LA_kwDOIPDwls8AAAABUpidpw", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/bug", "name": "bug", "color": "b60205", "default": true, "description": "Related to a bug, vulnerability, unexpected error with an existing feature"}], "state": "open", "locked": false, "assignee": {"login": "sydney-runkle", "id": 54324534, "node_id": "MDQ6VXNlcjU0MzI0NTM0", "avatar_url": "https://avatars.githubusercontent.com/u/54324534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sydney-runkle", "html_url": "https://github.com/sydney-runkle", "followers_url": "https://api.github.com/users/sydney-runkle/followers", "following_url": "https://api.github.com/users/sydney-runkle/following{/other_user}", "gists_url": "https://api.github.com/users/sydney-runkle/gists{/gist_id}", "starred_url": "https://api.github.com/users/sydney-runkle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sydney-runkle/subscriptions", "organizations_url": "https://api.github.com/users/sydney-runkle/orgs", "repos_url": "https://api.github.com/users/sydney-runkle/repos", "events_url": "https://api.github.com/users/sydney-runkle/events{/privacy}", "received_events_url": "https://api.github.com/users/sydney-runkle/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "assignees": [{"login": "sydney-runkle", "id": 54324534, "node_id": "MDQ6VXNlcjU0MzI0NTM0", "avatar_url": "https://avatars.githubusercontent.com/u/54324534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sydney-runkle", "html_url": "https://github.com/sydney-runkle", "followers_url": "https://api.github.com/users/sydney-runkle/followers", "following_url": "https://api.github.com/users/sydney-runkle/following{/other_user}", "gists_url": "https://api.github.com/users/sydney-runkle/gists{/gist_id}", "starred_url": "https://api.github.com/users/sydney-runkle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sydney-runkle/subscriptions", "organizations_url": "https://api.github.com/users/sydney-runkle/orgs", "repos_url": "https://api.github.com/users/sydney-runkle/repos", "events_url": "https://api.github.com/users/sydney-runkle/events{/privacy}", "received_events_url": "https://api.github.com/users/sydney-runkle/received_events", "type": "User", "user_view_type": "public", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2025-09-17T04:13:13Z", "updated_at": "2025-09-22T03:47:49Z", "closed_at": null, "author_association": "NONE", "type": {"id": 18879550, "node_id": "IT_kwDOB43M6c4BIBQ-", "name": "Bug", "description": "An unexpected problem or behavior", "color": "red", "created_at": "2024-02-16T01:43:33Z", "updated_at": "2024-07-26T15:00:33Z", "is_enabled": true}, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Checked other resources\n\n- [x] This is a bug, not a usage question.\n- [x] I added a clear and descriptive title that summarizes this issue.\n- [x] I used the GitHub search to find a similar question and didn't find it.\n- [x] I am sure that this is a bug in LangChain rather than my code.\n- [x] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n- [x] This is not related to the langchain-community package.\n- [x] I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n- [x] I posted a self-contained, minimal, reproducible example. A maintainer can copy it and run it AS IS.\n\n### Example Code\n\n\n#### Here's the code : \n``` \npython\ndef wikipedia_lookup(query: str) -> str:\n    \"\"\"Lookup information on Wikipedia.\"\"\"\n    print(\"=============================================================wiki============================================/n\")\n    # Use Wikipedia API for lookup\n    url = \"https://en.wikipedia.org/api/rest_v1/page/summary/\" + query\n    response = requests.get(url)\n    if response.status_code == 200:\n        data = response.json()\n        return data.get(\"extract\", \"No results found\")\n    else:\n        return \"No results found\"\n\n\n\n# Setup agent\nagent = create_agent(\n    \"openai:gpt-4o-mini\",\n    tools=[ wikipedia_lookup ],\n    middleware=[\n        HumanInTheLoopMiddleware(\n            tool_configs={\n                 \"wikipedia_lookup\": {\n                    \"require_approval\": True,\n                    \"description\": \"Wikipedia lookup requires approval\",\n                },\n\n\n            },\n            message_prefix=\"Tool execution pending approval\",\n        ),\n    ],\n    checkpointer=InMemorySaver(),  # Required for interrupts\n)\n# Create a travel itinerary for a trip to Paris\nconfig = {\"configurable\": {\"thread_id\": \"1\", \"recursion_limit\": 100}}  # Configuration for the agent\ninitial_message = HumanMessage(\"Use web search tool and create a travel itinerary for a trip to Paris in 30 words .\")\n\n# Step 1: First invoke (this will pause for approval if tool requires it)\nagent.invoke({\"messages\": [initial_message]}, config)\nstate = agent.get_state(config)\nif state.next:\n        request = state.tasks[0].interrupts[0].value[0][\"action_request\"]\n        print(f\"request--------------{request}\")\n        print(\"action:\", request[\"action\"])\n        print(\"args:\", request[\"args\"])\n\n        # Display the original suggestion\n        print(\"Original suggestion:\", request[\"args\"])\n\n        # Prompt for human input\n        approval_decision = input(\"Enter approval decision (accept/edit/ignore/response): \")\n\n        if approval_decision == \"accept\":\n          result = agent.invoke(Command(resume=[{\"type\": \"accept\"}]), config=config)\n\n        elif approval_decision == \"edit\":\n            new_query = input(\"Enter modified query: \")\n            nq= res=\"'+ new_query +'\"\n            result = agent.invoke(\n                Command(\n                    resume=[\n                        {\n                            \"type\": \"edit\",\n                            \"args\": {\n                                'action': 'wikipedia_lookup',  # tool name required\n                                'args': {'query': nq}  # wrap inside 'modified\n\n\n                        }}\n                    ]\n                ),\n                config=config,\n            )\n\n\n        elif approval_decision == \"ignore\":\n            result=agent.invoke(Command(resume=[{\"type\": \"ignore\"}]), config=config)\n\n        elif approval_decision == \"response\":\n            manual_response = input(\"Enter manual response: \")\n            res=\"'+ manual_response +'\"\n            agent.invoke(Command(resume=[{\"type\": \"response\", \"args\": res}]), config=config)\n\n        else:\n            print(\"Invalid decision. Please try again.\")\n\n\nprint(\"Final Itinerary:\\n\", result[\"messages\"][-1].content)\n```\n\n### **Output**: \n``` \nrequest--------------{'action': 'wikipedia_lookup', 'args': {'query': 'Paris travel itinerary'}}\naction: wikipedia_lookup\nargs: {'query': 'Paris travel itinerary'}\nOriginal suggestion: {'query': 'Paris travel itinerary'}\nEnter approval decision (accept/edit/ignore/response): edit\nEnter modified query: travel guide for kashmir\n=============================================================wiki============================================/n\nFinal Itinerary:\n I don't have web search capabilities at the moment, but I can help create a travel itinerary for Paris based on general knowledge:\n\n\"Day 1: Eiffel Tower, Seine River cruise; Day 2: Louvre Museum, Montmartre; Day 3: Notre Dame, Latin Quarter, shopping on Champs-\u00c9lys\u00e9es.\" \n``` \n\n### **The documentation says : **  edit: Modify arguments before execution - { type: \"edit\", args: { action: \"tool_name\", args: { modified: \"args\" } } }\n\n\n\n### Error Message and Stack Trace (if applicable)\n\n_No response_\n\n### Description\n\n _I\u2019m not sure if this is a bug or if I\u2019m using LangChain v1 incorrectly, but here\u2019s a minimal reproducible example and issue ._\n\n_I was trying to cretae a travel plan generator using langchain create_agent using HumanInTheLoopMiddleware - accept , ignore worked fine but in edit i modified args while passing in resume list but the agent was not resuming the process using new args . I followed the documentation [Langchain human in loop docs](https://docs.langchain.com/oss/python/langchain/middleware#human-in-the-loop) but i dont know if i passing args properly or not ._\n\n_The edit args still don't work as expected when i completely follow the request json structure for passing modified args ._\n\n### System Info\n\n_I was actually using google collab to run this code_ \n\n```C:\\Users\\Shagun Gupta>python -m langchain_core.sys_info```\n\nSystem Information\n------------------\n> OS:  Windows\n> OS Version:  10.0.26100\n> Python Version:  3.13.5 (tags/v3.13.5:6cb20a2, Jun 11 2025, 16:15:46) [MSC v.1943 64 bit (AMD64)]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.74\n> langchain: 0.3.27\n> langchain_community: 0.3.27\n> langsmith: 0.4.14\n> langchain_google_genai: 2.1.9\n> langchain_openai: 0.3.30\n> langchain_text_splitters: 0.3.9\n\nOptional packages not installed\n-------------------------------\n> langserve\n\nOther Dependencies\n------------------\n> aiohttp<4.0.0,>=3.8.3: Installed. No version info available.\n> async-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\n> dataclasses-json<0.7,>=0.5.7: Installed. No version info available.\n> filetype: 1.2.0\n> google-ai-generativelanguage: 0.6.18\n> httpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\n> httpx<1,>=0.23.0: Installed. No version info available.\n> jsonpatch<2.0,>=1.33: Installed. No version info available.\n> langchain-anthropic;: Installed. No version info available.\n> langchain-aws;: Installed. No version info available.\n> langchain-azure-ai;: Installed. No version info available.\n> langchain-cohere;: Installed. No version info available.\n> langchain-community;: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.66: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.72: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.74: Installed. No version info available.\n> langchain-deepseek;: Installed. No version info available.\n> langchain-fireworks;: Installed. No version info available.\n> langchain-google-genai;: Installed. No version info available.\n> langchain-google-vertexai;: Installed. No version info available.\n> langchain-groq;: Installed. No version info available.\n> langchain-huggingface;: Installed. No version info available.\n> langchain-mistralai;: Installed. No version info available.\n> langchain-ollama;: Installed. No version info available.\n> langchain-openai;: Installed. No version info available.\n> langchain-perplexity;: Installed. No version info available.\n> langchain-text-splitters<1.0.0,>=0.3.9: Installed. No version info available.\n> langchain-together;: Installed. No version info available.\n> langchain-xai;: Installed. No version info available.\n> langchain<1.0.0,>=0.3.26: Installed. No version info available.\n> langsmith-pyo3>=0.1.0rc2;: Installed. No version info available.\n> langsmith>=0.1.125: Installed. No version info available.\n> langsmith>=0.1.17: Installed. No version info available.\n> langsmith>=0.3.45: Installed. No version info available.\n> numpy>=1.26.2;: Installed. No version info available.\n> numpy>=2.1.0;: Installed. No version info available.\n> openai-agents>=0.0.3;: Installed. No version info available.\n> openai<2.0.0,>=1.99.9: Installed. No version info available.\n> opentelemetry-api>=1.30.0;: Installed. No version info available.\n> opentelemetry-exporter-otlp-proto-http>=1.30.0;: Installed. No version info available.\n> opentelemetry-sdk>=1.30.0;: Installed. No version info available.\n> orjson>=3.9.14;: Installed. No version info available.\n> packaging>=23.2: Installed. No version info available.\n> pydantic: 2.11.7\n> pydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\n> pydantic<3,>=1: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4: Installed. No version info available.\n> pydantic>=2.7.4: Installed. No version info available.\n> pytest>=7.0.0;: Installed. No version info available.\n> PyYAML>=5.3: Installed. No version info available.\n> requests-toolbelt>=1.0.0: Installed. No version info available.\n> requests<3,>=2: Installed. No version info available.\n> requests>=2.0.0: Installed. No version info available.\n> rich>=13.9.4;: Installed. No version info available.\n> SQLAlchemy<3,>=1.4: Installed. No version info available.\n> tenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\n> tenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\n> tiktoken<1,>=0.7: Installed. No version info available.\n> typing-extensions>=4.7: Installed. No version info available.\n> vcrpy>=7.0.0;: Installed. No version info available.\n> zstandard>=0.23.0: Installed. No version info available.", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/32988/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32988/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/32984", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32984/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32984/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32984/events", "html_url": "https://github.com/langchain-ai/langchain/issues/32984", "id": 3424227625, "node_id": "I_kwDOIPDwls7MGZEp", "number": 32984, "title": "Feature Request: Add native OpenSearch vector database support for vector storage and retrieval", "user": {"login": "mark-qin-derbysoft", "id": 154867909, "node_id": "U_kgDOCTsYxQ", "avatar_url": "https://avatars.githubusercontent.com/u/154867909?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mark-qin-derbysoft", "html_url": "https://github.com/mark-qin-derbysoft", "followers_url": "https://api.github.com/users/mark-qin-derbysoft/followers", "following_url": "https://api.github.com/users/mark-qin-derbysoft/following{/other_user}", "gists_url": "https://api.github.com/users/mark-qin-derbysoft/gists{/gist_id}", "starred_url": "https://api.github.com/users/mark-qin-derbysoft/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mark-qin-derbysoft/subscriptions", "organizations_url": "https://api.github.com/users/mark-qin-derbysoft/orgs", "repos_url": "https://api.github.com/users/mark-qin-derbysoft/repos", "events_url": "https://api.github.com/users/mark-qin-derbysoft/events{/privacy}", "received_events_url": "https://api.github.com/users/mark-qin-derbysoft/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 9260554836, "node_id": "LA_kwDOIPDwls8AAAACJ_jaVA", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/feature%20request", "name": "feature request", "color": "058a42", "default": false, "description": "request for an enhancement / additional functionality"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2025-09-17T01:30:35Z", "updated_at": "2025-09-17T01:31:35Z", "closed_at": "2025-09-17T01:31:35Z", "author_association": "NONE", "type": {"id": 18879553, "node_id": "IT_kwDOB43M6c4BIBRB", "name": "Feature", "description": "A request, idea, or new functionality", "color": "blue", "created_at": "2024-02-16T01:43:34Z", "updated_at": "2024-10-08T21:10:19Z", "is_enabled": true}, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Checked other resources\n\n- [x] This is a feature request, not a bug report or usage question.\n- [x] I added a clear and descriptive title that summarizes the feature request.\n- [x] I used the GitHub search to find a similar feature request and didn't find it.\n- [x] I checked the LangChain documentation and API reference to see if this feature already exists.\n- [x] This is not related to the langchain-community package.\n\n### Feature Description\n\n**Summary**\nLangChain currently supports various vector databases like Pinecone, Chroma, FAISS, and Weaviate, but lacks native support for OpenSearch (formerly Elasticsearch OSS). OpenSearch is a widely-used, open-source search and analytics engine with robust vector search capabilities that would benefit the LangChain community.\n\n**Motivation**\nEnterprise Adoption: Many organizations already use OpenSearch for logging, monitoring, and search, making it a natural choice for vector storage\nCost-Effective: Open-source alternative to proprietary vector databases\nScalability: Proven ability to handle large-scale deployments\nFeature Rich: Supports hybrid search (combining vector and keyword search), filtering, and advanced analytics\nSelf-Hosted: Allows organizations to maintain data sovereignty and control\n\n**Current Workaround**\nUsers currently need to implement custom vector store classes or use generic Elasticsearch integrations, which don\u2019t fully leverage OpenSearch\u2019s vector capabilities.\n\n\n\n### Use Case\n\nKey Features to Support\nBasic Vector Operations:\n\nDocument ingestion with automatic embedding\nVector similarity search (cosine, euclidean, dot product)\nMetadata filtering\nBatch operations\nAdvanced Features:\n\nHybrid search (vector + keyword)\nMaximum Marginal Relevance (MMR) search\nCustom scoring functions\nIndex management and optimization\nOpenSearch Specific:\n\nSupport for different vector engines (nmslib, faiss, lucene)\nIndex templates and mappings\nCluster management\nSecurity features (authentication, encryption)\n\n### Proposed Solution\n\n_No response_\n\n### Alternatives Considered\n\n_No response_\n\n### Additional Context\n\n_No response_", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/32984/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32984/timeline", "performed_via_github_app": null, "state_reason": "completed", "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/32981", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32981/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32981/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32981/events", "html_url": "https://github.com/langchain-ai/langchain/issues/32981", "id": 3423741450, "node_id": "I_kwDOIPDwls7MEiYK", "number": 32981, "title": "Reasoning tokens not passing through from OpenRouter to `ChatOpenAI`", "user": {"login": "sinanuozdemir", "id": 3695746, "node_id": "MDQ6VXNlcjM2OTU3NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3695746?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sinanuozdemir", "html_url": "https://github.com/sinanuozdemir", "followers_url": "https://api.github.com/users/sinanuozdemir/followers", "following_url": "https://api.github.com/users/sinanuozdemir/following{/other_user}", "gists_url": "https://api.github.com/users/sinanuozdemir/gists{/gist_id}", "starred_url": "https://api.github.com/users/sinanuozdemir/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sinanuozdemir/subscriptions", "organizations_url": "https://api.github.com/users/sinanuozdemir/orgs", "repos_url": "https://api.github.com/users/sinanuozdemir/repos", "events_url": "https://api.github.com/users/sinanuozdemir/events{/privacy}", "received_events_url": "https://api.github.com/users/sinanuozdemir/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 5680700839, "node_id": "LA_kwDOIPDwls8AAAABUpidpw", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/bug", "name": "bug", "color": "b60205", "default": true, "description": "Related to a bug, vulnerability, unexpected error with an existing feature"}, {"id": 6411661606, "node_id": "LA_kwDOIPDwls8AAAABfioxJg", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/investigate", "name": "investigate", "color": "0e8a16", "default": false, "description": "Flagged for investigation"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2025-09-16T21:02:30Z", "updated_at": "2025-09-21T23:26:51Z", "closed_at": null, "author_association": "NONE", "type": {"id": 18879550, "node_id": "IT_kwDOB43M6c4BIBQ-", "name": "Bug", "description": "An unexpected problem or behavior", "color": "red", "created_at": "2024-02-16T01:43:33Z", "updated_at": "2024-07-26T15:00:33Z", "is_enabled": true}, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Checked other resources\n\n- [x] This is a bug, not a usage question.\n- [x] I added a clear and descriptive title that summarizes this issue.\n- [x] I used the GitHub search to find a similar question and didn't find it.\n- [x] I am sure that this is a bug in LangChain rather than my code.\n- [x] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n- [x] This is not related to the langchain-community package.\n- [x] I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n- [x] I posted a self-contained, minimal, reproducible example. A maintainer can copy it and run it AS IS.\n\n### Example Code\n\nRun this code and inspect the response and see that it has no reasoning tokens even though anthropic returns them.\n\n```python\nimport os\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import HumanMessage\n\n\nchat = ChatOpenAI(\n    model=\"anthropic/claude-sonnet-4\",\n    api_key=os.environ.get(\"OPENROUTER_API_KEY\"),\n    openai_api_base=\"https://openrouter.ai/api/v1\"\n)\n\nmessage = HumanMessage(content=\"Hello! What's the capital of France now and what was it in 1937?\")\n\n\nresponse = chat.invoke(\n    [message],\n    extra_body={'reasoning': {'max_tokens': 10}}\n    )\nprint(f\"\ud83d\udce5 Response: {response.content}\")\n```\n\n### Error Message and Stack Trace (if applicable)\n\n_No response_\n\n### Description\n\nI'm using Anthropic Sonnet 4 through OpenRouter on Langchain (for agentic work) and noticed that the reasoning tokens aren't getting passed from OpenRouter to Langchain\n\n### System Info\n\nObtaining file:///Users/sinanozdemir/Fun/langchain/libs/core\n  Installing build dependencies ...   Checking if build backend supports build_editable ...   Getting requirements to build editable ...   Preparing editable metadata (pyproject.toml) ... Requirement already satisfied: langsmith>=0.3.45 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain-core==0.3.76) (0.4.28)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain-core==0.3.76) (9.1.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain-core==0.3.76) (1.33)\nRequirement already satisfied: PyYAML>=5.3 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain-core==0.3.76) (6.0.2)\nRequirement already satisfied: typing-extensions>=4.7 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain-core==0.3.76) (4.15.0)\nRequirement already satisfied: packaging>=23.2 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain-core==0.3.76) (25.0)\nRequirement already satisfied: pydantic>=2.7.4 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain-core==0.3.76) (2.11.9)\nRequirement already satisfied: jsonpointer>=1.9 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from jsonpatch<2.0,>=1.33->langchain-core==0.3.76) (3.0.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langsmith>=0.3.45->langchain-core==0.3.76) (0.28.1)\nRequirement already satisfied: orjson>=3.9.14 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langsmith>=0.3.45->langchain-core==0.3.76) (3.11.3)\nRequirement already satisfied: requests-toolbelt>=1.0.0 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langsmith>=0.3.45->langchain-core==0.3.76) (1.0.0)\nRequirement already satisfied: requests>=2.0.0 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langsmith>=0.3.45->langchain-core==0.3.76) (2.32.5)\nRequirement already satisfied: zstandard>=0.23.0 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langsmith>=0.3.45->langchain-core==0.3.76) (0.25.0)\nRequirement already satisfied: anyio in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core==0.3.76) (4.10.0)\nRequirement already satisfied: certifi in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core==0.3.76) (2025.8.3)\nRequirement already satisfied: httpcore==1.* in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core==0.3.76) (1.0.9)\nRequirement already satisfied: idna in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core==0.3.76) (3.10)\nRequirement already satisfied: h11>=0.16 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core==0.3.76) (0.16.0)\nRequirement already satisfied: annotated-types>=0.6.0 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from pydantic>=2.7.4->langchain-core==0.3.76) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from pydantic>=2.7.4->langchain-core==0.3.76) (2.33.2)\n...\n    Found existing installation: langchain-core 0.3.76\n    Uninstalling langchain-core-0.3.76:\n      Successfully uninstalled langchain-core-0.3.76\nSuccessfully installed langchain-core-0.3.76\nOutput is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?fda711b8-29a4-4c79-a7f3-5f671eb2dc9d) or open in a [text editor](command:workbench.action.openLargeOutput?fda711b8-29a4-4c79-a7f3-5f671eb2dc9d). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)...\nObtaining file:///Users/sinanozdemir/Fun/langchain/libs/langchain\n  Installing build dependencies ...   Checking if build backend supports build_editable ...   Getting requirements to build editable ...   Preparing editable metadata (pyproject.toml) ... Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain==0.3.27) (0.3.76)\nRequirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain==0.3.27) (0.3.11)\nRequirement already satisfied: langsmith>=0.1.17 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain==0.3.27) (0.4.28)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain==0.3.27) (2.11.9)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain==0.3.27) (2.0.43)\nRequirement already satisfied: requests<3,>=2 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain==0.3.27) (2.32.5)\nRequirement already satisfied: PyYAML>=5.3 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain==0.3.27) (6.0.2)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (9.1.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (1.33)\nRequirement already satisfied: typing-extensions>=4.7 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (4.15.0)\nRequirement already satisfied: packaging>=23.2 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (25.0)\nRequirement already satisfied: jsonpointer>=1.9 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (3.0.0)\nRequirement already satisfied: annotated-types>=0.6.0 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from pydantic<3.0.0,>=2.7.4->langchain==0.3.27) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from pydantic<3.0.0,>=2.7.4->langchain==0.3.27) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from pydantic<3.0.0,>=2.7.4->langchain==0.3.27) (0.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from requests<3,>=2->langchain==0.3.27) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from requests<3,>=2->langchain==0.3.27) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from requests<3,>=2->langchain==0.3.27) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from requests<3,>=2->langchain==0.3.27) (2025.8.3)\nRequirement already satisfied: httpx<1,>=0.23.0 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langsmith>=0.1.17->langchain==0.3.27) (0.28.1)\n...\n    Found existing installation: langchain 0.3.27\n    Uninstalling langchain-0.3.27:\n      Successfully uninstalled langchain-0.3.27\nSuccessfully installed langchain-0.3.27\nOutput is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?04ac20a4-60fb-4361-864f-003adc808b13) or open in a [text editor](command:workbench.action.openLargeOutput?04ac20a4-60fb-4361-864f-003adc808b13). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)...\nObtaining file:///Users/sinanozdemir/Fun/langchain/libs/text-splitters\n  Installing build dependencies ...   Checking if build backend supports build_editable ...   Getting requirements to build editable ...   Preparing editable metadata (pyproject.toml) ... Requirement already satisfied: langchain-core<2.0.0,>=0.3.75 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain-text-splitters==0.3.11) (0.3.76)\nRequirement already satisfied: langsmith>=0.3.45 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain-core<2.0.0,>=0.3.75->langchain-text-splitters==0.3.11) (0.4.28)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain-core<2.0.0,>=0.3.75->langchain-text-splitters==0.3.11) (9.1.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain-core<2.0.0,>=0.3.75->langchain-text-splitters==0.3.11) (1.33)\nRequirement already satisfied: PyYAML>=5.3 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain-core<2.0.0,>=0.3.75->langchain-text-splitters==0.3.11) (6.0.2)\nRequirement already satisfied: typing-extensions>=4.7 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain-core<2.0.0,>=0.3.75->langchain-text-splitters==0.3.11) (4.15.0)\nRequirement already satisfied: packaging>=23.2 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain-core<2.0.0,>=0.3.75->langchain-text-splitters==0.3.11) (25.0)\nRequirement already satisfied: pydantic>=2.7.4 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain-core<2.0.0,>=0.3.75->langchain-text-splitters==0.3.11) (2.11.9)\nRequirement already satisfied: jsonpointer>=1.9 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from jsonpatch<2.0,>=1.33->langchain-core<2.0.0,>=0.3.75->langchain-text-splitters==0.3.11) (3.0.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain-text-splitters==0.3.11) (0.28.1)\nRequirement already satisfied: orjson>=3.9.14 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain-text-splitters==0.3.11) (3.11.3)\nRequirement already satisfied: requests-toolbelt>=1.0.0 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain-text-splitters==0.3.11) (1.0.0)\nRequirement already satisfied: requests>=2.0.0 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain-text-splitters==0.3.11) (2.32.5)\nRequirement already satisfied: zstandard>=0.23.0 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain-text-splitters==0.3.11) (0.25.0)\nRequirement already satisfied: anyio in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain-text-splitters==0.3.11) (4.10.0)\nRequirement already satisfied: certifi in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain-text-splitters==0.3.11) (2025.8.3)\nRequirement already satisfied: httpcore==1.* in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain-text-splitters==0.3.11) (1.0.9)\nRequirement already satisfied: idna in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain-text-splitters==0.3.11) (3.10)\nRequirement already satisfied: h11>=0.16 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain-text-splitters==0.3.11) (0.16.0)\nRequirement already satisfied: annotated-types>=0.6.0 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from pydantic>=2.7.4->langchain-core<2.0.0,>=0.3.75->langchain-text-splitters==0.3.11) (0.7.0)\n...\n    Found existing installation: langchain-text-splitters 0.3.11\n    Uninstalling langchain-text-splitters-0.3.11:\n      Successfully uninstalled langchain-text-splitters-0.3.11\nSuccessfully installed langchain-text-splitters-0.3.11\nOutput is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?43a7f737-8c31-415f-9c3c-0b2f579d5a10) or open in a [text editor](command:workbench.action.openLargeOutput?43a7f737-8c31-415f-9c3c-0b2f579d5a10). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)...\n\u2705 Successfully imported LangChain packages!\nlangchain_core version: 0.3.76\nlangchain version: 0.3.27\nCreating HumanMessage with content: Hello, LangChain!\n\u2705 Created test message: content='Hello, LangChain!' additional_kwargs={} response_metadata={}\n\n\ud83c\udf89 LangChain development environment is working correctly!\nObtaining file:///Users/sinanozdemir/Fun/langchain/libs/partners/openai\n  Installing build dependencies ...   Checking if build backend supports build_editable ...   Getting requirements to build editable ...   Preparing editable metadata (pyproject.toml) ... Requirement already satisfied: langchain-core<1.0.0,>=0.3.76 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain-openai==0.3.33) (0.3.76)\nRequirement already satisfied: openai<2.0.0,>=1.104.2 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain-openai==0.3.33) (1.107.3)\nRequirement already satisfied: tiktoken<1,>=0.7 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain-openai==0.3.33) (0.11.0)\nRequirement already satisfied: langsmith>=0.3.45 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain-core<1.0.0,>=0.3.76->langchain-openai==0.3.33) (0.4.28)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain-core<1.0.0,>=0.3.76->langchain-openai==0.3.33) (9.1.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain-core<1.0.0,>=0.3.76->langchain-openai==0.3.33) (1.33)\nRequirement already satisfied: PyYAML>=5.3 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain-core<1.0.0,>=0.3.76->langchain-openai==0.3.33) (6.0.2)\nRequirement already satisfied: typing-extensions>=4.7 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain-core<1.0.0,>=0.3.76->langchain-openai==0.3.33) (4.15.0)\nRequirement already satisfied: packaging>=23.2 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain-core<1.0.0,>=0.3.76->langchain-openai==0.3.33) (25.0)\nRequirement already satisfied: pydantic>=2.7.4 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from langchain-core<1.0.0,>=0.3.76->langchain-openai==0.3.33) (2.11.9)\nRequirement already satisfied: jsonpointer>=1.9 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.76->langchain-openai==0.3.33) (3.0.0)\nRequirement already satisfied: anyio<5,>=3.5.0 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from openai<2.0.0,>=1.104.2->langchain-openai==0.3.33) (4.10.0)\nRequirement already satisfied: distro<2,>=1.7.0 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from openai<2.0.0,>=1.104.2->langchain-openai==0.3.33) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from openai<2.0.0,>=1.104.2->langchain-openai==0.3.33) (0.28.1)\nRequirement already satisfied: jiter<1,>=0.4.0 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from openai<2.0.0,>=1.104.2->langchain-openai==0.3.33) (0.11.0)\nRequirement already satisfied: sniffio in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from openai<2.0.0,>=1.104.2->langchain-openai==0.3.33) (1.3.1)\nRequirement already satisfied: tqdm>4 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from openai<2.0.0,>=1.104.2->langchain-openai==0.3.33) (4.67.1)\nRequirement already satisfied: idna>=2.8 in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from anyio<5,>=3.5.0->openai<2.0.0,>=1.104.2->langchain-openai==0.3.33) (3.10)\nRequirement already satisfied: certifi in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain-openai==0.3.33) (2025.8.3)\nRequirement already satisfied: httpcore==1.* in [./.venv/lib/python3.12/site-packages](https://file+.vscode-resource.vscode-cdn.net/Users/sinanozdemir/Fun/langchain/.venv/lib/python3.12/site-packages) (from httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain-openai==0.3.33) (1.0.9)\n...\n    Found existing installation: langchain-openai 0.3.33\n    Uninstalling langchain-openai-0.3.33:\n      Successfully uninstalled langchain-openai-0.3.33\nSuccessfully installed langchain-openai-0.3.33\nOutput is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?8b1444d4-c14c-494e-a61e-6f87208f04cb) or open in a [text editor](command:workbench.action.openLargeOutput?8b1444d4-c14c-494e-a61e-6f87208f04cb). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)...\nenv: OPENROUTER_API_KEY=sk-XX\nCreating HumanMessage with content: Hello! What's the capital of France now and what was it in 1937?\n\n\ud83d\udce4 Sending message: Hello! What's the capital of France now and what was it in 1937?\nresponse: ChatCompletion(id='gen-1758055935-3q6eJHmTr4FGfEz1BTPm', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello! The capital of France is **Paris**, both now and in 1937. \\n\\nParis has been the capital of France for many centuries and there hasn't been any change in this regard between 1937 and the present day. It has remained the political, cultural, and economic center of France throughout this entire period.\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning=\"This is a straightforward question about geography and history.\\n\\nThe capital of France now (currently) is Paris.\\n\\nIn 1937, the capital of France was also Paris. Paris has been the capital of France for many centuries, well before 1937. There wasn't a change in the capital between 1937 and now.\\n\\nLet me double-check this - Paris became the capital of France in the medieval period and has remained so continuously, except for brief periods during wars (like during WWII when the government fled, but Paris remained the official capital). In 1937, France was still the Third Republic and Paris was definitely the capital.\", reasoning_details=[{'type': 'reasoning.text', 'text': \"This is a straightforward question about geography and history.\\n\\nThe capital of France now (currently) is Paris.\\n\\nIn 1937, the capital of France was also Paris. Paris has been the capital of France for many centuries, well before 1937. There wasn't a change in the capital between 1937 and now.\\n\\nLet me double-check this - Paris became the capital of France in the medieval period and has remained so continuously, except for brief periods during wars (like during WWII when the government fled, but Paris remained the official capital). In 1937, France was still the Third Republic and Paris was definitely the capital.\", 'signature': 'EpYGCkYIBxgCKkCqYI6mN7yNfTw4n3Fw3ZnizbHZ1c4apsdZd6J079KSwcNeWuL3MSm4KXY+bvXOjQywep3C6jpiSEJj1mMD+GzzEgzo/2JY4oJGPVMUoS4aDP3jAsgxMhn6bvC/WyIwf8OfNsft8pFIGX50DHsjgiSWTqugjRGVZmBeH/7+Hnn6HSGiiGzcln2f4XAL6TCeKv0EdLMkxJV9JAUyvJzE99jb6XITl46Sk/xpqK1vbvzOOH8XMkWfjL0A/kcVmW/wxjFGnr1MLGq9k/rLLQi67Tk47UpRnigvpSxJAD5rvfm1XIs36k4H8Z52YfU5R8N5OLpNetVOdQ6OBm6pn08tuB20fCpgDY3JOctGilzKZ5Bzn0wkDsnMY45sZ85Ib+7qIFitX3LcwJyQSTba5tskU9eoopsdXrWTEP8h38O8LY2yziUR7EOCWR44dLoHoKpeY/V0UsQs38JYVL7jONrEo1dCGK/2S2dL6rTq1i8z2SqQdeA07G+arPndTEfM7ee2G+4h7RTRdJTVffsGvdGw6N0fgqeclZdrVdzt7UvET9xPPIDnuC8e8Uta85fAJEf0dHH9Ucv3NTXIvuf3I2GqSMJHEyHT8M+GD/UkmH9p8adkCd4EHmIN4EZ3zEpSQMQ4MSp64w2YZgN5k/Npwd8y/Mtu3GHsPYjQLwsNl37GCJS6yCFSYB1X1TnSLrjQOho7ndDPRpDWZH/RfrwIZ0EGkx5YvwZlpd4xruEgT9B5pDbJ2QBAWpk8UQLUEvmPAKB5USiLy5nkJDzvuiSraksw2MNHqZhP0fuLF/kk9k00RzcNo4h7KX+lh5m9DVv5RSl0Fr3YH8yPQ7GCPfb/kJ/2q0VBvd+N9pZb5fOxjDzbk2skKOnm1HeIJ2F5lM/ztOhdAVNCFOXfHvHauPEnU0G3Emg18Sh+OH1SqjTva5jfHcNmDzgy+jBAGvS+wccX7lmWlTfyeO7zY02B5ZxChatBX8pplFsUy72J1QHKf1sNrqNmFNtFnTLe36qt5FQ7tsnZLQi8Tv0Vib/+VClzIMaHwRgB', 'format': 'anthropic-claude-v1', 'index': 0}]), native_finish_reason='stop')], created=1758055935, model='anthropic/claude-sonnet-4', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=216, prompt_tokens=53, total_tokens=269, completion_tokens_details=None, prompt_tokens_details=None), provider='Anthropic')\n\ud83d\udce5 Response: Hello! The capital of France is **Paris**, both now and in 1937. \n\nParis has been the capital of France for many centuries and there hasn't been any change in this regard between 1937 and the present day. It has remained the political, cultural, and economic center of France throughout this entire period.\n\ud83d\udcca Token Usage:\n   Input tokens: 53\n   Output tokens: 216\n   Total tokens: 269\nAIMessage(content=\"Hello! The capital of France is **Paris**, both now and in 1937. \\n\\nParis has been the capital of France for many centuries and there hasn't been any change in this regard between 1937 and the present day. It has remained the political, cultural, and economic center of France throughout this entire period.\", additional_kwargs={'refusal': None, 'reasoning_content': \"This is a straightforward question about geography and history.\\n\\nThe capital of France now (currently) is Paris.\\n\\nIn 1937, the capital of France was also Paris. Paris has been the capital of France for many centuries, well before 1937. There wasn't a change in the capital between 1937 and now.\\n\\nLet me double-check this - Paris became the capital of France in the medieval period and has remained so continuously, except for brief periods during wars (like during WWII when the government fled, but Paris remained the official capital). In 1937, France was still the Third Republic and Paris was definitely the capital.\"}, response_metadata={'token_usage': {'completion_tokens': 216, 'prompt_tokens': 53, 'total_tokens': 269, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'anthropic/claude-sonnet-4', 'system_fingerprint': None, 'id': 'gen-1758055935-3q6eJHmTr4FGfEz1BTPm', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--f3718611-64ec-4335-9bf1-5640149e3990-0', usage_metadata={'input_tokens': 53, 'output_tokens': 216, 'total_tokens': 269, 'input_token_details': {}, 'output_token_details': {}})\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[15], line 1\n----> 1 raw_client_response = raw_client.chat.completions.create(\n      2     model=\"anthropic/claude-sonnet-4\",\n      3     messages=[{\"role\": \"user\", \"content\": \"Hello! What's the capital of France now and what was it in 1937?\"}],\n      4     extra_body={'reasoning': {'max_tokens': 10}}\n      5 )\n\nFile ~/Fun/langchain/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py:286, in required_args.<locals>.inner.<locals>.wrapper(*args, **kwargs)\n    284             msg = f\"Missing required argument: {quote(missing[0])}\"\n    285     raise TypeError(msg)\n--> 286 return func(*args, **kwargs)\n\nFile ~/Fun/langchain/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:1147, in Completions.create(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\n   1101 @required_args([\"messages\", \"model\"], [\"messages\", \"model\", \"stream\"])\n   1102 def create(\n   1103     self,\n   (...)   1144     timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN,\n   1145 ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n   1146     validate_response_format(response_format)\n-> 1147     return self._post(\n   1148         \"/chat/completions\",\n   1149         body=maybe_transform(\n   1150             {\n...\n-> 1106         return self._sslobj.read(len)\n   1107 except SSLError as x:\n   1108     if x.args[0] == SSL_ERROR_EOF and self.suppress_ragged_eofs:\n\nKeyboardInterrupt: \nOutput is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?b429ac10-60bc-43fe-97c5-1c359467bcff) or open in a [text editor](command:workbench.action.openLargeOutput?b429ac10-60bc-43fe-97c5-1c359467bcff). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)...\nChatCompletion(id='gen-1758055894-j9XPj5rWl0LUiKgOZLNm', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello! The capital of France is **Paris**, both now and in 1937. \\n\\nParis has been the capital of France for many centuries, consistently serving in this role with only brief interruptions during certain historical periods. In 1937, France was the Third Republic, and Paris remained its capital city, just as it is today under the Fifth Republic.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning='This is a straightforward question about the capital of France. \\n\\nThe capital of France now is Paris, and it has been Paris for a very long time. In 1937, the capital of France was also Paris. Paris has been the capital of France since it was established as such, with some brief exceptions during certain historical periods (like during WWII when there was the Vichy regime, but even then Paris remained the nominal capital, just under German occupation).\\n\\nSo both answers should be Paris.', reasoning_details=[{'type': 'reasoning.text', 'text': 'This is a straightforward question about the capital of France. \\n\\nThe capital of France now is Paris, and it has been Paris for a very long time. In 1937, the capital of France was also Paris. Paris has been the capital of France since it was established as such, with some brief exceptions during certain historical periods (like during WWII when there was the Vichy regime, but even then Paris remained the nominal capital, just under German occupation).\\n\\nSo both answers should be Paris.', 'signature': 'EpQFCkYIBxgCKkDY0p7LmZAkfU9lVCC6/4J3GDDG9ctMqC7fLmu+WMj3BeMOMd/t0HV/VHJwr2Nuf6nVlo7Pj56mw/hKFg4IPM4ZEgyv0Y7q3X2gENKROrIaDEwZtUD1q4AfSc2IuiIwGVKc1qHElT+grebBXANwqClqHcnXYL0Z9JMSf35twFEA9ui1rEt59CipJJ/r9C3tKvsDMDp7ONC458l7Lvvs85UcOwy24ucD90Bl9mo7Uob1w1ZcswVHJAWhVXWdKHTVkOUCPAQZ8jbdzVEQ5wvltIZqlMS6c68vZS0Qwqqas2B3afu/qGdqUsLyWrxQkC2ru0hqowkWtC9kuTVy9b/7BWbH97fUEQlYXj+a8bQJhXwKyqdWqqiNGJDJFfVT1dm65qiEIG8CVwwSLILu9mZ91qDbnGW+UZyx7gXQHgUAR9sR7MbUTEsEmq+rrcrYXchUu/CkvX8YEe1wLfxltPeQi/DFCDjQxKf6LBMOnCDHTU3RhEo9k1ekXxi2ixNJIAjugF+JOU/P/aHiESj0aJXNZYkgb4UK1BFJi+VKovLIfjPj50meGeT0ptRL8YB+wbXcojQHcJE9tHwhK/nncgFKsGlOOruujSyYc0XNtsmlk54VDidP1t2Cy4ghVtwH6rjh6qLF+FMJbA2FBTqWk5LdpOGCcFX7AmJ/wzENbg6OsCEgHkfkdTQM+h7Pr+deGTXqlvjF/ko8waOjXYWkDZGZhU7RfBN9OA9KGWd4j27l9AGdX/CPgDIQvE/i6xArwTWwsR+rqC/+FdKWcGu1cZ+rG+JyS+GeXByyfrMqk8LhkREbVH/4u/8Gbw3sBFkwDl2fmIWOKTfZwkhoHPWRJ/MDlmdHu59gxK0ZtQ3//gKFGAE=', 'format': 'anthropic-claude-v1', 'index': 0}]), native_finish_reason='stop')], created=1758055894, model='anthropic/claude-sonnet-4', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=191, prompt_tokens=53, total_tokens=244, completion_tokens_details=None, prompt_tokens_details=None), provider='Anthropic')\n\nSystem Information\n------------------\n> OS:  Darwin\n> OS Version:  Darwin Kernel Version 24.5.0: Tue Apr 22 19:54:49 PDT 2025; root:xnu-11417.121.6~2/RELEASE_ARM64_T6000\n> Python Version:  3.12.3 (main, Apr 18 2025, 10:09:16) [Clang 16.0.0 (clang-1600.0.26.6)]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.76\n> langchain: 0.3.27\n> langsmith: 0.4.28\n> langchain_anthropic: 0.3.20\n> langchain_deepseek: 0.1.4\n> langchain_openai: 0.3.33\n> langchain_text_splitters: 0.3.11\n\nOptional packages not installed\n-------------------------------\n> langserve\n\nOther Dependencies\n------------------\n> anthropic<1,>=0.67.0: Installed. No version info available.\n> async-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\n> httpx<1,>=0.23.0: Installed. No version info available.\n> jsonpatch<2.0,>=1.33: Installed. No version info available.\n> langchain-anthropic;: Installed. No version info available.\n> langchain-aws;: Installed. No version info available.\n> langchain-azure-ai;: Installed. No version info available.\n> langchain-cohere;: Installed. No version info available.\n> langchain-community;: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.70: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.72: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.76: Installed. No version info available.\n> langchain-core<2.0.0,>=0.3.75: Installed. No version info available.\n> langchain-deepseek;: Installed. No version info available.\n> langchain-fireworks;: Installed. No version info available.\n> langchain-google-genai;: Installed. No version info available.\n> langchain-google-vertexai;: Installed. No version info available.\n> langchain-groq;: Installed. No version info available.\n> langchain-huggingface;: Installed. No version info available.\n> langchain-mistralai;: Installed. No version info available.\n> langchain-ollama;: Installed. No version info available.\n> langchain-openai;: Installed. No version info available.\n> langchain-openai<1.0.0,>=0.3.28: Installed. No version info available.\n> langchain-perplexity;: Installed. No version info available.\n> langchain-text-splitters<1.0.0,>=0.3.9: Installed. No version info available.\n> langchain-together;: Installed. No version info available.\n> langchain-xai;: Installed. No version info available.\n> langsmith-pyo3>=0.1.0rc2;: Installed. No version info available.\n> langsmith>=0.1.17: Installed. No version info available.\n> langsmith>=0.3.45: Installed. No version info available.\n> openai-agents>=0.0.3;: Installed. No version info available.\n> openai<2.0.0,>=1.104.2: Installed. No version info available.\n> opentelemetry-api>=1.30.0;: Installed. No version info available.\n> opentelemetry-exporter-otlp-proto-http>=1.30.0;: Installed. No version info available.\n> opentelemetry-sdk>=1.30.0;: Installed. No version info available.\n> orjson>=3.9.14;: Installed. No version info available.\n> packaging>=23.2: Installed. No version info available.\n> pydantic<3,>=1: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4: Installed. No version info available.\n> pydantic>=2.7.4: Installed. No version info available.\n> pytest>=7.0.0;: Installed. No version info available.\n> PyYAML>=5.3: Installed. No version info available.\n> requests-toolbelt>=1.0.0: Installed. No version info available.\n> requests<3,>=2: Installed. No version info available.\n> requests>=2.0.0: Installed. No version info available.\n> rich>=13.9.4;: Installed. No version info available.\n> SQLAlchemy<3,>=1.4: Installed. No version info available.\n> tenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\n> tiktoken<1,>=0.7: Installed. No version info available.\n> typing-extensions>=4.7: Installed. No version info available.\n> vcrpy>=7.0.0;: Installed. No version info available.\n> zstandard>=0.23.0: Installed. No version info available.", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/32981/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32981/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/32977", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32977/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32977/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32977/events", "html_url": "https://github.com/langchain-ai/langchain/issues/32977", "id": 3422867986, "node_id": "I_kwDOIPDwls7MBNIS", "number": 32977, "title": "Unable to retrieve raw LLM response on JSON parsing error during structured output with retries; subsequent retries are extremely slow", "user": {"login": "rushant001", "id": 21120232, "node_id": "MDQ6VXNlcjIxMTIwMjMy", "avatar_url": "https://avatars.githubusercontent.com/u/21120232?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rushant001", "html_url": "https://github.com/rushant001", "followers_url": "https://api.github.com/users/rushant001/followers", "following_url": "https://api.github.com/users/rushant001/following{/other_user}", "gists_url": "https://api.github.com/users/rushant001/gists{/gist_id}", "starred_url": "https://api.github.com/users/rushant001/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rushant001/subscriptions", "organizations_url": "https://api.github.com/users/rushant001/orgs", "repos_url": "https://api.github.com/users/rushant001/repos", "events_url": "https://api.github.com/users/rushant001/events{/privacy}", "received_events_url": "https://api.github.com/users/rushant001/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 5680700839, "node_id": "LA_kwDOIPDwls8AAAABUpidpw", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/bug", "name": "bug", "color": "b60205", "default": true, "description": "Related to a bug, vulnerability, unexpected error with an existing feature"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2025-09-16T15:56:55Z", "updated_at": "2025-09-16T23:35:47Z", "closed_at": null, "author_association": "NONE", "type": {"id": 18879550, "node_id": "IT_kwDOB43M6c4BIBQ-", "name": "Bug", "description": "An unexpected problem or behavior", "color": "red", "created_at": "2024-02-16T01:43:33Z", "updated_at": "2024-07-26T15:00:33Z", "is_enabled": true}, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Checked other resources\n\n- [x] This is a bug, not a usage question.\n- [x] I added a clear and descriptive title that summarizes this issue.\n- [x] I used the GitHub search to find a similar question and didn't find it.\n- [x] I am sure that this is a bug in LangChain rather than my code.\n- [x] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n- [x] This is not related to the langchain-community package.\n- [x] I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n- [x] I posted a self-contained, minimal, reproducible example. A maintainer can copy it and run it AS IS.\n\n### Example Code\n\nWhen using ChatOpenAI with .with_structured_output(..., include_raw=True), if the underlying API returns an invalid or malformed JSON response (e.g., due to network issues, partial response, or model output error), a JSONDecodeError is raised deep in the call stack.\n\nThe main problems are:\n\nNo access to raw response: Even though include_raw=True is set, when a JSON parsing error occurs, LangChain immediately raises the exception without returning the raw response via the raw field in the result.\nSubsequent retries are extremely slow: After a failure and retry (via max_retries=3), the next invocation takes significantly longer than usual \u2014 sometimes tens of seconds or more \u2014 even though the input and context remain unchanged.\nThis makes it difficult to:\n\nDebug what malformed response was actually returned by the LLM.\nUnderstand whether the issue originates from OpenAI's side (e.g., incomplete streaming response) or post-processing.\nDiagnose why retries become so slow.\n\n```\nfrom typing import List\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import HumanMessage, AIMessage\n\nclass Response(BaseModel):\n    \"\"\"\n    Structured reasoning output to determine next agent and input.\n    \"\"\"\n    todolist: List[str] = Field(description=\"[Optional] Execution plan steps\", default=[])\n    current_todo_status: str = Field(description=\"[Optional] Status description of current plan\", default=\"\")\n    next_agent: str = Field(description=\"[Required] Next agent to invoke\")\n    next_agent_input: str = Field(description=\"[Required] Natural language instruction for next agent or final summary\")\n    reasoning: str = Field(description=\"[Required] Reasoning path: from chat history \u2192 agent selection \u2192 input generation\")\n\n# Initialize LLM\nllm = ChatOpenAI(\n    model='openai/gpt-4.1-mini',  # openrouter\n    temperature=0.1,\n    timeout=180,\n    max_tokens=30000,\n    max_retries=3\n)\n\n# Use structured output with raw included\nllm_with_structured_output = llm.with_structured_output(Response, include_raw=True)\n\n# Example chat history\nchat_history = [\n    HumanMessage(content=\"Plan a trip to Tokyo next month.\"),\n    AIMessage(content=\"Okay, I'll help you plan your trip.\")\n]\n\n# Async invoke that may fail\ntry:\n    llm_output = await llm_with_structured_output.ainvoke(chat_history)\nexcept Exception as e:\n    print(e)  # JSONDecodeError thrown, but no way to get raw response\n```\nExpected Behavior\nWhen include_raw=True, even if JSON parsing fails, the raw HTTP response body should be accessible in the returned object or attached to the exception.\nRetries should maintain consistent performance unless rate-limited \u2014 slowness suggests potential backoff or stuck state.\nActual Behavior\njson.decoder.JSONDecodeError: Expecting value: line 859 column 1 (char 4719)\n\n### Error Message and Stack Trace (if applicable)\n\n        llm_output = await llm_with_structured_output.ainvoke(chat_history, config=self.config)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n      File \"/Users/rushant\n/Documents/baidu/nlp-qa/tools-factory/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3088, in ainvoke\n        input_ = await coro_with_context(part(), context, create_task=True)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n      File \"/Users/rushant\n/Documents/baidu/nlp-qa/tools-factory/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3816, in ainvoke\n        results = await asyncio.gather(\n                  ^^^^^^^^^^^^^^^^^^^^^\n      File \"/Users/rushant\n/Documents/baidu/nlp-qa/tools-factory/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3808, in _ainvoke_step\n        return await coro_with_context(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n      File \"/Users/rushant\n/Documents/baidu/nlp-qa/tools-factory/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 5447, in ainvoke\n        return await self.bound.ainvoke(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n      File \"/Users/rushant\n/Documents/baidu/nlp-qa/tools-factory/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n        llm_result = await self.agenerate_prompt(\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n      File \"/Users/rushant\n/Documents/baidu/nlp-qa/tools-factory/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 991, in agenerate_prompt\n        return await self.agenerate(\n               ^^^^^^^^^^^^^^^^^^^^^\n      File \"/Users/rushant\n/Documents/baidu/nlp-qa/tools-factory/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 949, in agenerate\n        raise exceptions[0]\n      File \"/Users/rushant\n/Documents/baidu/nlp-qa/tools-factory/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1117, in _agenerate_with_cache\n        result = await self._agenerate(\n                 ^^^^^^^^^^^^^^^^^^^^^^\n      File \"/Users/rushant\n/Documents/baidu/nlp-qa/tools-factory/.venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 950, in _agenerate\n        response = await self.root_async_client.beta.chat.completions.parse(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n      File \"/Users/rushant\n/Documents/baidu/nlp-qa/tools-factory/.venv/lib/python3.11/site-packages/openai/resources/beta/chat/completions.py\", line 435, in parse\n        return await self._post(\n               ^^^^^^^^^^^^^^^^^\n      File \"/Users/rushant\n/Documents/baidu/nlp-qa/tools-factory/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1843, in post\n        return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n      File \"/Users/rushant\n/Documents/baidu/nlp-qa/tools-factory/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1537, in request\n        return await self._request(\n               ^^^^^^^^^^^^^^^^^^^^\n      File \"/Users/rushant\n/Documents/baidu/nlp-qa/tools-factory/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1640, in _request\n        return await self._process_response(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n      File \"/Users/rushant\n/Documents/baidu/nlp-qa/tools-factory/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1737, in _process_response\n        return await api_response.parse()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n      File \"/Users/rushant\n/Documents/baidu/nlp-qa/tools-factory/.venv/lib/python3.11/site-packages/openai/_response.py\", line 424, in parse\n        parsed = self._parse(to=to)\n                 ^^^^^^^^^^^^^^^^^^\n      File \"/Users/rushant\n/Documents/baidu/nlp-qa/tools-factory/.venv/lib/python3.11/site-packages/openai/_response.py\", line 259, in _parse\n        data = response.json()\n               ^^^^^^^^^^^^^^^\n      File \"/Users/rushant\n/Documents/baidu/nlp-qa/tools-factory/.venv/lib/python3.11/site-packages/httpx/_models.py\", line 764, in json\n        return jsonlib.loads(self.content, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n      File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/__init__.py\", line 346, in loads\n        return _default_decoder.decode(s)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n      File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/decoder.py\", line 337, in decode\n        obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n      File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/decoder.py\", line 355, in raw_decode\n        raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n    json.decoder.JSONDecodeError: Expecting value: line 859 column 1 (char 4719)\n\n### Description\n\n Expected Behavior\n\n- When include_raw=True, even if JSON parsing fails, the raw HTTP response body should be accessible in the returned object or attached to the exception.\n\n- Retries should maintain consistent performance unless rate-limited \u2014 slowness suggests potential backoff or stuck state.\n\n### System Info\n\nSystem Information\n------------------\n> OS:  Darwin\n> OS Version:  Darwin Kernel Version 23.5.0: Wed May  1 20:14:59 PDT 2024; root:xnu-10063.121.3~5/RELEASE_ARM64_T8122\n> Python Version:  3.11.9 (v3.11.9:de54cf5be3, Apr  2 2024, 07:12:50) [Clang 13.0.0 (clang-1300.0.29.30)]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.74\n> langchain: 0.3.27\n> langchain_community: 0.3.27\n> langsmith: 0.4.4\n> langchain_anthropic: 0.3.3\n> langchain_aws: 0.2.18\n> langchain_mcp_adapters: 0.1.9\n> langchain_openai: 0.3.1\n> langchain_text_splitters: 0.3.9\n> langchainhub: 0.1.15\n> langgraph_sdk: 0.1.74\n\nOptional packages not installed\n-------------------------------\n> langserve\n\nOther Dependencies\n------------------\n> aiohttp<4.0.0,>=3.8.3: Installed. No version info available.\n> anthropic: 0.64.0\n> async-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\n> boto3: 1.39.9\n> dataclasses-json<0.7,>=0.5.7: Installed. No version info available.\n> defusedxml: 0.7.1\n> httpx: 0.27.0\n> httpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\n> httpx>=0.25.2: Installed. No version info available.\n> jsonpatch<2.0,>=1.33: Installed. No version info available.\n> langchain-anthropic;: Installed. No version info available.\n> langchain-aws;: Installed. No version info available.\n> langchain-azure-ai;: Installed. No version info available.\n> langchain-cohere;: Installed. No version info available.\n> langchain-community;: Installed. No version info available.\n> langchain-core<0.4,>=0.3.36: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.66: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.72: Installed. No version info available.\n> langchain-deepseek;: Installed. No version info available.\n> langchain-fireworks;: Installed. No version info available.\n> langchain-google-genai;: Installed. No version info available.\n> langchain-google-vertexai;: Installed. No version info available.\n> langchain-groq;: Installed. No version info available.\n> langchain-huggingface;: Installed. No version info available.\n> langchain-mistralai;: Installed. No version info available.\n> langchain-ollama;: Installed. No version info available.\n> langchain-openai;: Installed. No version info available.\n> langchain-perplexity;: Installed. No version info available.\n> langchain-text-splitters<1.0.0,>=0.3.9: Installed. No version info available.\n> langchain-together;: Installed. No version info available.\n> langchain-xai;: Installed. No version info available.\n> langchain<1.0.0,>=0.3.26: Installed. No version info available.\n> langsmith-pyo3: Installed. No version info available.\n> langsmith>=0.1.125: Installed. No version info available.\n> langsmith>=0.1.17: Installed. No version info available.\n> langsmith>=0.3.45: Installed. No version info available.\n> mcp>=1.9.2: Installed. No version info available.\n> numpy: 1.26.4\n> numpy>=1.26.2;: Installed. No version info available.\n> numpy>=2.1.0;: Installed. No version info available.\n> openai: 1.58.1\n> openai-agents: Installed. No version info available.\n> opentelemetry-api: 1.36.0\n> opentelemetry-exporter-otlp-proto-http: Installed. No version info available.\n> opentelemetry-sdk: 1.36.0\n> orjson: 3.11.0\n> orjson>=3.10.1: Installed. No version info available.\n> packaging: 24.2\n> packaging>=23.2: Installed. No version info available.\n> pydantic: 2.11.7\n> pydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4: Installed. No version info available.\n> pydantic>=2.7.4: Installed. No version info available.\n> pytest: 7.4.0\n> PyYAML>=5.3: Installed. No version info available.\n> requests: 2.32.4\n> requests-toolbelt: 1.0.0\n> requests<3,>=2: Installed. No version info available.\n> rich: 14.0.0\n> SQLAlchemy<3,>=1.4: Installed. No version info available.\n> tenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\n> tenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\n> tiktoken: 0.9.0\n> types-requests: 2.32.4.20250611\n> typing-extensions>=4.14.0: Installed. No version info available.\n> typing-extensions>=4.7: Installed. No version info available.\n> zstandard: 0.23.0\n", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/32977/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32977/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/32972", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32972/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32972/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32972/events", "html_url": "https://github.com/langchain-ai/langchain/issues/32972", "id": 3422491141, "node_id": "I_kwDOIPDwls7L_xIF", "number": 32972, "title": "test(openai): increase max `test_base` max tokens", "user": {"login": "mdrxy", "id": 61371264, "node_id": "MDQ6VXNlcjYxMzcxMjY0", "avatar_url": "https://avatars.githubusercontent.com/u/61371264?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mdrxy", "html_url": "https://github.com/mdrxy", "followers_url": "https://api.github.com/users/mdrxy/followers", "following_url": "https://api.github.com/users/mdrxy/following{/other_user}", "gists_url": "https://api.github.com/users/mdrxy/gists{/gist_id}", "starred_url": "https://api.github.com/users/mdrxy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mdrxy/subscriptions", "organizations_url": "https://api.github.com/users/mdrxy/orgs", "repos_url": "https://api.github.com/users/mdrxy/repos", "events_url": "https://api.github.com/users/mdrxy/events{/privacy}", "received_events_url": "https://api.github.com/users/mdrxy/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 9019011443, "node_id": "LA_kwDOIPDwls8AAAACGZMxcw", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/integration", "name": "integration", "color": "bfd4f2", "default": false, "description": "Related to a provider partner package integration"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2025-09-16T14:14:59Z", "updated_at": "2025-09-18T07:12:14Z", "closed_at": null, "author_association": "COLLABORATOR", "type": {"id": 18879548, "node_id": "IT_kwDOB43M6c4BIBQ8", "name": "Task", "description": "A specific piece of work", "color": "yellow", "created_at": "2024-02-16T01:43:33Z", "updated_at": "2024-07-26T15:00:33Z", "is_enabled": true}, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Privileged issue\n\n- [x] I am a LangChain maintainer, or was asked directly by a LangChain maintainer to create an issue here.\n\n### Issue Content\n\ntests/integration_tests/chat_models/test_base.py::test_o1 is failing in CI\n\ncc @ccurme \n\nhttps://github.com/langchain-ai/langchain/blob/master/libs/partners/openai/tests/integration_tests/chat_models/test_base.py#L1019", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/32972/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32972/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/32967", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32967/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32967/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32967/events", "html_url": "https://github.com/langchain-ai/langchain/issues/32967", "id": 3421446854, "node_id": "I_kwDOIPDwls7L7yLG", "number": 32967, "title": "OpenRouter via ChatOpenAI in create_agent", "user": {"login": "teron131", "id": 131616347, "node_id": "U_kgDOB9hOWw", "avatar_url": "https://avatars.githubusercontent.com/u/131616347?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teron131", "html_url": "https://github.com/teron131", "followers_url": "https://api.github.com/users/teron131/followers", "following_url": "https://api.github.com/users/teron131/following{/other_user}", "gists_url": "https://api.github.com/users/teron131/gists{/gist_id}", "starred_url": "https://api.github.com/users/teron131/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teron131/subscriptions", "organizations_url": "https://api.github.com/users/teron131/orgs", "repos_url": "https://api.github.com/users/teron131/repos", "events_url": "https://api.github.com/users/teron131/events{/privacy}", "received_events_url": "https://api.github.com/users/teron131/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 5680700839, "node_id": "LA_kwDOIPDwls8AAAABUpidpw", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/bug", "name": "bug", "color": "b60205", "default": true, "description": "Related to a bug, vulnerability, unexpected error with an existing feature"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2025-09-16T09:57:22Z", "updated_at": "2025-09-16T09:58:32Z", "closed_at": null, "author_association": "NONE", "type": {"id": 18879550, "node_id": "IT_kwDOB43M6c4BIBQ-", "name": "Bug", "description": "An unexpected problem or behavior", "color": "red", "created_at": "2024-02-16T01:43:33Z", "updated_at": "2024-07-26T15:00:33Z", "is_enabled": true}, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Checked other resources\n\n- [x] This is a bug, not a usage question.\n- [x] I added a clear and descriptive title that summarizes this issue.\n- [x] I used the GitHub search to find a similar question and didn't find it.\n- [x] I am sure that this is a bug in LangChain rather than my code.\n- [x] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n- [x] This is not related to the langchain-community package.\n- [x] I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n- [x] I posted a self-contained, minimal, reproducible example. A maintainer can copy it and run it AS IS.\n\n### Example Code\n\n```python\nimport os\n\nfrom dotenv import load_dotenv\nfrom langchain.chat_models import init_chat_model\n\nload_dotenv()\n\nmodel = init_chat_model(\n    \"openai:x-ai/grok-code-fast-1\",\n    temperature=0,\n    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n    base_url=\"https://openrouter.ai/api/v1\",\n)\nmodel.invoke(\"Hello, world!\").content\n\n# 'Hello! Ah, the timeless \"Hello, world!\" \u2013 the universal greeting of programmers everywhere. What can I help you with today? Whether it\\'s code, curiosity, or cosmic queries, I\\'m all ears. \ud83d\ude80'\n```\n\nCOPIED FROM THE EXAMPLE AT https://docs.langchain.com/oss/python/langchain/agents#structured-output:\n```python\nfrom langchain.agents import create_agent\nfrom pydantic import BaseModel\n\n\nclass ContactInfo(BaseModel):\n    name: str\n    email: str\n    phone: str\n\n\nagent = create_agent(model, tools=[], response_format=ContactInfo)\n\nresult = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}]})\n\nresult[\"structured_response\"]\n# ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567')\n```\n\n### Error Message and Stack Trace (if applicable)\n\n```bash\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[8], line 13\n      8     phone: str\n     11 agent = create_agent(model, tools=[], response_format=ContactInfo)\n---> 13 result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}]})\n     15 result[\"structured_response\"]\n     16 # ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567')\n\nFile ~/projects/langchain-playground/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:3026, in Pregel.invoke(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\n   3023 chunks: list[dict[str, Any] | Any] = []\n   3024 interrupts: list[Interrupt] = []\n-> 3026 for chunk in self.stream(\n   3027     input,\n   3028     config,\n   3029     context=context,\n   3030     stream_mode=[\"updates\", \"values\"]\n   3031     if stream_mode == \"values\"\n   3032     else stream_mode,\n   3033     print_mode=print_mode,\n   3034     output_keys=output_keys,\n   3035     interrupt_before=interrupt_before,\n   3036     interrupt_after=interrupt_after,\n   3037     durability=durability,\n   3038     **kwargs,\n   3039 ):\n   3040     if stream_mode == \"values\":\n   3041         if len(chunk) == 2:\n\nFile ~/projects/langchain-playground/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:2647, in Pregel.stream(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\n   2645 for task in loop.match_cached_writes():\n   2646     loop.output_writes(task.id, task.writes, cached=True)\n-> 2647 for _ in runner.tick(\n   2648     [t for t in loop.tasks.values() if not t.writes],\n   2649     timeout=self.step_timeout,\n   2650     get_waiter=get_waiter,\n   2651     schedule_task=loop.accept_push,\n   2652 ):\n   2653     # emit output\n   2654     yield from _output(\n   2655         stream_mode, print_mode, subgraphs, stream.get, queue.Empty\n   2656     )\n   2657 loop.after_tick()\n\nFile ~/projects/langchain-playground/.venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py:162, in PregelRunner.tick(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\n    160 t = tasks[0]\n    161 try:\n--> 162     run_with_retry(\n    163         t,\n    164         retry_policy,\n    165         configurable={\n    166             CONFIG_KEY_CALL: partial(\n    167                 _call,\n    168                 weakref.ref(t),\n    169                 retry_policy=retry_policy,\n    170                 futures=weakref.ref(futures),\n    171                 schedule_task=schedule_task,\n    172                 submit=self.submit,\n    173             ),\n    174         },\n    175     )\n    176     self.commit(t, None)\n    177 except Exception as exc:\n\nFile ~/projects/langchain-playground/.venv/lib/python3.12/site-packages/langgraph/pregel/_retry.py:42, in run_with_retry(task, retry_policy, configurable)\n     40     task.writes.clear()\n     41     # run the task\n---> 42     return task.proc.invoke(task.input, config)\n     43 except ParentCommand as exc:\n     44     ns: str = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n\nFile ~/projects/langchain-playground/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:657, in RunnableSeq.invoke(self, input, config, **kwargs)\n    655     # run in context\n    656     with set_config_context(config, run) as context:\n--> 657         input = context.run(step.invoke, input, config, **kwargs)\n    658 else:\n    659     input = step.invoke(input, config)\n\nFile ~/projects/langchain-playground/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:394, in RunnableCallable.invoke(self, input, config, **kwargs)\n    392     # run in context\n    393     with set_config_context(child_config, run) as context:\n--> 394         ret = context.run(self.func, *args, **kwargs)\n    395 except BaseException as e:\n    396     run_manager.on_chain_error(e)\n\nFile ~/projects/langchain-playground/.venv/lib/python3.12/site-packages/langchain/agents/react_agent.py:609, in _AgentBuilder.create_model_node.<locals>.call_model(state, runtime, config)\n    606 prepared_messages = prompt_runnable.invoke(model_input, config)\n    608 # Then invoke the model with the prepared messages\n--> 609 response = cast(\"AIMessage\", model.invoke(prepared_messages, config))\n    610 response.name = self.name\n    612 if _are_more_steps_needed(state, response):\n\nFile ~/projects/langchain-playground/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:5495, in RunnableBindingBase.invoke(self, input, config, **kwargs)\n   5488 @override\n   5489 def invoke(\n   5490     self,\n   (...)   5493     **kwargs: Optional[Any],\n   5494 ) -> Output:\n-> 5495     return self.bound.invoke(\n   5496         input,\n   5497         self._merge_configs(config),\n   5498         **{**self.kwargs, **kwargs},\n   5499     )\n\nFile ~/projects/langchain-playground/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:436, in BaseChatModel.invoke(self, input, config, stop, **kwargs)\n    422 @override\n    423 def invoke(\n    424     self,\n   (...)    429     **kwargs: Any,\n    430 ) -> AIMessage:\n    431     config = ensure_config(config)\n    432     return cast(\n    433         \"AIMessage\",\n    434         cast(\n    435             \"ChatGeneration\",\n--> 436             self.generate_prompt(\n    437                 [self._convert_input(input)],\n    438                 stop=stop,\n    439                 callbacks=config.get(\"callbacks\"),\n    440                 tags=config.get(\"tags\"),\n    441                 metadata=config.get(\"metadata\"),\n    442                 run_name=config.get(\"run_name\"),\n    443                 run_id=config.pop(\"run_id\", None),\n    444                 **kwargs,\n    445             ).generations[0][0],\n    446         ).message,\n    447     )\n\nFile ~/projects/langchain-playground/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1114, in BaseChatModel.generate_prompt(self, prompts, stop, callbacks, **kwargs)\n   1105 @override\n   1106 def generate_prompt(\n   1107     self,\n   (...)   1111     **kwargs: Any,\n   1112 ) -> LLMResult:\n   1113     prompt_messages = [p.to_messages() for p in prompts]\n-> 1114     return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n\nFile ~/projects/langchain-playground/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:932, in BaseChatModel.generate(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\n    929 for i, m in enumerate(input_messages):\n    930     try:\n    931         results.append(\n--> 932             self._generate_with_cache(\n    933                 m,\n    934                 stop=stop,\n    935                 run_manager=run_managers[i] if run_managers else None,\n    936                 **kwargs,\n    937             )\n    938         )\n    939     except BaseException as e:\n    940         if run_managers:\n\nFile ~/projects/langchain-playground/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1208, in BaseChatModel._generate_with_cache(self, messages, stop, run_manager, **kwargs)\n   1206     result = generate_from_stream(iter(chunks))\n   1207 elif inspect.signature(self._generate).parameters.get(\"run_manager\"):\n-> 1208     result = self._generate(\n   1209         messages, stop=stop, run_manager=run_manager, **kwargs\n   1210     )\n   1211 else:\n   1212     result = self._generate(messages, stop=stop, **kwargs)\n\nFile ~/projects/langchain-playground/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1184, in BaseChatOpenAI._generate(self, messages, stop, run_manager, **kwargs)\n   1182     if raw_response is not None and hasattr(raw_response, \"http_response\"):\n   1183         e.response = raw_response.http_response  # type: ignore[attr-defined]\n-> 1184     raise e\n   1185 if (\n   1186     self.include_response_headers\n   1187     and raw_response is not None\n   1188     and hasattr(raw_response, \"headers\")\n   1189 ):\n   1190     generation_info = {\"headers\": dict(raw_response.headers)}\n\nFile ~/projects/langchain-playground/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1152, in BaseChatOpenAI._generate(self, messages, stop, run_manager, **kwargs)\n   1149 payload.pop(\"stream\")\n   1150 try:\n   1151     raw_response = (\n-> 1152         self.root_client.chat.completions.with_raw_response.parse(\n   1153             **payload\n   1154         )\n   1155     )\n   1156     response = raw_response.parse()\n   1157 except openai.BadRequestError as e:\n\nAttributeError: 'CompletionsWithRawResponse' object has no attribute 'parse'\nDuring task with name 'agent' and id '1862ccc8-05d7-9eeb-9a8f-bb0b14eb9fc0'\n```\n\n### Description\n\nThe support for **OpenRouter** has been poor. Always need to override the `api_key` and `base_url` in `ChatOpenAI`. I am trying the new LangChain v1 about structured outputs in the example. Using `\"openai:gpt-5\"` etc, native models, would completely work. I guess this is about the newer versions of OpenAI response API not compatible with the traditional OpenAI API.\n\n### System Info\n\nSystem Information\n------------------\n> OS:  Linux\n> OS Version:  #149-Ubuntu SMP Fri Apr 11 22:06:13 UTC 2025\n> Python Version:  3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 20:50:58) [GCC 12.3.0]\n\nPackage Information\n-------------------\n> langchain_core: 1.0.0a2\n> langchain: 1.0.0a5\n> langchain_community: 0.3.24\n> langsmith: 0.4.28\n> langchain_google_genai: 2.1.5\n> langchain_openai: 0.3.33\n> langchain_playground: 0.1.0\n> langchain_text_splitters: 0.3.11\n> langgraph_sdk: 0.2.6\n\nOptional packages not installed\n-------------------------------\n> langserve\n\nOther Dependencies\n------------------\n> aiohttp<4.0.0,>=3.8.3: Installed. No version info available.\n> dataclasses-json<0.7,>=0.5.7: Installed. No version info available.\n> docling: 2.41.0\n> fal-client: 0.7.0\n> filetype: 1.2.0\n> google-ai-generativelanguage: 0.6.18\n> google-genai: 1.23.0\n> httpx: 0.28.1\n> httpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\n> httpx<1,>=0.23.0: Installed. No version info available.\n> httpx>=0.25.2: Installed. No version info available.\n> ipython: 9.3.0\n> jsonpatch<2.0,>=1.33: Installed. No version info available.\n> langchain-anthropic;: Installed. No version info available.\n> langchain-aws;: Installed. No version info available.\n> langchain-azure-ai;: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.59: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.76: Installed. No version info available.\n> langchain-core<2.0.0,>=0.3.75: Installed. No version info available.\n> langchain-deepseek;: Installed. No version info available.\n> langchain-fireworks;: Installed. No version info available.\n> langchain-google-genai;: Installed. No version info available.\n> langchain-google-vertexai;: Installed. No version info available.\n> langchain-groq;: Installed. No version info available.\n> langchain-huggingface;: Installed. No version info available.\n> langchain-mistralai;: Installed. No version info available.\n> langchain-ollama;: Installed. No version info available.\n> langchain-openai;: Installed. No version info available.\n> langchain-perplexity;: Installed. No version info available.\n> langchain-text-splitters<1.0.0,>=0.3.11: Installed. No version info available.\n> langchain-together;: Installed. No version info available.\n> langchain-xai;: Installed. No version info available.\n> langchain<1.0.0,>=0.3.25: Installed. No version info available.\n> langgraph: 1.0.0a3\n> langgraph>=0.6.7: Installed. No version info available.\n> langsmith-pyo3>=0.1.0rc2;: Installed. No version info available.\n> langsmith<0.4,>=0.1.125: Installed. No version info available.\n> langsmith>=0.3.45: Installed. No version info available.\n> more-itertools: 10.7.0\n> numpy: 2.2.6\n> numpy>=1.26.2;: Installed. No version info available.\n> numpy>=2.1.0;: Installed. No version info available.\n> openai: 1.84.0\n> openai-agents>=0.0.3;: Installed. No version info available.\n> openai<2.0.0,>=1.104.2: Installed. No version info available.\n> opencc-python-reimplemented: 0.1.7\n> opentelemetry-api>=1.30.0;: Installed. No version info available.\n> opentelemetry-exporter-otlp-proto-http>=1.30.0;: Installed. No version info available.\n> opentelemetry-sdk>=1.30.0;: Installed. No version info available.\n> optimum;: Installed. No version info available.\n> orjson>=3.10.1: Installed. No version info available.\n> orjson>=3.9.14;: Installed. No version info available.\n> packaging>=23.2: Installed. No version info available.\n> pandas: 2.3.1\n> pillow: 11.2.1\n> pydantic: 2.12.0a1\n> pydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\n> pydantic<3,>=1: Installed. No version info available.\n> pydantic>=2.7.4: Installed. No version info available.\n> pydub: 0.25.1\n> pytest>=7.0.0;: Installed. No version info available.\n> python-dotenv: 1.1.0\n> pytubefix: 9.4.1\n> PyYAML>=5.3: Installed. No version info available.\n> replicate;: Installed. No version info available.\n> requests-toolbelt>=1.0.0: Installed. No version info available.\n> requests<3,>=2: Installed. No version info available.\n> requests>=2.0.0: Installed. No version info available.\n> rich>=13.9.4;: Installed. No version info available.\n> smolagents: 1.19.0\n> SQLAlchemy<3,>=1.4: Installed. No version info available.\n> tavily-python: 0.7.5\n> tenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\n> tenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\n> tiktoken: 0.9.0\n> tiktoken<1,>=0.7: Installed. No version info available.\n> torch;: Installed. No version info available.\n> tqdm: 4.67.1\n> transformers;: Installed. No version info available.\n> typing-extensions>=4.7: Installed. No version info available.\n> vcrpy>=7.0.0;: Installed. No version info available.\n> zstandard>=0.23.0: Installed. No version info available.", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/32967/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32967/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/32954", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32954/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32954/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32954/events", "html_url": "https://github.com/langchain-ai/langchain/issues/32954", "id": 3417525311, "node_id": "I_kwDOIPDwls7Ls0w_", "number": 32954, "title": "Support configuring `init_chat_model` via `context_schema`", "user": {"login": "NIK-TIGER-BILL", "id": 59732804, "node_id": "MDQ6VXNlcjU5NzMyODA0", "avatar_url": "https://avatars.githubusercontent.com/u/59732804?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NIK-TIGER-BILL", "html_url": "https://github.com/NIK-TIGER-BILL", "followers_url": "https://api.github.com/users/NIK-TIGER-BILL/followers", "following_url": "https://api.github.com/users/NIK-TIGER-BILL/following{/other_user}", "gists_url": "https://api.github.com/users/NIK-TIGER-BILL/gists{/gist_id}", "starred_url": "https://api.github.com/users/NIK-TIGER-BILL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NIK-TIGER-BILL/subscriptions", "organizations_url": "https://api.github.com/users/NIK-TIGER-BILL/orgs", "repos_url": "https://api.github.com/users/NIK-TIGER-BILL/repos", "events_url": "https://api.github.com/users/NIK-TIGER-BILL/events{/privacy}", "received_events_url": "https://api.github.com/users/NIK-TIGER-BILL/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 9260554836, "node_id": "LA_kwDOIPDwls8AAAACJ_jaVA", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/feature%20request", "name": "feature request", "color": "058a42", "default": false, "description": "request for an enhancement / additional functionality"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2025-09-15T11:24:26Z", "updated_at": "2025-09-15T11:24:26Z", "closed_at": null, "author_association": "NONE", "type": {"id": 18879553, "node_id": "IT_kwDOB43M6c4BIBRB", "name": "Feature", "description": "A request, idea, or new functionality", "color": "blue", "created_at": "2024-02-16T01:43:34Z", "updated_at": "2024-10-08T21:10:19Z", "is_enabled": true}, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Checked other resources\n\n- [x] This is a feature request, not a bug report or usage question.\n- [x] I added a clear and descriptive title that summarizes the feature request.\n- [x] I used the GitHub search to find a similar feature request and didn't find it.\n- [x] I checked the LangChain documentation and API reference to see if this feature already exists.\n- [x] This is not related to the langchain-community package.\n\n### Feature Description\n\n**Description:**\nSince version 0.6, the LangGraph documentation recommends avoiding `RunnableConfig` with `configurable` and instead using `context_schema` for static runtime context.\n\nHowever, when initializing a chat model with `init_chat_model`, the setup still relies on `configurable` instead of `context_schema`. Conceptually, a model (and its parameters) also belongs to the static runtime context.\n\nThis creates an inconsistency in practice:\n\n* Some settings (e.g. model and its parameters) must be configured via `configurable`.\n* Other settings (e.g. prompt definition and versioning) must be passed via `context_schema`.\n\nThis separation feels unintuitive and makes project configuration less straightforward.\n\n**Feature Request:**\nAdd support for configuring `init_chat_model` through `context_schema` so that all static runtime context can be managed consistently in one place.\n\n### Use Case\n\nWhen building a project with multiple runtime configurations, I currently need to split configuration between `configurable` (for models and their parameters) and `context_schema` (for prompt definitions and versioning). This separation feels inconsistent and makes configuration management harder to maintain\n\n### Proposed Solution\n\n_No response_\n\n### Alternatives Considered\n\n_No response_\n\n### Additional Context\n\n_No response_", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/32954/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32954/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}, {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/32953", "repository_url": "https://api.github.com/repos/langchain-ai/langchain", "labels_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32953/labels{/name}", "comments_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32953/comments", "events_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32953/events", "html_url": "https://github.com/langchain-ai/langchain/issues/32953", "id": 3417201649, "node_id": "I_kwDOIPDwls7Lrlvx", "number": 32953, "title": "Tool decorated with `@tool` returns string instead of structured Pydantic object in `on_tool_end` callback", "user": {"login": "rushant001", "id": 21120232, "node_id": "MDQ6VXNlcjIxMTIwMjMy", "avatar_url": "https://avatars.githubusercontent.com/u/21120232?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rushant001", "html_url": "https://github.com/rushant001", "followers_url": "https://api.github.com/users/rushant001/followers", "following_url": "https://api.github.com/users/rushant001/following{/other_user}", "gists_url": "https://api.github.com/users/rushant001/gists{/gist_id}", "starred_url": "https://api.github.com/users/rushant001/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rushant001/subscriptions", "organizations_url": "https://api.github.com/users/rushant001/orgs", "repos_url": "https://api.github.com/users/rushant001/repos", "events_url": "https://api.github.com/users/rushant001/events{/privacy}", "received_events_url": "https://api.github.com/users/rushant001/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [{"id": 5680700839, "node_id": "LA_kwDOIPDwls8AAAABUpidpw", "url": "https://api.github.com/repos/langchain-ai/langchain/labels/bug", "name": "bug", "color": "b60205", "default": true, "description": "Related to a bug, vulnerability, unexpected error with an existing feature"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2025-09-15T09:56:35Z", "updated_at": "2025-09-17T00:44:29Z", "closed_at": "2025-09-17T00:44:29Z", "author_association": "NONE", "type": {"id": 18879550, "node_id": "IT_kwDOB43M6c4BIBQ-", "name": "Bug", "description": "An unexpected problem or behavior", "color": "red", "created_at": "2024-02-16T01:43:33Z", "updated_at": "2024-07-26T15:00:33Z", "is_enabled": true}, "active_lock_reason": null, "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "issue_dependencies_summary": {"blocked_by": 0, "total_blocked_by": 0, "blocking": 0, "total_blocking": 0}, "body": "### Checked other resources\n\n- [x] This is a bug, not a usage question.\n- [x] I added a clear and descriptive title that summarizes this issue.\n- [x] I used the GitHub search to find a similar question and didn't find it.\n- [x] I am sure that this is a bug in LangChain rather than my code.\n- [x] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n- [x] This is not related to the langchain-community package.\n- [x] I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n- [x] I posted a self-contained, minimal, reproducible example. A maintainer can copy it and run it AS IS.\n\n### Example Code\n\nI'm using the @tool decorator from langchain_core.tools to define an async tool that returns a Pydantic model (CreateTestDataResponse). While the return type is correctly defined and the function returns a valid Pydantic instance, when observing the output in the on_tool_end callback (via tracing), the result appears as a stringified format, rather than the original structured JSON or Pydantic object.\n\n### Error Message and Stack Trace (if applicable)\n\nfrom langchain_core.tools import tool\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional\n\nclass TestDataExample(BaseModel):\n    prompt: str = Field(..., description=\"Prompt text\")\n    max_tokens: Optional[int] = Field(None, description=\"Max tokens\")\n    expected: Optional[str] = Field(None, description=\"Expected output\")\n\nclass CreateTestDataResponse(BaseModel):\n    data_url: str\n    status: str\n    url: str\n    test_type: str\n    model: str\n    total_count: int\n    examples: List[TestDataExample]\n    message: str\n\n@tool(parse_docstring=True, return_direct=False)\nasync def create_test_data(test_type: str, test_model_id: str, sample_count: int = 100) -> CreateTestDataResponse:\n    \"\"\"Create test data.\n\n    Args:\n        test_type: Type of test, e.g., performance/functional.\n        test_model_id: Model ID, e.g., qw35/qw45.\n        sample_count: Number of samples, default 100.\n\n    Returns:\n        CreateTestDataResponse: Structured response with test data info.\n    \"\"\"\n    # ... (mock logic to build response)\n    \n    return CreateTestDataResponse(\n        data_url=data_url,\n        status=\"success\",\n        url=url,\n        test_type=test_type,\n        model=test_model_id,\n        total_count=sample_count,\n        examples=example_objects,\n        message=\"Test data created successfully\"\n    )\n\nIn the on_tool_end callback or in LangSmith traces, I expect the tool's output to be accessible as a structured dictionary or JSON-compatible object (ideally preserving Pydantic types), so that downstream processing can access nested fields like examples[0].prompt directly.\n\nActual Behavior\nThe output in on_tool_end is received as a formatted string, for example:\n\ndata_url='data-68280' status='success' url='https://data.example.com/data-68280' test_type='functional' model='qw35' total_count=20 examples=[TestDataExample(prompt='1+1=?', max_tokens=None, expected='2'), ...] message='\u6d4b\u8bd5\u6570\u636e\u521b\u5efa\u6210\u529f'\nThis makes it difficult to parse or extract structured information without fragile string parsing.\n\nQuestion\nHow can I ensure that the tool preserves the return value as a JSON-serializable dict or raw Pydantic object in callbacks/tracing, instead of being converted to a string representation?\n\nIs this related to serialization settings in @tool, or is there a way to configure the run tracer to keep structured outputs?\n\nAny guidance or workaround would be appreciated!\n\n\n\n### Description\n\nQuestion\nHow can I ensure that the tool preserves the return value as a JSON-serializable dict or raw Pydantic object in callbacks/tracing, instead of being converted to a string representation?\n\nIs this related to serialization settings in @tool, or is there a way to configure the run tracer to keep structured outputs?\n\nAny guidance or workaround would be appreciated!\n\n\n\n### System Info\n\n\n\nSystem Information\n------------------\n> OS:  Darwin\n> OS Version:  Darwin Kernel Version 23.5.0: Wed May  1 20:14:59 PDT 2024; root:xnu-10063.121.3~5/RELEASE_ARM64_T8122\n> Python Version:  3.11.9 (v3.11.9:de54cf5be3, Apr  2 2024, 07:12:50) [Clang 13.0.0 (clang-1300.0.29.30)]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.74\n> langchain: 0.3.27\n> langchain_community: 0.3.27\n> langsmith: 0.4.4\n> langchain_anthropic: 0.3.3\n> langchain_aws: 0.2.18\n> langchain_mcp_adapters: 0.1.9\n> langchain_openai: 0.3.1\n> langchain_text_splitters: 0.3.9\n> langchainhub: 0.1.15\n> langgraph_sdk: 0.1.74\n\nOptional packages not installed\n-------------------------------\n> langserve\n\nOther Dependencies\n------------------\n> aiohttp<4.0.0,>=3.8.3: Installed. No version info available.\n> anthropic: 0.64.0\n> async-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\n> boto3: 1.39.9\n> dataclasses-json<0.7,>=0.5.7: Installed. No version info available.\n> defusedxml: 0.7.1\n> httpx: 0.27.0\n> httpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\n> httpx>=0.25.2: Installed. No version info available.\n> jsonpatch<2.0,>=1.33: Installed. No version info available.\n> langchain-anthropic;: Installed. No version info available.\n> langchain-aws;: Installed. No version info available.\n> langchain-azure-ai;: Installed. No version info available.\n> langchain-cohere;: Installed. No version info available.\n> langchain-community;: Installed. No version info available.\n> langchain-core<0.4,>=0.3.36: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.66: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.72: Installed. No version info available.\n> langchain-deepseek;: Installed. No version info available.\n> langchain-fireworks;: Installed. No version info available.\n> langchain-google-genai;: Installed. No version info available.\n> langchain-google-vertexai;: Installed. No version info available.\n> langchain-groq;: Installed. No version info available.\n> langchain-huggingface;: Installed. No version info available.\n> langchain-mistralai;: Installed. No version info available.\n> langchain-ollama;: Installed. No version info available.\n> langchain-openai;: Installed. No version info available.\n> langchain-perplexity;: Installed. No version info available.\n> langchain-text-splitters<1.0.0,>=0.3.9: Installed. No version info available.\n> langchain-together;: Installed. No version info available.\n> langchain-xai;: Installed. No version info available.\n> langchain<1.0.0,>=0.3.26: Installed. No version info available.\n> langsmith-pyo3: Installed. No version info available.\n> langsmith>=0.1.125: Installed. No version info available.\n> langsmith>=0.1.17: Installed. No version info available.\n> langsmith>=0.3.45: Installed. No version info available.\n> mcp>=1.9.2: Installed. No version info available.\n> numpy: 1.26.4\n> numpy>=1.26.2;: Installed. No version info available.\n> numpy>=2.1.0;: Installed. No version info available.\n> openai: 1.58.1\n> openai-agents: Installed. No version info available.\n> opentelemetry-api: 1.36.0\n> opentelemetry-exporter-otlp-proto-http: Installed. No version info available.\n> opentelemetry-sdk: 1.36.0\n> orjson: 3.11.0\n> orjson>=3.10.1: Installed. No version info available.\n> packaging: 24.2\n> packaging>=23.2: Installed. No version info available.\n> pydantic: 2.11.7\n> pydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4: Installed. No version info available.\n> pydantic>=2.7.4: Installed. No version info available.\n> pytest: 7.4.0\n> PyYAML>=5.3: Installed. No version info available.\n> requests: 2.32.4\n> requests-toolbelt: 1.0.0\n> requests<3,>=2: Installed. No version info available.\n> rich: 14.0.0\n> SQLAlchemy<3,>=1.4: Installed. No version info available.\n> tenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\n> tenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\n> tiktoken: 0.9.0\n> types-requests: 2.32.4.20250611\n> typing-extensions>=4.14.0: Installed. No version info available.\n> typing-extensions>=4.7: Installed. No version info available.\n> zstandard: 0.23.0\n", "reactions": {"url": "https://api.github.com/repos/langchain-ai/langchain/issues/32953/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/langchain-ai/langchain/issues/32953/timeline", "performed_via_github_app": null, "state_reason": "completed", "score": 1.0}]}